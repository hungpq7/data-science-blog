{
  
    
        "post0": {
            "title": "Graph Construction",
            "content": "1. Matplotlib API . Matplotlib is a famous visualization library in Python, is the foundation for many other visualization libraries. Matplotlib along with NumPy can be considered equivalent to Matlab. . This topic focuses only on Pyplot - the most essential sub-module of Matplotlib and how to use it to build graphs of mathematical functions. For data visualization plots such as bar chart, histogram or pie chart, there are other libraries that do a better job than Matplotlib, and they will be discussed on later topics. . 1.1. Plotting interfaces . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Object-oriented interface . Every plot created by Matplotlib is under the control of two objects, figure and axes. . An figure object is the whole image generated by Matplotlib, where everything is drawn on. It is a top-level object and works as a container for a number of axes objects. | Each axes object usually refers to a 2-dimensional Cartesian coordinate system. It gives access to plot elements such as plots, labels, titles, text. | . fig = plt.figure(figsize=(4,3)) # add an axes with some text ax = fig.add_subplot() ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . Instead of creating figure and axes independently, Matplotlib provides a single function subplots() creates the two objects at once. This function is highly recommended in practice, and the introduction to the previous method is to clarify how figure and axes work and how they are related. . fig, ax = plt.subplots(figsize=(4,3)) ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . When there are more than one axes, Matplotlib arranges them in a matrix of axes objects. Accessing each axes can be done using NumPy&#39;s array slicing. . fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,6), sharex=True, sharey=True) ax[0,2].text(0.5, 0.5, &#39;Row = 0 nColumn = 2&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) ax[1,1].text(0.5, 0.5, &#39;Row = 1 nColumn = 1&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . State-machine interface . Besides the object-oriented interface, Matplotlib also provides another way that makes use of state-machine to create plots. When using this, the state-machine implicitly and automatically creates figures and axes to achieve the desired plot. Then a set of simple functions is used to add plot elements to the current axes in the current figure. . Compared to object-oriented, state-machine interface is a bit more convenient for making a single axes, but it is not recommended for arranging multiple axes. Overall, object-oriented interface is still the go-to method. . plt.text(0.5, 0.5, &#39;Data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=32) plt.axvline(x=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.axhline(y=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.title(&#39;State-machine interface&#39;) plt.show() . 1.2. Controlling axes . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Equalizing axes . Graphs require the two axes to have the same scale. . fig, ax = plt.subplots() # make the two axes scaled ax.axis(&#39;scaled&#39;) # set tick frequencies of both axes to be 0.02 ax.xaxis.set_major_locator(plt.MultipleLocator(0.02)) ax.yaxis.set_major_locator(plt.MultipleLocator(0.02)) plt.show() . Axes limits . fig, ax = plt.subplots() ax.axis(&#39;scaled&#39;) # set limit for each axis ax.set_xlim(0, 2) ax.set_ylim(0, 1) ax.xaxis.set_major_locator(plt.MultipleLocator(0.2)) plt.show() . Formatting axes . def axis_formatter(value, tick): N = int(np.round(2 * value / np.pi)) if N == 0: return &quot;0&quot; elif N == 2: return r&quot;$ pi$&quot; elif N == -2: return r&#39;$- pi$&#39; elif N % 2 == 1 and N &gt; 0: return fr&quot;$ frac{{{N}}}{{2}} pi$&quot; elif N % 2 == 1 and N &lt; 0: return fr&quot;$- frac{{{-N}}}{{2}} pi$&quot; else: return fr&quot;${N//2} pi$&quot; . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(10,10)) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) # set x-tick frequency to be pi/2 and apply a custom format strategy ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi/2)) ax.xaxis.set_major_formatter(plt.FuncFormatter(axis_formatter)) ax.set_ylim(-2.5, 2.5) plt.show() . 1.3. Plotting . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Graphs . Matplotlib does not really have a way to make graphs, but this can be achieved indirectly by using the axes.plot() method. The original functionality of this function is to plot a polyline connects data points. . It has an optional parameter, fmt that defines basic formatting following the syntax: &#39;{marker}{linestyle}{color}&#39;. The formatting string must not strictly follow the order in the syntax, but note that the parsing may be ambiguous. The table below summarizes some useful formatting strings: . Parameter Character Meaning . marker | &#39;.&#39; | point marker | . marker | &#39;o&#39; | big point marker | . linestyle | &#39;-&#39; | solid line style | . linestyle | &#39;--&#39; | dashed line style | . linestyle | &#39;:&#39; | dotted line style | . linestyle | &#39;-.&#39; | dash-dot line style | . color | &#39;k&#39; | black | . color | &#39;r&#39; | red | . color | &#39;c&#39; | cyan | . color | &#39;m&#39; | magenta | . color | &#39;g&#39; | green | . With different combinations of formatting strings, axes.plot() can result in graphs (indirectly) and points (directly). In order to make graphs, the input for x-axis needs to be a dense set of values. . x1 = np.linspace(-5, 10, 1000) y1 = np.sin(x1) + np.sin(2*x1) x2 = np.array([0, 4, 5.5, -4]) y2 = np.array([1, 2, 0, -2]) fig, ax = plt.subplots(figsize=(10,4)) ax.plot(x1, y1, &#39;k&#39;) ax.plot(x2, y2, &#39;oc&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Spans . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(12,4)) # add two lines represent the two axes ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # add a vertical span across y-axis ax.axvspan(xmin=4.5, xmax=6.5, alpha=0.3) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Vectors . The axes.quiver() method in Matplotlib allows plotting vectors. It has five important parameters, which must have the same length: . X and Y locate the origins of vectors | U and V define the vectors | color sets vector colors | . For some reasons, the parameters units=&#39;xy&#39; and scale=1 must always be set in order to draw vectors correctly. . fig, ax = plt.subplots() ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.quiver( [1, 0, 2], [1, 0, 0], [1, -3, -4], [2, 1, -2], color=[&#39;firebrick&#39;, &#39;teal&#39;, &#39;seagreen&#39;], units=&#39;xy&#39;, scale=1 ) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4,3) ax.set_ylim(-3,4) plt.show() . 1.4. Annotations . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Text positioning . The axes.text() method adds text to the axes at the location defined by two parameters, x and y, use true coordinate values by default. By setting transform=ax.transAxes (highly recommended), these two parameters are now on a relative coordinates where (0,0) is the lower-left corner and (1,1) is the upper-right corner. . There is also a pair of parameters, ha and va (horizontal/vertical alignment), specify the relative position of the coordinates to the text box. Possible values for these parameters are left right center and top bottom center, respectively. . fig, ax = plt.subplots(figsize=(5,5)) ax.text(0.5, 0.5, &#39;(0.5, 0.5) ncenter center&#39;, va=&#39;center&#39;, ha=&#39;center&#39;, size=14, transform=ax.transAxes) ax.text(0, 0, &#39;(0, 0) nbottom left&#39;, va=&#39;bottom&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 1, &#39;(1, 1) ntop right&#39;, va=&#39;top&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.text(0, 1, &#39;(0, 1) ntop left&#39;, va=&#39;top&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 0, &#39;(1, 0) nbottom right&#39;, va=&#39;bottom&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.yaxis.set_major_formatter(plt.NullFormatter()) plt.show() . Plot titles . x = np.linspace(-1, 8, 1000) fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(5,7)) fig.suptitle(&#39;Mathematics&#39;, fontsize=16) ax[0].plot(x, np.sin(x)+2*np.sin(2*x), &#39;k&#39;) ax[0].set_title(&#39;$y= sin(x)+2 sin(x)$&#39;) ax[0].axis(&#39;equal&#39;) ax[0].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[0].yaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax[1].set_title(&#39;$y= sin(x2 pi) cdot e^{-x}$&#39;) ax[1].axis(&#39;scaled&#39;) ax[1].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].yaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Axis labels . x = np.linspace(-1, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) # set axis labels ax.set_xlabel(&#39;x-axis&#39;) ax.set_ylabel(&#39;y-axis&#39;) plt.show() . Legends . The axes.lengend() method takes a list of plot names, which is indexed the same as the number of axes.plot() methods called. It can also use the value of label parameter in each axes.plot() method to generate the legend. . By default, Matplotlib automatically determines the best location for the legend inside the plot. This can be changed using the parameter loc, which specifies locations using strings such as &quot;lower center&quot; and &quot;upper right&quot;. When there is no optimized location like in the example below, the legend should be placed outside of the axes. This can be achieved using bbox_to_anchor alongside with loc; this pair of parameters behaves exactly the same as text positioning. . Another useful configuration is the ncol parameter. It specifies the number of columns in the grid of legend labels. . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # pass the list of labels into the ax.legend method ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;, label=&#39;$ sin(x)$&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;, label=&#39;$ sin(2x)$&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;, label=&#39;$ sin(x)+ sin(2x)$&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend(bbox_to_anchor=(0.5, 1), loc=&#39;lower center&#39;, ncol=3, title=&#39;Graphs&#39;) plt.show() . 1.5. Themes and colors . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Themes . Matplotlib supports a variety of themes such as ggplot seaborn tableau-colorblind10 inspired by other popular visualization tools. However, since drawing graphs requires a white background, then I usually use seaborn-whitegrid. . plt.style.use(&#39;ggplot&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . plt.style.use(&#39;seaborn&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Built-in Matplotlib colors . Besides themes, Matplotlib also provides a collection of colors for better visualization. Some nice colors are: dimgrey indianred tomato goldenrod seagreen teal darkturquoise darkslategrey slategrey royalblue rebeccapurple. . . 2. Mathematical functions . In Mathematics, a function) (usually denoted $f$) is a process of transforming each given input number (denoted $x$) to exactly one output number (denoted $y$). This process can be in the form of a mathematical formula or some logical rules and can be expressed using the notation: . $$y=f(x)$$ . The set of inputs and the set of outputs are called the domain (denoted $X$) and codomain (denoted $Y$), consecutively. The function in this context is written as $f colon X to Y$. . The set of all pairs $(x,y)$, formally denoted $G= {(x,y) mid x in X }$ is called the graph of the function. It popularly means the illustration of the function with the condition $x,y in mathbb{R}$. . Another related concept to function is map), which is often used as a synonym of function. However, from now on in this series about Data Science, map refers to a generalization of function, which extends the scope of $x$ and $y$ not to be restricted to numbers only, but can also be other data-like objects such as strings and datetime. In reality, the more common meaning of the word map, Google Maps, for example, is actually made by mapping the Earth&#39;s surface to a sheet of paper. The notation for map is: . $$f colon x mapsto y$$ . 2.1. Elementary functions . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Polynomial functions . A polynomial is a function having the following form: . $$y=a_nx^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2}+ dots+a_1x+a_0$$ . where: . $n ;(n in mathbb{N})$ is the degree of the polynomial | $n,n-1,n-2, dots,0$ are the degrees of the corresponding monomial | $a_n,a_{n-1}, dots,a_0 ;(a_n neq0)$ are the coefficients | . Each polynomial has the domain of $x in mathbb{R}$ and the codomain of $y in mathbb{R}$. It has a maximum of $n$ solutions and a maximum of $n-1$ extrema. . Some popular polynomials are: . $f(x)=ax+b$, linear functions | $f(x)=ax^2+bx+c$, quadratic functions, which has the parabola shape | . x = np.linspace(-5, 4, 1000) y1 = x**2 + 4*x + 2 y2 = 1/4*x**3 + 3/4*x**2 - 3/2*x fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;Quadratic (degree 2)&#39;) ax.plot(x, y2, label=&#39;Cubic (degree 3)&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_ylim(-3, 6) ax.legend() plt.show() . Exponential functions . An exponential function is a function having $x$ in its power. The form of the function is: . $$y=b^x$$ . where $b ;(b&gt;0)$ is the base. . Exponential functions have the domain of $x in mathbb{R}$ and the codomain of $y in(0,+ infty)$. Exponential functions are monotonic, and can be either increasing or decreasing as the value of $b$ changes. All exponential functions go through the point $(0,1)$, since $b^0=1$ for any value of $b$. . Some popular exponential functions are: . $f(x)=2^x$, the foundation of binary system being used in almost all modern computers | $f(x)=10^x$, the foundation of decimal numeral system | $f(x)=e^x= exp{(x)}$, the natural) exponential function, the function equals to its own derivative | . x = np.linspace(-5, 5, 1000) y1 = 2**x y2 = np.e**x y3 = 10**x y4 = (1/2)**x fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y=2^x$&#39;) ax.plot(x, y2, label=&#39;$y=e^x$&#39;) ax.plot(x, y3, label=&#39;$y=10^x$&#39;) ax.plot(x, y4, label=&#39;$y=0.5^x$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-1, 7) ax.legend(bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Logarithm functions . Logarithm is the inverse function of exponentiation. With a given base $b ;(b&gt;0)$, logarithm functions have the following form: . $$y= log_b{x}$$ . The domain of the logarithm functions is $x in(0,+ infty)$. Popular logarithm functions are: . $f(x)= log_2{x}$, the binary logarithm | $f(x)= log_{10}{x}= lg{x}$, the common logarithm | $f(x)= log_e{x}= ln{x}= log{x}$, the natural logarithm | . Notice that in many Python libraries, the log function refers to the natural logarithm instead of base 10. . x = np.linspace(1e-6, 10, 1000) y1 = np.log2(x) y2 = np.log10(x) y3 = np.log(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= log_2{x}$&#39;) ax.plot(x, y2, label=&#39;$y= log_{10}{x}$&#39;) ax.plot(x, y3, label=&#39;$y= ln{x}$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(0, 9) ax.set_ylim(-3, 3) ax.legend() plt.show() . Power functions . Power functions are a family of functions having $x$ in their bases, they should not be confused with exponential functions. The most common type of power function being applied in Data Science is the ones with a natural exponent: . $$y=x^n$$ . where $n ;(n in mathbb{N})$ is the exponent. . The domain of power functions is $x in mathbb{R}$. If $n$ is even, the function is reflectional symmetric since $f(x)=f(-x)$; if $n$ is odd, the function is rotationally symmetric since $f(x)=-f(-x)$. Some popular named functions of this type are: . $f(x)=x^2$, the square function), applied in calculating the area | $f(x)=x^3$, the cube function), applied in calculating the volume | . x = np.linspace(-5, 5, 1000) y1 = x**2 y2 = x**5 fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y=x^2$&#39;) ax.plot(x, y2, label=&#39;$y=x^5$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-3, 3) ax.set_ylim(-4, 4) ax.legend() plt.show() . Root functions . Root functions are power functions having rational exponents: . $$y=x^{a/b}= sqrt[b]{x^a} ;(a,b in mathbb{N})$$ . The domain of root functions is $x geq0$ for even values of $b$ and is $x in mathbb{R}$ for odd values of $b$. Some popular root functions are: . $f(x)= sqrt{x}$, the square root function | $f(x)= sqrt[3]{x}$, the cube root function | . x1 = np.linspace(0, 5, 1000) y1 = np.sqrt(x1) x2 = np.linspace(-5, 5, 1000) y2 = np.cbrt(x2) fig, ax = plt.subplots() ax.plot(x1, y1, label=&#39;$y=x^2$&#39;) ax.plot(x2, y2, label=&#39;$y=x^5$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-2.5, 2.5) ax.legend() plt.show() . Reciprocal functions . Reciprocal function is a special case of power function, when the exponent is a negative rational number: . $$y=x^{-q}= frac{1}{x^q} ;(q in mathbb{Q})$$ . The domain of reciprocal functions is $x neq0$. They also have symmetric properties like natural power functions. The term reciprocal usually refers to the most common case, $f(x)=x^{-1}$, which has a beautiful property: $x cdot x^{-1}=1$. . x = np.linspace(-5, 5, 1000) y = 1/x fig, ax = plt.subplots() ax.plot(x, y, &#39;k&#39;, label=&#39;$y=x^{-1}$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.legend() plt.show() . Trigonometric functions . Three most used trigonometric functions in modern Mathematics are the sine ($y= sin{x}$), the cosine ($y= cos{x}$) and the tangent ($y= tan{x}$). They are defined on the unit circle $x^2+y^2=1$. Since all trigonometric functions are periodic, the visualization for them is limited to be in the domain $x in(-2 pi,2 pi)$. . x = np.linspace(-2*np.pi, 2*np.pi, 1000) y1 = np.sin(x) y2 = np.cos(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= sin(x)$&#39;) ax.plot(x, y2, label=&#39;$y= cos(x)$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-2*np.pi, 2*np.pi) ax.set_ylim(-1.5, 1.5) ax.legend(bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Hyperbolic functions . Hyperbolic functions are trigonometric functions defined using the unit hyperbola $x^2-y^2=1$ rather than the circle. The formulas for the three most popular hyperbolic functions are: . $ displaystyle{ sinh{x}= frac{e^x-e^{-x}}{2}}$ . | $ displaystyle{ cosh{x}= frac{e^x+e^{-x}}{2}}$ . | $ displaystyle{ tanh{x}= frac{ sinh{x}}{ cosh{x}}= frac{e^x-e^{-x}}{e^x+e^{-x}}}$ . | . x = np.linspace(-5, 5, 1000) y1 = np.sinh(x) y2 = np.cosh(x) y3 = np.tanh(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= sinh(x)$&#39;) ax.plot(x, y2, label=&#39;$y= cosh(x)$&#39;) ax.plot(x, y3, label=&#39;$y= tanh(x)$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . 2.2. Function transformations . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Translation . Translation) refers to the process of shifting the entire graph to another position. Given a constant $c ;(c&gt;0)$, the formulas below show how $c$ moves the graph of $f(x)$: . $f(x)+c$ shifts the graph $c$ units up | $f(x)-c$ shifts the graph $c$ units down | $f(x+c)$ shifts the graph $c$ units to the left | $f(x-c)$ shifts the graph $c$ units to the right | . The whole process can also be represented using a matrix multiplication: . $$ begin{bmatrix} begin{array}{r}1 &amp; 0 &amp; c1 0 &amp; 1 &amp; c2 0 &amp; 0 &amp; 1 end{array} end{bmatrix} cdot begin{bmatrix}x y 1 end{bmatrix} = begin{bmatrix}x+c_1 y+c_2 1 end{bmatrix} $$ def f(x): y = x**2/2 - 2*x - 2 return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(x)+3, label=&#39;$f(x)+3$&#39;, c=&#39;steelblue&#39;) ax.plot(x, f(x-2), label=&#39;$f(x-2)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 4, 4], [-2, -2], [ 0, 2], [ 3, 0], color=[&#39;steelblue&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-3, 9) ax.set_ylim(-5, 5) ax.legend() plt.show() . Dilation . Dilation refers to the process of compressing/stretching the graph. Given a constant $c ;(c&gt;1)$, the formulas below show how $c$ resizes the graph of $f(x)$: . $c cdot f(x)$ stretches the graph $c$ times vertically (in the $y$-direction) | $ displaystyle{ frac{1}{c} cdot f(x)}$ compresses the graph $c$ times vertically (in the $y$-direction) | $f(cx)$ compress the graph $c$ times horizontally (in the $x$-direction) | $ displaystyle{f left( frac{x}{c} right)}$ stretches the graph $c$ times horizontally (in the $x$-direction) | . Note that resizing to a similar shape requires compressing/stretching the same times on both axes. This can be written in matrix form: . $$ begin{bmatrix}c&amp;0 0&amp;c end{bmatrix} cdot begin{bmatrix}x y end{bmatrix} = begin{bmatrix}cx cy end{bmatrix} $$ def f(x): y = 2**x return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(x)/4, label=&#39;$f(x)/4$&#39;, c=&#39;steelblue&#39;) ax.plot(x, f(x*2), label=&#39;$f(2x)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 2, 2], [ 4, 4], [ 0, -2], [-4, 0], color=[&#39;steelblue&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-1, 8) ax.legend() plt.show() . Reflection . Reflection) considers a line as the axis of reflection to map the graph to its image. Here are reflection formulas: . $f(-x)$ results in the graph being reflected across the $y$-axis | $f-(x)$ results in the graph being reflected across the $x$-axis | . The refelection process can also be written as the dot product of a square matrix with the vector $ begin{bmatrix}x y end{bmatrix}$, where each matrix uses a different axis of reflection. For example, $ begin{bmatrix} begin{array}{r}1&amp;0 0&amp;-1 end{array} end{bmatrix}$ for a reflection over $x$-axis, $ begin{bmatrix} begin{array}{r}-1&amp;0 0&amp;1 end{array} end{bmatrix}$ for a reflection over $y$-axis and $ begin{bmatrix}0&amp;1 1&amp;0 end{bmatrix}$ for a reflection over the line $y=x$. . def f(x): y = 2**x return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(-x), label=&#39;$f(-x)$&#39;, c=&#39;steelblue&#39;) ax.plot(x, -f(x), label=&#39;$-f(x)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 0, 0, 1, 1], [ 2, 2, 0, 0], [ 1, -1, 0, 0], [ 0, 0, 2, -2], color=[&#39;black&#39;, &#39;steelblue&#39;, &#39;black&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . Rotation . Graph rotation actually means rotating points on the graph one by one. The rotated graph is no longer a function and cannot be represented in the form $y=f(x)$. The formula below shows how to rotate a point about the origin, counterclockwise and by an angle of $ theta$: . $$ begin{aligned} x&#39;=x cos theta-y sin theta y&#39;=x sin theta+y cos theta end{aligned}$$This beautiful formula can also be written using matrix notation as: . $$ begin{bmatrix}x&#39; y&#39; end{bmatrix}= begin{bmatrix} begin{array}{r} cos theta &amp;- sin theta sin theta &amp; cos theta end{array} end{bmatrix} begin{bmatrix}x y end{bmatrix} $$or using complex numbers: . $$ begin{aligned} x&#39;+iy&#39; &amp;= (x cos theta-y sin theta)+i(x sin theta+y cos theta) &amp;= ( cos theta+i sin theta)(x+iy) &amp;= e^{i theta}(x+iy) end{aligned}$$ def rotate(x, y, theta): x_rotated = x*np.cos(theta) - y*np.sin(theta) y_rotated = x*np.sin(theta) + y*np.cos(theta) return x_rotated, y_rotated x = np.linspace(-10, 10, 1000) y = x**2 - 3 theta = np.pi/2 x_rotated, y_rotated = rotate(x, y, theta) fig, ax = plt.subplots() ax.plot(x, y, label=&#39;Original graph&#39;, c=&#39;black&#39;) ax.plot(x_rotated, y_rotated, label=&#39;Rotated graph&#39;, c=&#39;steelblue&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . . &#9829; By Quang Hung x Thuy Linh &#9829; . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hungpq7.github.io/data-science-blog/jupyter/visualization/matplotlib/2022/08/24/matplotlib-graph.html",
            "relUrl": "/jupyter/visualization/matplotlib/2022/08/24/matplotlib-graph.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "1. Overview",
            "content": "1.1. Exhausted search . There are two popular methods of searching hyperparameters, implemented in Scikit-learn via the classes [GridSearchCV][1] and [RandomizedSearchCV][2]. The idea of each method are already explained clearly in the image below, where the red area shows how much each hyperparameter contributes to model score. . . Grid Search goes to every possible combinations, and thus it will revisit a parameter value multiple times. This makes Grid Search have a low coverage and is very expensive, especially for algorithms with a huge number of hyperparameters such as XGBoost and LightGBM. . Random Search is a more efficient method, it randomly takes a number combinations to train models. With the same number of iterations provided, Random Search can cover a wider range of values, and thus it is able to reach the optimal value that Grid Search cannot. Another advantage is that you can include unimportant hyperparameters in your search without increasing the number of trials. . 1.2. Sequential search . It&#39;s not difficult to see that searching methods perform expensive trials independently, thus they are actually wasting a lot of useful information from previous trials. In order to take advantages of historical information, SMBO (Sequential Model-Based Optimization) comes to the rescue. This is a class contains algorithms that smartly predict the most promising configuration sequentially, thus reduce the number of training rounds needed significantly. . The basic idea of SMBO is to build a probabilistic surrogate model of the black-box function $f: text{hyperparameter} rightarrow text{score}$ using results from observed trials. You can think of it as Machine Learning of Machine Learning; this concept can be a bit complicated but is exactly how it works. There are many algorithms using this sequential search strategy, but in this topic, we are discussing Bayesian Optimization and Tree-structured Parzen Estimators. . 1.3. Bandit-based . Another effective strategy is using Multi-Armed Bandit. This method is all about resources allocation, which smartly uses the most resources on the most promising hyperparameters sets. Multi-Armed Bandit fits so well for Machine Learning algorithms with iterative training behavior, as number of estimators in Gradient Boosting and number of epochs in Deep Learning may serve as the role of resources. . A well-known algorithm of this family is Successive Halving, which is implemented in Scikit-learn via [HalvingGridSearchCV][2] and [HalvingRandomSearchCV][3]. There are also improve variants HyperBand and BOHB (a combination of Bayes Optimization and HyperBand). . 1.4. Evolutionary . Evolutionary algorithms are inspired from natural selection treat each hyperparameter as a gene and iteratively discard candidates with bad genes. Only good genes are kept in order to perform random permutations on it. . 2. Bayesian Optimization . A SMBO using Gaussian Process as the surrogate function is called Bayesian Optimization (BO), since Gaussian Process use Bayes&#39; rule to update the posterior distribution. It has some disadvantages: . It does not support categorical hyperparameters, such as impurity criterion in Decision Tree. | Gaussian Process does not scale very well | . 2.1. Algorithm . Input: . A domain, or search space of hyperparameters | $f$, the black-box function to be optimized, it maps hyperpameters to model score: $y=f(x)$ and is very expensive to evaluate | $T$, the number of trials budget | $ mathcal{S}$, a surrogate model (Gaussian Process) which takes finished trials as input then returns a distribution of model score for each unobserved trial | $ mathcal{A}$, an acquisition function for deciding where the next trial should locate at | . Step 1: Create a set $ mathcal{H}$ for storing historical trials $(x,y)$ with a randomly initialized trial $(x_0,y_0)$. . Step 2: For each trial $t$ for $t=1,2, dots,T$: . Fit the surrogate model on $ mathcal{H}$ to get $ mathcal{S}_t$ | Compute the acquisition function $ mathcal{A}_t(x)$ using $ mathcal{S}_t$ and $ mathcal{H}$ | Evaluate the most promising query point $x_t= arg max mathcal{A}_t(x)$ | Train the expensive Machine Learning model using hyperparameters $x_t$ and compute its score $y_t=f(x_t)$ | Append the result $(x_t,y_t)$ to $ mathcal{H}$ | . 2.2. Acquisition functions . Acquisition function is a very important component of SMBO, it defines the strategy to select the next set of hyperparameters to be used in training Machine Learning model. Acqusition functions control the balance between exploitation and exploration. In this section, we learn about 4 most popular options, using these notations: . $x^ star,y^ star$ are the best hyperparameters so far in $ mathcal{H}$ and the associated model score | $ varphi( cdot)$ and $ psi( cdot)$ indicate the PDF and CDF, respectively | $ mu_y$ and $ sigma_y$ indicate the mean and standard deviation of model score according to the surrogate model | . Probability of Improvement . The idea of PI is very simple, it measures, at each unused configuration $x$, the probability that the corresponding score $y$ is higher than the current best score $y^ star$. A small positive offset $ epsilon$ is added to maintain the trade-off between exploration and exploitation. . $$ text{PI}= text{Prob }(y&gt;y^ star+ epsilon)$$ . The intuition behind $ epsilon$ is all about uncertainty. For certain areas, the distribution of model scores are clustered around the mean, so that only a small $ epsilon$ penalizes their PI a lot. For this reason, higher values of $ epsilon$ favor exploration; but if too high, the behaviour will be a lot like Active Learning. A balanced $ epsilon$ will make SMBO explore just enough and spend trials effectively on exploitation. . import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt; plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) import seaborn as sns from sklearn.gaussian_process import GaussianProcessRegressor %config InlineBackend.figure_format = &#39;retina&#39; . f = lambda x: 0.009*x**5 + 0.06*x**4 - 0.03*x**3 - 0.72*x**2 - 0.75*x + 1.8 np.random.seed(7) xTrain = np.array([-4, 0.5, 1.5, 3]) yTrain = f(xTrain) xTest = np.linspace(-5, 3.5, 100) yTest = f(xTest) . algo = GaussianProcessRegressor() model = algo.fit(xTrain.reshape(-1,1), yTrain) meanPred, sigmaPred = model.predict(xTest.reshape(-1,1), return_std=True) lower, upper = meanPred - sigmaPred, meanPred + sigmaPred . eps = 0.5 yStar = yTrain.max() yThres = yStar + eps fig, ax = plt.subplots(figsize=(12,6)) ax.plot(xTest, yTest, &#39;indianred&#39;, label=&#39;Ground Truth&#39;) ax.plot(xTest, meanPred, color=&#39;grey&#39;, label=&#39;Surrogate Mean&#39;) ax.plot(xTrain, yTrain, &#39;ok&#39;, label=&#39;Trials Done&#39;) ax.axhline(yTrain.max(), ls=&#39;--&#39;, lw=1, c=&#39;grey&#39;) ax.axhline(yTrain.max()+eps, ls=&#39;--&#39;, lw=1, c=&#39;grey&#39;) yStar = yTrain.max() yThres = yStar + eps ax.text(ax.get_xlim()[1]*1.02, yStar, &#39;Best score&#39;, va=&#39;center&#39;, ha=&#39;left&#39;) ax.text(ax.get_xlim()[1]*1.02, yThres, &#39;Best score + Offset&#39;, va=&#39;center&#39;, ha=&#39;left&#39;) for idx, xQuery in enumerate([-0.7, 0.2]): xQuery = np.array(xQuery) muQuery, sigmaQuery = model.predict(xQuery.reshape(-1,1), return_std=True) muQuery, sigmaQuery = muQuery[0], sigmaQuery[0] distQuery = stats.norm(muQuery, sigmaQuery) lowerQuery, upperQuery = muQuery - 3*sigmaQuery, muQuery + 3*sigmaQuery yRange = np.linspace(lowerQuery, upperQuery, 100) yPlot = yRange[yRange &lt; yThres] xPlot = xQuery + distQuery.pdf(yPlot) yPI = yRange[yRange &gt;= yThres] xPI = xQuery + distQuery.pdf(yPI) ax.axvline(xQuery, ls=&#39;--&#39;, lw=1, c=&#39;grey&#39;) if idx == 0: ax.fill_betweenx(yPlot, xPlot, xQuery, color=&#39;grey&#39;, alpha=0.1, label=&#39;Surrogate Distribution&#39;) ax.fill_betweenx(yPI, xPI, xQuery, color=&#39;teal&#39;, alpha=0.2, label=&#39;Probability of Improvement&#39;) else: ax.fill_betweenx(yPlot, xPlot, xQuery, color=&#39;grey&#39;, alpha=0.1) ax.fill_betweenx(yPI, xPI, xQuery, color=&#39;teal&#39;, alpha=0.2) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.legend() ax.grid(False) ax.tick_params(axis=&#39;both&#39;, bottom=False, left=False, labelbottom=False, labelleft=False) ax.set_xlabel(&#39;Hyperparameter&#39;) ax.set_ylabel(&#39;Model Score&#39;) plt.show() . Expected Improvement . As PI only considers how likely to improve, we need EI to answer how much the improvement can be. We simply measure the expected value of improvement $y-y^ star$, or more percisely, $ max(y-y^ star- epsilon,0)$. In the later formula, we are clipping negative improvements at zero. An offset $ epsilon$ is also added with the same intuition as in Probability of Improvement. . $$ text{EI}= int_{- infty}^{ infty} {(y-y^ star- epsilon) , varphi(y) , text{d}y}$$ . As EI and PI overcome the limitation of the other, we can combine them: $ text{PI}+ lambda text{EI}$ to get a new acquisition function, PI-EI. . Upper Confidence Bound . This acquisition function is a linear combination of the surrogate mean $ mu_y$ and the surrogate uncertainty $ sigma_y$. A multiplier $ lambda$ is attached to $ sigma_y$, it also favors exploration. . $$ text{UCB}= mu_y+ lambda sigma_y$$ . Thomson Sampling . As its name states, this method randomly samples a function that follows the surrogate distribution at each trial. Such a stochastic behaviour is non-deterministic in short-term but can converge in long-term as it is naturally exploration-exploitation balance. . 2.3. Implementation . 3. Parzen Estimators . Parzen-window Estimation is another name of Kernel Density Estimation which is usually included in histograms. Tree-structured Parzen Estimators (TPE) is an algorithm of SMBO family and is proposed in the same paper with BO. TPE estimates the likelihood $ text{Pr }(x|y)$ rather than the posterior $ text{Pr }(y|x)$ as in BO. . 3.1. Algorithm . Input: . A domain, or search space of hyperparameters | $f$, the black-box function to be optimized, it maps hyperpameters to model score: $y=f(x)$ and is very expensive to evaluate | $T$, the number of trials budget | . Step 1: Warm up several trials using Random Search. The observed scores are splited into two groups: low and high score, denoted $ mathcal{L}$ and $ mathcal{H}$. The high score group is usually 10-25% number of observed trials. . Step 2: Construct KDEs for low-score and high-score groups. They serve as surrogate models representing the likelihoods $ text{Pr }(x|y in mathcal{L})$ and $ text{Pr }(x|y in mathcal{H})$, respectively. We would probably want our next trial is more likely to be in $ mathcal{H}$ and less likely to be in $ mathcal{L}$. . Step 3: Select the acqusition function: $ mathcal{A}(x)= text{Pr }(x|y in mathcal{H}) div text{Pr }(x|y in mathcal{L})$. Selecting $x= arg max mathcal{A}(x)$ ends up getting the same result as maximizing EI in Bayesian Optimization, but this method considers the top best trials rather than only the best trial. Repeat step 2 and 3 until the budget $T$ elapses. . 3.2. Implementation . 4. HyperBand . 5. Implementation . References . papers.nips.cc - Algorithms for Hyper-parameters Optimization | arxiv.org - Non-stochastic Best Arm Identification and Hyperparameter Optimization | arxiv.org - Hyperband: Bandit-based configuration evaluation for hyperparameter optimization | arxiv.org - BOHB: Robust and Efficient Hyperparameter Optimization at Scale | medium.com - A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning | medium.com - Hyper-parameter optimization algorithms: a short review | krasserm.github.io - Bayesian Optimization | distill.pub - Exploring Bayesian Optimization | borealisai.com - Tutorial #8: Bayesian optimization | neupy.com - Hyperparmeter optimization for Neural Networks | ekamperi.github.io - Acquisition functions | . . &#9829; By Quang Hung x Thuy Linh &#9829; .",
            "url": "https://hungpq7.github.io/data-science-blog/2022/08/24/hyperparameter-optimization.html",
            "relUrl": "/2022/08/24/hyperparameter-optimization.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Ensemble Learning",
            "content": "1. Overview . Classical Machine Learning algorithms are usually shown to be poor when handling real-world datasets. Models fit from these algorithms often suffer from two problems: high bias and high variance; such a model is called a weak learner. In this topic, we are going through some elegant techniques that combine multiple weak learners to form a powerful model, which produces an improved overall result. This is referred to generally as Ensemble Learning. Enemble Learning methods have proven their effectiveness in many Machine Learing competitions. . 1.1. Big picture . Stacking (Wolpert, 1992) Voting | Stacking | Blending | . | Bagging RandomForest (Breiman, 1995) | ExtraTrees (Geurts, 2006) | . | Boosting Adaptive Boosting AdaBoost (Freund and Schapire, 1995) | . | Gradient Boosting GBDT (Friedman, 2001) XGBoost (Chen and Guestrin, 02/2014) | LightGBM (Guolin, 08/2016) | CatBoost (Yandex, 07/2017) | NGBoost (Duan and Avati, 06/2018) | . | . | . | . 1.2. Bias-variance analysis . 2. Stacking . 2.1. Voting . Voting (for classification) or Averaging (for regression) is the simplest ensembling method. When doing voting for classification, there are two strategies can be applied: marjority voting on predicted results (hard voting) and taking argmax of the weighted average of predicted probabilities (soft voting). . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import VotingClassifier, VotingRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . clf1 = SVC(probability=True) clf2 = LogisticRegression(solver=&#39;liblinear&#39;) clf3 = DecisionTreeClassifier() modelsBase = [clf1, clf2, clf3] modelsBaseNamed = [(model.__class__.__name__, model) for model in modelsBase] ensembler = VotingClassifier(modelsBaseNamed, voting=&#39;soft&#39;) . for model in modelsBase + [ensembler]: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9464 [SVC] AUC = 0.9931 [LogisticRegression] AUC = 0.8512 [DecisionTreeClassifier] AUC = 0.9851 [VotingClassifier] . 2.2. Stacking . Stacking technique organizes its members into two levels: . Level 1, a number of base models is fit to ther dataset. Build a new dataset where the values predicted by base models are input variables while the output variable remains the same. | Level 2, a meta model is train on the new dataset to get final prediction. | . The idea behind stacking is that each base model has an unique approach, it might discover some parts of the ground truth that other models do hot have. Combining them might utilize the their strengths and thus improve the overall quality. Note that Voting is a special case of Stacking, where the final combiner is a very simple model. . In the implementation of Stacking, the base models are often selected heterogeneously, and the meta model is often a simple Logistic Regression model. . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import StackingClassifier, StackingRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . clf1 = KNeighborsClassifier() clf2 = GaussianNB() clf3 = SVC(probability=True) clf4 = LogisticRegression(solver=&#39;liblinear&#39;) clf5 = DecisionTreeClassifier() modelsBase = [clf1, clf2, clf3, clf4, clf5] modelsBaseNamed = [(model.__class__.__name__, model) for model in modelsBase] modelMeta = LogisticRegression() ensembler = StackingClassifier(modelsBaseNamed, modelMeta) . for model in modelsBase + [ensembler]: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9568 [KNeighborsClassifier] AUC = 0.9775 [GaussianNB] AUC = 0.9464 [SVC] AUC = 0.9931 [LogisticRegression] AUC = 0.8442 [DecisionTreeClassifier] AUC = 0.9891 [StackingClassifier] . 2.3. Blending . 3. Bagging . Bootstrap Aggregating (Bagging) uses averaging/voting method over a number of homogeneous weak models in order to reduce variance. Specifically, Bagging is divided into two parts: bootstrapping and aggregating. . Boostrapping: The entire dataset is performed random sampling with replacement on both rows and columns. This outputs a number of bootstraps where each of them is different from the others. | Aggregating: after boostrap samples are generated, they are fit into the weak learners. All the model results will be combined by averaging (for regression) or voting (for classification). | . A Bagging ensembler operates as a committee that outperforms any individual weak model. This wonderful effect - the wisdom of crowds - can be explained that weak models protect each other from their individual errors. If the members share the same behaviors, they also make the same mistakes. Therefore, the low correlation between weak models is the key. Note that the Bagging method requires the initial sample to be large enough for the bootstrapping step to be statistical significant. . 3.1. Random Forest . Random Forest is the implementation of Bagging method on Decision Trees. It can be easily parallelized, does not requires too much hyperparameters tuning and has a decent prediction power. Random Forest is a very popular algorithm, before Boosting methods take the crown. . RandomForestClassifier and RandomForestRegressor classes have the following Bagging hyperparameters (ones inherited from Decision Tree are not mentioned): . n_estimators: the number of trees in the forest, defaults to 100. Control the complexity of the algorithm. Try increasing this when the model is underfitting, but it will take a longer training time. | max_features: the ratio of features used in each tree, defaults to auto (square root of nFeature). A lower value increases bias and reduces variance. | max_samples: the ratio of instances used in each tree, defaults to None (100% of nSample). A lower value increases bias and reduces variance. | . 3.2. Extra Trees . Besides Random Forest, Sickit-learn also develops a quite similar algorithm, Extremely Randomized Trees (Extra Trees for short). Instead of finding the split with highest information gain at each step, this method goes one step further in randomness by selecting the best candidate among a number of randomly-generated cut points. . The ExtraTreesClassifier and ExtraTreesRegressor classes have same hyperparameters as in Random Forest, there are only some small differences in their default values. . 3.3. Implementation . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.linear_model import LogisticRegression from sklearn.ensemble import BaggingClassifier, BaggingRegressor from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . modelBase = LogisticRegression(solver=&#39;liblinear&#39;, class_weight={0:1, 1:10}) ensembler = BaggingClassifier(modelBase, 10, random_state=7) model1 = RandomForestClassifier(n_estimators=20, random_state=7) model2 = ExtraTreesClassifier(n_estimators=20, random_state=7) . models = [modelBase, ensembler, model1, model2] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9845 [LogisticRegression] AUC = 0.9897 [BaggingClassifier] AUC = 0.9688 [RandomForestClassifier] AUC = 0.9749 [ExtraTreesClassifier] . 4. Boosting . Boosting works in the same spirit as Bagging: it also build a group of homogeneous models to obtain a more powerful predictor. The difference is that Boosting trains weak models sequentially while Bagging perform the training independently. The idea behind Boosting is to fit models iteratively such that the training of each model depends on the previous ones. Using this strategy, badly handled observations in the earlier steps will be taken care better in the later steps. Since the Boosting method puts its efforts on important cases, we end up have a strong learner with lower bias. . In many competitions, Boosting methods used on Decision Trees are so effective for tabular datasets and is widely used by top competitors. For the rest of this article, we will take a deep dive into a bunch of interesting Boosting algorithms. To start off, let&#39;s take a quick overview of two Boosting approaches, Adaptive Boosting and Gradient Boosting. They do both train trees consequently, but behave differently. We first denote: $ eta$ - the learning rate, $T$ - the number of iterations, $f^{(t)}$ and $f^{(t)}( mathbf{X})$ - the tree number $t$ and its predicted value for $t=1,2, dots,T$. . Adaptive Boosting . Train $f^{(1)}$ | Train $f^{(2)}$ base on $f^{(1)}$ | Train $f^{(3)}$ base on $f^{(2)}$ | ... | Train $f^{(T)}$ base on $f^{(T-1)}$ | Scale each tree by a coefficient $ eta$ and predict $ hat{ mathbf{y}} leftarrow eta f^{(1)}( mathbf{X})+ eta f^{(2)}( mathbf{X})+ dots+ eta f^{(T)}( mathbf{X})$ | . Gradient Boosting . Intialize $ hat{ mathbf{y}}^{(0)}$ | Train $f^{(1)}$ base on $ hat{ mathbf{y}}^{(0)}$ and compute $ hat{ mathbf{y}}^{(1)}= hat{ mathbf{y}}^{(0)}+ eta f^{(1)}( mathbf{X})$ | Train $f^{(2)}$ base on $ hat{ mathbf{y}}^{(1)}$ and compute $ hat{ mathbf{y}}^{(2)}= hat{ mathbf{y}}^{(1)}+ eta f^{(2)}( mathbf{X})$ | ... | Train $f^{(T)}$ base on $ hat{ mathbf{y}}^{(T-1)}$ and compute $ hat{ mathbf{y}}^{(T)}= hat{ mathbf{y}}^{(T-1)}+ eta f^{(T)}( mathbf{X})$ | Predict $ hat{ mathbf{y}} leftarrow hat{ mathbf{y}}^{(T)}$ | . 4.1. Adaptive Boosting . Adaptive Boosting was originally designed for binary classification problems. This method can be used to boost any algorithm, but Decision Tree is always the go-to choice. More specifically, Decision Trees used here are very shallow, they only have one root and two leaves, explaining why they are also called Decision Stumps. . Algorithm . Input: . A dataset having $N$ observations $( mathbf{X}, mathbf{y})= {( mathbf{s}_n,y_n) }_{n=1}^N$ where $y_n in {-1,1 }$ | The number of weak models, $T$ | The learning rate, $ eta$ | . Step 1. Initialize the weight for each observation: $w_n^{(1)}=1/N$. . Step 2. For each iteration number $t$ where $t=1,2, dots,T$: . Train a weak model $f^{(t)}$ that minimizes the sum of weights over misclassifications, represented by the error: . $$ epsilon^{(t)}= sum_{n=1}^{N}{w_n^{(t)} left[ hat{y}_n neq y_n right]}$$ . | Calculate $ alpha^{(t)}$ the amount of say for the current weak classifier; deciding how much $f^{(t)}$ will contribute in the final prediction. This calculation rewards $f^{(t)}$ a very high influence if its total error is low and penalizes $f^{(t)}$ a negative influence for a high total error. . $$ alpha^{(t)}= frac{ eta}{2} , log{ frac{1- epsilon^{(t)}}{ epsilon^{(t)}}}$$ . | Update sample weights for the next iteration so that: the weights of the correctly classied samples decrease $ exp{( alpha^{(t)})}$ times and the weights of misclassifications increase the same amount. Notice that the term $- hat{y}_n y_n$ equals to $1$ if the prediction is correct and equals to $-1$ if the prediction is incorrect. . $$w_n^{(t+1)}=w_n^{(t)} exp{ left(- hat{y}_n y_n alpha^{(t)} right)}$$ . | Normalize new weights so that they add up to $1$. This step is required to make the calculation of $ alpha^{(t+1)}$ meaningful. At this step, some implementations resample the dataset so that the distribution of observations follows the newly calculated weights. . | . Step 3. Build an additive strong model that performs weighted voting over $T$ weak learners; this is model outputs the prediction of the algorithm, $ hat{ mathbf{y}}$. The formula uses the notation $ text{sign}( bullet)$, indicating the sign function. . $$ hat{ mathbf{y}}= text{sign} left( sum_{t=1}^T alpha^{(t)} f^{(t)}( mathbf{X}) right)$$ . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . eta = 0.2 x = np.linspace(0.01, 0.99, 1000) y = eta * 1/2 * np.log(1/x - 1) fig, ax = plt.subplots(figsize=(5,5)) ax.plot(x, y, &#39;k&#39;, label=fr&#39;Learning rate = {eta}&#39;) ax.axis(&#39;scaled&#39;) ax.set_xlabel(r&#39;Error, $ epsilon_t$&#39;) ax.set_ylabel(r&#39;Influence, $ alpha_t$&#39;) ax.legend() plt.show() . Implementation . The classes AdaBoostClassifier and AdaBoostRegressor have the following Boosting hyperparameters: . base_estimator: the algorithm to be boosted, defaults to None (Decision Tree with max depth of 1). | n_estimators: the number of boosting stages ($T$), defaults to 50. | learning_rate: the learning rate ($ eta$), defaults to 1. | . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . modelBase = DecisionTreeClassifier(max_depth=1) ensembler = AdaBoostClassifier(modelBase, n_estimators=50, learning_rate=1) . models = [ensembler] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9828 [AdaBoostClassifier] . 4.2 Gradient Boosting . Grandient Boosting is another boosting strategy beside Adaptive Boosting. The idea of this method is mostly inspired by Gradient Descent, thus the name Gradient Boosting. Just like other ensembling methods, this algorithm works best on Decision Trees and becomes the foundation for its modern variants such as XGBoost, LightGBM and CatBoost. . Gradient Boosting Trees was originally designed for regression problems. For classification, we use regression approach to predict log of the odds. . Algorithm . Input: . A dataset having $N$ labeled observations $( mathbf{X}, mathbf{y})= {( mathbf{s}_n,y_n) }_{n=1}^N$ | The number of weak models, $T$ | The learning rate, $ eta$ | A differentiable loss function $ mathcal{L}( hat{ mathbf{y}})$ (squared error is a popular choice) | . Step 1. Initialize the prediction as a constant. Since this is the very first prediction and will be updated step-by-step, we denote this value $ hat{ mathbf{y}}^{(0)}$. When the loss function is MSE, this value is nothing but the mean of $ mathbf{y}$. . $$ hat{ mathbf{y}}^{(0)}= arg min sum_{n=1}^N mathcal{L}( hat{y}_n)$$ . Step 2. For $t=1$ to $T$: . Compute the psuedo-residual $r_n^{(t)}$ equals to the negative gradient of the loss function with respect to the prediction of the iteration $t-1$. When MSE is used, this term is proportional to the actual residual, $y_n- hat{y}_n^{(t-1)}$. In general, we call it pseudo-residual which allows plugging in different loss functions. . $$r_n^{(t)}=-g_n^{(t)}=- frac{ partial mathcal{L}( hat{y}_n^{(t-1)})}{ partial hat{y}_n^{(t-1)}}$$ . | Fit a weak learner (regression tree) $f^{(t)}$ using the training set $( mathbf{X}, mathbf{r}^{(t)})$. This step results in a tree with $M$ leaf nodes; meaning the input space is split into $M$ disjoint regions, each region is denoted $R_m ;(m=1,2, dots,M)$. Trees in this step are not restricted to be stumps as in AdaBoost. Compute $f^{(t)}( mathbf{X})$, the predicted value for the model $f^{(t)}$ so that it minimizes the loss function at the current step. . $$f^{(t)}( mathbf{X})= underset{f}{ arg min} sum_{n=1}^{N}{ mathcal{L} left( hat{y}_n^{(t-1)}+f( mathbf{s}_n) right)}$$ . | Use first-order Taylor approximation: $f(x=a) approx f(a)+f&#39;(a)(x-a)$ to estimate the loss function evaluated at step $t-1$. Here, $x$ corresponds to $ hat{y}_n^{(t-1)}+f( mathbf{s}_n)$ and $a$ corresponds to $ hat{y}_n^{(t-1)}$. Using the notation $g_n^{(t)}$ defined earlier, we have: $ mathcal{L} left( hat{y}_n^{(t-1)}+f( mathbf{s}_n) right) approx mathcal{L} left( hat{y}_n^{(t-1)} right)+g_n^{(t)}f( mathbf{s}_n)$. We can prove that $f^{(t)}( mathbf{X})$ is proportional to the negative gradient as follows. When MSE is chosen, is simply computes the average residual in each leaf. . $$ begin{align} f^{(t)}( mathbf{X}) &amp; approx underset{f}{ arg min} sum_{n=1}^{N} mathcal{L} left( hat{y}_n^{(t-1)} right)+g_n^{(t)}f( mathbf{s}_n) &amp;= underset{f}{ arg min} sum_{n=1}^{N}g_n^{(t)}f( mathbf{s}_n) propto -g_n^{(t)} end{align}$$ . | Compute the predicted value up to the current step, $ hat{ mathbf{y}}^{(t)}$. Since we are adding negative gradient $-g_n^{(t-1)}$ scaled by the learning rate $ eta$ step-by-step, this can be considered a Gradient Descent process. . $$ hat{ mathbf{y}}^{(t)}= hat{ mathbf{y}}^{(t-1)}+ eta f^{(t)}( mathbf{X})$$ . | . Step 3. Take the last round&#39;s predicted value as the final prediction: $ hat{ mathbf{y}} leftarrow hat{ mathbf{y}}^{(T)}$. Note that in Gradient Boosting, the prediction at each iteration $ hat{ mathbf{y}}^{(t)}$ has taken into account all weak learners up to the current step. This behaviour is not like Adaptive Boosting, in which the strong model is only built once all weak leaners was trained successfully. . Implementation . The GradientBoostingClassifier and GradientBoostingRegressor classes are the original impelementation of Gradient Boosting Trees. After a while, HistGradientBoostingClassifier and HistGradientBoostingRegressor which use histogram-based split finding were introduced. The later implementation is significantly faster for big datasets. The common hyperparameters of these algorithms are: . loss: the type of loss function, defaults to deviance (classification) and squared_error (regression). | n_estimators: the number of boosting stages ($T$), defaults to 100. | learning_rate: the learning rate ($ eta$), defaults to 0.1. | max_features: the ratio of features used in each tree, defaults to auto (square root of nFeature). A lower value increases bias and reduces variance. | subsample: the ratio of instances used in each tree, defaults to 1 (100% of nSample). A lower value increases bias and reduces variance. | criterion: the measure of quality of splits, defaults to friedman_mse. | . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . model1 = GradientBoostingClassifier(random_state=7) model2 = HistGradientBoostingClassifier(random_state=7) models = [model1, model2] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9729 [GradientBoostingClassifier] AUC = 0.9831 [HistGradientBoostingClassifier] . . &#9829; By Quang Hung x Thuy Linh &#9829; .",
            "url": "https://hungpq7.github.io/data-science-blog/jupyter/machine-learning/ensemble-learning/decision-tree/bagging/boosting/stacking/2022/08/24/ensemble-learning.html",
            "relUrl": "/jupyter/machine-learning/ensemble-learning/decision-tree/bagging/boosting/stacking/2022/08/24/ensemble-learning.html",
            "date": " • Aug 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hungpq7.github.io/data-science-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hungpq7.github.io/data-science-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}