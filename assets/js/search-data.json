{
  
    
        "post0": {
            "title": "Interactive Visualization",
            "content": "1. Introduction . Plotly is an Python library interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases. . Plotly has two important submodules, plotly.express and plotly.graph_objects, but this topic focuses on plotly.express only. It allows quickly creating more than 30 types of charts, each made only in a single function call. . Useful scripts . To display interactive Plotly charts in Jupyter Lab, it is required to install an extension. . jupyter labextension install jupyterlab-plotly@4.12.0 . To export a chart to a .html file, use the script below in which fig is the name of the graph object. . import plotly plotly.offline.plot(fig, filename=&#39;My interactive figure.html&#39;) . 2. Basic charts . 2.1. Pie Chart . import pandas as pd import numpy as np import plotly.express as px import plotly.graph_objects as go from IPython.display import HTML, display # from plotly.offline import init_notebook_mode, iplot # init_notebook_mode(connected = True) . dfTip = px.data.tips() dfTip.head() . fig = px.pie(dfTip, names=&#39;day&#39;, values=&#39;tip&#39;, template=&#39;plotly&#39;) display(HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;))) . fig = px.pie(dfTip, names=&#39;day&#39;, values=&#39;tip&#39;, template=&#39;plotly&#39;) fig.show() .",
            "url": "https://hungpq7.github.io/data-science-blog/jupyter/visualization/2022/02/20/plotly.html",
            "relUrl": "/jupyter/visualization/2022/02/20/plotly.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hungpq7.github.io/data-science-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Graph Construction",
            "content": "1. Matplotlib API . Matplotlib is a famous visualization library in Python, is the foundation for many other visualization libraries. Matplotlib along with NumPy can be considered equivalent to Matlab. . This topic focuses only on Pyplot - the most essential sub-module of Matplotlib and how to use it to build graphs of mathematical functions. For data visualization plots such as bar chart, histogram or pie chart, there are other libraries that do a better job than Matplotlib, and they will be discussed on later topics. . 1.1. Plotting interfaces . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Object-oriented interface . Every plot created by Matplotlib is under the control of two objects, figure and axes. . An figure object is the whole image generated by Matplotlib, where everything is drawn on. It is a top-level object and works as a container for a number of axes objects. | Each axes object usually refers to a 2-dimensional Cartesian coordinate system. It gives access to plot elements such as plots, labels, titles, text. | . fig = plt.figure(figsize=(4,3)) # add an axes with some text ax = fig.add_subplot() ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . Instead of creating figure and axes independently, Matplotlib provides a single function subplots() creates the two objects at once. This function is highly recommended in practice, and the introduction to the previous method is to clarify how figure and axes work and how they are related. . fig, ax = plt.subplots(figsize=(4,3)) ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . When there are more than one axes, Matplotlib arranges them in a matrix of axes objects. Accessing each axes can be done using NumPy&#39;s array slicing. . fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,6), sharex=True, sharey=True) ax[0,2].text(0.5, 0.5, &#39;Row = 0 nColumn = 2&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) ax[1,1].text(0.5, 0.5, &#39;Row = 1 nColumn = 1&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . State-machine interface . Besides the object-oriented interface, Matplotlib also provides another way that makes use of state-machine to create plots. When using this, the state-machine implicitly and automatically creates figures and axes to achieve the desired plot. Then a set of simple functions is used to add plot elements to the current axes in the current figure. . Compared to object-oriented, state-machine interface is a bit more convenient for making a single axes, but it is not recommended for arranging multiple axes. Overall, object-oriented interface is still the go-to method. . plt.text(0.5, 0.5, &#39;Data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=32) plt.axvline(x=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.axhline(y=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.title(&#39;State-machine interface&#39;) plt.show() . 1.2. Controlling axes . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Equalizing axes . Graphs require the two axes to have the same scale. . fig, ax = plt.subplots() # make the two axes scaled ax.axis(&#39;scaled&#39;) # set tick frequencies of both axes to be 0.02 ax.xaxis.set_major_locator(plt.MultipleLocator(0.02)) ax.yaxis.set_major_locator(plt.MultipleLocator(0.02)) plt.show() . Axes limits . fig, ax = plt.subplots() ax.axis(&#39;scaled&#39;) # set limit for each axis ax.set_xlim(0, 2) ax.set_ylim(0, 1) ax.xaxis.set_major_locator(plt.MultipleLocator(0.2)) plt.show() . Formatting axes . def axis_formatter(value, tick): N = int(np.round(2 * value / np.pi)) if N == 0: return &quot;0&quot; elif N == 2: return r&quot;$ pi$&quot; elif N == -2: return r&#39;$- pi$&#39; elif N % 2 == 1 and N &gt; 0: return fr&quot;$ frac{{{N}}}{{2}} pi$&quot; elif N % 2 == 1 and N &lt; 0: return fr&quot;$- frac{{{-N}}}{{2}} pi$&quot; else: return fr&quot;${N//2} pi$&quot; . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(10,10)) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) # set x-tick frequency to be pi/2 and apply a custom format strategy ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi/2)) ax.xaxis.set_major_formatter(plt.FuncFormatter(axis_formatter)) ax.set_ylim(-2.5, 2.5) plt.show() . 1.3. Plotting . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Graphs . Matplotlib does not really have a way to make graphs, but this can be achieved indirectly by using the axes.plot() method. The original functionality of this function is to plot a polyline connects data points. . It has an optional parameter, fmt that defines basic formatting following the syntax: &#39;{marker}{linestyle}{color}&#39;. The formatting string must not strictly follow the order in the syntax, but note that the parsing may be ambiguous. The table below summarizes some useful formatting strings: . Parameter Character Meaning . marker | &#39;.&#39; | point marker | . marker | &#39;o&#39; | big point marker | . linestyle | &#39;-&#39; | solid line style | . linestyle | &#39;--&#39; | dashed line style | . linestyle | &#39;:&#39; | dotted line style | . linestyle | &#39;-.&#39; | dash-dot line style | . color | &#39;k&#39; | black | . color | &#39;r&#39; | red | . color | &#39;c&#39; | cyan | . color | &#39;m&#39; | magenta | . color | &#39;g&#39; | green | . With different combinations of formatting strings, axes.plot() can result in graphs (indirectly) and points (directly). In order to make graphs, the input for x-axis needs to be a dense set of values. . x1 = np.linspace(-5, 10, 1000) y1 = np.sin(x1) + np.sin(2*x1) x2 = np.array([0, 4, 5.5, -4]) y2 = np.array([1, 2, 0, -2]) fig, ax = plt.subplots(figsize=(10,4)) ax.plot(x1, y1, &#39;k&#39;) ax.plot(x2, y2, &#39;oc&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Spans . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(12,4)) # add two lines represent the two axes ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # add a vertical span across y-axis ax.axvspan(xmin=4.5, xmax=6.5, alpha=0.3) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Vectors . The axes.quiver() method in Matplotlib allows plotting vectors. It has five important parameters, which must have the same length: . X and Y locate the origins of vectors | U and V define the vectors | color sets vector colors | . For some reasons, the parameters units=&#39;xy&#39; and scale=1 must always be set in order to draw vectors correctly. . fig, ax = plt.subplots() ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.quiver( [1, 0, 2], [1, 0, 0], [1, -3, -4], [2, 1, -2], color=[&#39;firebrick&#39;, &#39;teal&#39;, &#39;seagreen&#39;], units=&#39;xy&#39;, scale=1 ) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4,3) ax.set_ylim(-3,4) plt.show() . 1.4. Annotations . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Text positioning . The axes.text() method adds text to the axes at the location defined by two parameters, x and y, use true coordinate values by default. By setting transform=ax.transAxes (highly recommended), these two parameters are now on a relative coordinates where (0,0) is the lower-left corner and (1,1) is the upper-right corner. . There is also a pair of parameters, ha and va (horizontal/vertical alignment), specify the relative position of the coordinates to the text box. Possible values for these parameters are left right center and top bottom center, respectively. . fig, ax = plt.subplots(figsize=(5,5)) ax.text(0.5, 0.5, &#39;(0.5, 0.5) ncenter center&#39;, va=&#39;center&#39;, ha=&#39;center&#39;, size=14, transform=ax.transAxes) ax.text(0, 0, &#39;(0, 0) nbottom left&#39;, va=&#39;bottom&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 1, &#39;(1, 1) ntop right&#39;, va=&#39;top&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.text(0, 1, &#39;(0, 1) ntop left&#39;, va=&#39;top&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 0, &#39;(1, 0) nbottom right&#39;, va=&#39;bottom&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.yaxis.set_major_formatter(plt.NullFormatter()) plt.show() . Plot titles . x = np.linspace(-1, 8, 1000) fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(5,7)) fig.suptitle(&#39;Mathematics&#39;, fontsize=16) ax[0].plot(x, np.sin(x)+2*np.sin(2*x), &#39;k&#39;) ax[0].set_title(&#39;$y= sin(x)+2 sin(x)$&#39;) ax[0].axis(&#39;equal&#39;) ax[0].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[0].yaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax[1].set_title(&#39;$y= sin(x2 pi) cdot e^{-x}$&#39;) ax[1].axis(&#39;scaled&#39;) ax[1].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].yaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Axis labels . x = np.linspace(-1, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) # set axis labels ax.set_xlabel(&#39;x-axis&#39;) ax.set_ylabel(&#39;y-axis&#39;) plt.show() . Legends . The axes.lengend() method takes a list of plot names, which is indexed the same as the number of axes.plot() methods called. It can also use the value of label parameter in each axes.plot() method to generate the legend. . By default, Matplotlib automatically determines the best location for the legend inside the plot. This can be changed using the parameter loc, which specifies locations using strings such as &quot;lower center&quot; and &quot;upper right&quot;. When there is no optimized location like in the example below, the legend should be placed outside of the axes. This can be achieved using bbox_to_anchor alongside with loc; this pair of parameters behaves exactly the same as text positioning. . Another useful configuration is the ncol parameter. It specifies the number of columns in the grid of legend labels. . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # pass the list of labels into the ax.legend method ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;, label=&#39;$ sin(x)$&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;, label=&#39;$ sin(2x)$&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;, label=&#39;$ sin(x)+ sin(2x)$&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend(bbox_to_anchor=(0.5, 1), loc=&#39;lower center&#39;, ncol=3, title=&#39;Graphs&#39;) plt.show() . 1.5. Themes and colors . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Themes . Matplotlib supports a variety of themes such as ggplot seaborn tableau-colorblind10 inspired by other popular visualization tools. However, since drawing graphs requires a white background, then I usually use seaborn-whitegrid. . plt.style.use(&#39;ggplot&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . plt.style.use(&#39;seaborn&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Built-in Matplotlib colors . Besides themes, Matplotlib also provides a collection of colors for better visualization. Some nice colors are: dimgrey indianred tomato goldenrod seagreen teal darkturquoise darkslategrey slategrey royalblue rebeccapurple. . . 2. Mathematical functions . In Mathematics, a function) (usually denoted $f$) is a process of transforming each given input number (denoted $x$) to exactly one output number (denoted $y$). This process can be in the form of a mathematical formula or some logical rules and can be expressed using the notation: . $$y=f(x)$$ . The set of inputs and the set of outputs are called the domain (denoted $X$) and codomain (denoted $Y$), consecutively. The function in this context is written as $f colon X to Y$. . The set of all pairs $(x,y)$, formally denoted $G= {(x,y) mid x in X }$ is called the graph of the function. It popularly means the illustration of the function with the condition $x,y in mathbb{R}$. . Another related concept to function is map), which is often used as a synonym of function. However, from now on in this series about Data Science, map refers to a generalization of function, which extends the scope of $x$ and $y$ not to be restricted to numbers only, but can also be other data-like objects such as strings and datetime. In reality, the more common meaning of the word map, Google Maps, for example, is actually made by mapping the Earth&#39;s surface to a sheet of paper. The notation for map is: . $$f colon x mapsto y$$ . 2.1. Elementary functions . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Polynomial functions . A polynomial is a function having the following form: . $$y=a_nx^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2}+ dots+a_1x+a_0$$ . where: . $n ;(n in mathbb{N})$ is the degree of the polynomial | $n,n-1,n-2, dots,0$ are the degrees of the corresponding monomial | $a_n,a_{n-1}, dots,a_0 ;(a_n neq0)$ are the coefficients | . Each polynomial has the domain of $x in mathbb{R}$ and the codomain of $y in mathbb{R}$. It has a maximum of $n$ solutions and a maximum of $n-1$ extrema. . Some popular polynomials are: . $f(x)=ax+b$, linear functions | $f(x)=ax^2+bx+c$, quadratic functions, which has the parabola shape | . x = np.linspace(-5, 4, 1000) y1 = x**2 + 4*x + 2 y2 = 1/4*x**3 + 3/4*x**2 - 3/2*x fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;Quadratic (degree 2)&#39;) ax.plot(x, y2, label=&#39;Cubic (degree 3)&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_ylim(-3, 6) ax.legend() plt.show() . Exponential functions . An exponential function is a function having $x$ in its power. The form of the function is: . $$y=b^x$$ . where $b ;(b&gt;0)$ is the base. . Exponential functions have the domain of $x in mathbb{R}$ and the codomain of $y in(0,+ infty)$. Exponential functions are monotonic, and can be either increasing or decreasing as the value of $b$ changes. All exponential functions go through the point $(0,1)$, since $b^0=1$ for any value of $b$. . Some popular exponential functions are: . $f(x)=2^x$, the foundation of binary system being used in almost all modern computers | $f(x)=10^x$, the foundation of decimal numeral system | $f(x)=e^x= exp{(x)}$, the natural) exponential function, the function equals to its own derivative | . x = np.linspace(-5, 5, 1000) y1 = 2**x y2 = np.e**x y3 = 10**x y4 = (1/2)**x fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y=2^x$&#39;) ax.plot(x, y2, label=&#39;$y=e^x$&#39;) ax.plot(x, y3, label=&#39;$y=10^x$&#39;) ax.plot(x, y4, label=&#39;$y=0.5^x$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-1, 7) ax.legend(bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Logarithm functions . Logarithm is the inverse function of exponentiation. With a given base $b ;(b&gt;0)$, logarithm functions have the following form: . $$y= log_b{x}$$ . The domain of the logarithm functions is $x in(0,+ infty)$. Popular logarithm functions are: . $f(x)= log_2{x}$, the binary logarithm | $f(x)= log_{10}{x}= lg{x}$, the common logarithm | $f(x)= log_e{x}= ln{x}= log{x}$, the natural logarithm | . Notice that in many Python libraries, the log function refers to the natural logarithm instead of base 10. . x = np.linspace(1e-6, 10, 1000) y1 = np.log2(x) y2 = np.log10(x) y3 = np.log(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= log_2{x}$&#39;) ax.plot(x, y2, label=&#39;$y= log_{10}{x}$&#39;) ax.plot(x, y3, label=&#39;$y= ln{x}$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(0, 9) ax.set_ylim(-3, 3) ax.legend() plt.show() . Power functions . Power functions are a family of functions having $x$ in their bases, they should not be confused with exponential functions. The most common type of power function being applied in Data Science is the ones with a natural exponent: . $$y=x^n$$ . where $n ;(n in mathbb{N})$ is the exponent. . The domain of power functions is $x in mathbb{R}$. If $n$ is even, the function is reflectional symmetric since $f(x)=f(-x)$; if $n$ is odd, the function is rotationally symmetric since $f(x)=-f(-x)$. Some popular named functions of this type are: . $f(x)=x^2$, the square function), applied in calculating the area | $f(x)=x^3$, the cube function), applied in calculating the volume | . x = np.linspace(-5, 5, 1000) y1 = x**2 y2 = x**5 fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y=x^2$&#39;) ax.plot(x, y2, label=&#39;$y=x^5$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-3, 3) ax.set_ylim(-4, 4) ax.legend() plt.show() . Root functions . Root functions are power functions having rational exponents: . $$y=x^{a/b}= sqrt[b]{x^a} ;(a,b in mathbb{N})$$ . The domain of root functions is $x geq0$ for even values of $b$ and is $x in mathbb{R}$ for odd values of $b$. Some popular root functions are: . $f(x)= sqrt{x}$, the square root function | $f(x)= sqrt[3]{x}$, the cube root function | . x1 = np.linspace(0, 5, 1000) y1 = np.sqrt(x1) x2 = np.linspace(-5, 5, 1000) y2 = np.cbrt(x2) fig, ax = plt.subplots() ax.plot(x1, y1, label=&#39;$y=x^2$&#39;) ax.plot(x2, y2, label=&#39;$y=x^5$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-2.5, 2.5) ax.legend() plt.show() . Reciprocal functions . Reciprocal function is a special case of power function, when the exponent is a negative rational number: . $$y=x^{-q}= frac{1}{x^q} ;(q in mathbb{Q})$$ . The domain of reciprocal functions is $x neq0$. They also have symmetric properties like natural power functions. The term reciprocal usually refers to the most common case, $f(x)=x^{-1}$, which has a beautiful property: $x cdot x^{-1}=1$. . x = np.linspace(-5, 5, 1000) y = 1/x fig, ax = plt.subplots() ax.plot(x, y, &#39;k&#39;, label=&#39;$y=x^{-1}$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-5, 5) ax.legend() plt.show() . Trigonometric functions . Three most used trigonometric functions in modern Mathematics are the sine ($y= sin{x}$), the cosine ($y= cos{x}$) and the tangent ($y= tan{x}$). They are defined on the unit circle $x^2+y^2=1$. Since all trigonometric functions are periodic, the visualization for them is limited to be in the domain $x in(-2 pi,2 pi)$. . x = np.linspace(-2*np.pi, 2*np.pi, 1000) y1 = np.sin(x) y2 = np.cos(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= sin(x)$&#39;) ax.plot(x, y2, label=&#39;$y= cos(x)$&#39;, c=&#39;indianred&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-2*np.pi, 2*np.pi) ax.set_ylim(-1.5, 1.5) ax.legend(bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Hyperbolic functions . Hyperbolic functions are trigonometric functions defined using the unit hyperbola $x^2-y^2=1$ rather than the circle. The formulas for the three most popular hyperbolic functions are: . $ displaystyle{ sinh{x}= frac{e^x-e^{-x}}{2}}$ . | $ displaystyle{ cosh{x}= frac{e^x+e^{-x}}{2}}$ . | $ displaystyle{ tanh{x}= frac{ sinh{x}}{ cosh{x}}= frac{e^x-e^{-x}}{e^x+e^{-x}}}$ . | . x = np.linspace(-5, 5, 1000) y1 = np.sinh(x) y2 = np.cosh(x) y3 = np.tanh(x) fig, ax = plt.subplots() ax.plot(x, y1, label=&#39;$y= sinh(x)$&#39;) ax.plot(x, y2, label=&#39;$y= cosh(x)$&#39;) ax.plot(x, y3, label=&#39;$y= tanh(x)$&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . 2.2. Function transformations . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Translation . Translation) refers to the process of shifting the entire graph to another position. Given a constant $c ;(c&gt;0)$, the formulas below show how $c$ moves the graph of $f(x)$: . $f(x)+c$ shifts the graph $c$ units up | $f(x)-c$ shifts the graph $c$ units down | $f(x+c)$ shifts the graph $c$ units to the left | $f(x-c)$ shifts the graph $c$ units to the right | . The whole process can also be represented using a matrix multiplication: . $$ begin{bmatrix} begin{array}{r}1 &amp; 0 &amp; c1 0 &amp; 1 &amp; c2 0 &amp; 0 &amp; 1 end{array} end{bmatrix} cdot begin{bmatrix}x y 1 end{bmatrix} = begin{bmatrix}x+c_1 y+c_2 1 end{bmatrix} $$ def f(x): y = x**2/2 - 2*x - 2 return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(x)+3, label=&#39;$f(x)+3$&#39;, c=&#39;steelblue&#39;) ax.plot(x, f(x-2), label=&#39;$f(x-2)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 4, 4], [-2, -2], [ 0, 2], [ 3, 0], color=[&#39;steelblue&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-3, 9) ax.set_ylim(-5, 5) ax.legend() plt.show() . Dilation . Dilation refers to the process of compressing/stretching the graph. Given a constant $c ;(c&gt;1)$, the formulas below show how $c$ resizes the graph of $f(x)$: . $c cdot f(x)$ stretches the graph $c$ times vertically (in the $y$-direction) | $ displaystyle{ frac{1}{c} cdot f(x)}$ compresses the graph $c$ times vertically (in the $y$-direction) | $f(cx)$ compress the graph $c$ times horizontally (in the $x$-direction) | $ displaystyle{f left( frac{x}{c} right)}$ stretches the graph $c$ times horizontally (in the $x$-direction) | . Note that resizing to a similar shape requires compressing/stretching the same times on both axes. This can be written in matrix form: . $$ begin{bmatrix}c&amp;0 0&amp;c end{bmatrix} cdot begin{bmatrix}x y end{bmatrix} = begin{bmatrix}cx cy end{bmatrix} $$ def f(x): y = 2**x return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(x)/4, label=&#39;$f(x)/4$&#39;, c=&#39;steelblue&#39;) ax.plot(x, f(x*2), label=&#39;$f(2x)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 2, 2], [ 4, 4], [ 0, -2], [-4, 0], color=[&#39;steelblue&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-5, 5) ax.set_ylim(-1, 8) ax.legend() plt.show() . Reflection . Reflection) considers a line as the axis of reflection to map the graph to its image. Here are reflection formulas: . $f(-x)$ results in the graph being reflected across the $y$-axis | $f-(x)$ results in the graph being reflected across the $x$-axis | . The refelection process can also be written as the dot product of a square matrix with the vector $ begin{bmatrix}x y end{bmatrix}$, where each matrix uses a different axis of reflection. For example, $ begin{bmatrix} begin{array}{r}1&amp;0 0&amp;-1 end{array} end{bmatrix}$ for a reflection over $x$-axis, $ begin{bmatrix} begin{array}{r}-1&amp;0 0&amp;1 end{array} end{bmatrix}$ for a reflection over $y$-axis and $ begin{bmatrix}0&amp;1 1&amp;0 end{bmatrix}$ for a reflection over the line $y=x$. . def f(x): y = 2**x return y x = np.linspace(-10, 10, 1000) fig, ax = plt.subplots() ax.plot(x, f(x), label=&#39;$f(x)$&#39;, c=&#39;black&#39;) ax.plot(x, f(-x), label=&#39;$f(-x)$&#39;, c=&#39;steelblue&#39;) ax.plot(x, -f(x), label=&#39;$-f(x)$&#39;, c=&#39;indianred&#39;) ax.quiver( [ 0, 0, 1, 1], [ 2, 2, 0, 0], [ 1, -1, 0, 0], [ 0, 0, 2, -2], color=[&#39;black&#39;, &#39;steelblue&#39;, &#39;black&#39;, &#39;indianred&#39;], scale_units=&#39;xy&#39;, angles=&#39;uv&#39;, scale=1, width=0.005, ) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . Rotation . Graph rotation actually means rotating points on the graph one by one. The rotated graph is no longer a function and cannot be represented in the form $y=f(x)$. The formula below shows how to rotate a point about the origin, counterclockwise and by an angle of $ theta$: . $$ begin{aligned} x&#39;=x cos theta-y sin theta y&#39;=x sin theta+y cos theta end{aligned}$$This beautiful formula can also be written using matrix notation as: . $$ begin{bmatrix}x&#39; y&#39; end{bmatrix}= begin{bmatrix} begin{array}{r} cos theta &amp;- sin theta sin theta &amp; cos theta end{array} end{bmatrix} begin{bmatrix}x y end{bmatrix} $$or using complex numbers: . $$ begin{aligned} x&#39;+iy&#39; &amp;= (x cos theta-y sin theta)+i(x sin theta+y cos theta) &amp;= ( cos theta+i sin theta)(x+iy) &amp;= e^{i theta}(x+iy) end{aligned}$$ def rotate(x, y, theta): x_rotated = x*np.cos(theta) - y*np.sin(theta) y_rotated = x*np.sin(theta) + y*np.cos(theta) return x_rotated, y_rotated x = np.linspace(-10, 10, 1000) y = x**2 - 3 theta = np.pi/2 x_rotated, y_rotated = rotate(x, y, theta) fig, ax = plt.subplots() ax.plot(x, y, label=&#39;Original graph&#39;, c=&#39;black&#39;) ax.plot(x_rotated, y_rotated, label=&#39;Rotated graph&#39;, c=&#39;steelblue&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.axhline(y=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4, 4) ax.set_ylim(-4, 4) ax.legend() plt.show() . . &#9829; By Quang Hung x Thuy Linh &#9829; . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hungpq7.github.io/data-science-blog/jupyter/2020/02/20/graph.html",
            "relUrl": "/jupyter/2020/02/20/graph.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hungpq7.github.io/data-science-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Principal Component Analysis (PCA) from Scratch",
            "content": ". Relevance . Principal Component Analysis (PCA) is a data-reduction technique that finds application in a wide variety of fields, including biology, sociology, physics, medicine, and audio processing. PCA may be used as a &quot;front end&quot; processing step that feeds into additional layers of machine learning, or it may be used by itself, for example when doing data visualization. It is so useful and ubiquitious that is is worth learning not only what it is for and what it is, but how to actually do it. . In this interactive worksheet, we work through how to perform PCA on a few different datasets, writing our own code as we go. . Other (better?) treatments . My treatment here was written several months after viewing... . the excellent demo page at setosa.io | this quick 1m30s video of a teapot, | this great StatsQuest video | this lecture from Andrew Ng&#39;s course | . Basic Idea . Put simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or &quot;features&quot;) you started with to a set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of a few of these &#39;components&#39; of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation. . Once you &quot;get it,&quot; you&#39;ll find PCA to be almost no big deal, if it weren&#39;t for the fact that it&#39;s so darn useful! . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset | Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) | Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance | Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space | (Check: How much did we lose by that truncation?) | Caveats . Since PCA will involve making linear transformations, there are some situations where PCA won&#39;t help but...pretty much it&#39;s handy enough that it&#39;s worth giving it a shot! . Covariance . If you&#39;ve got two data dimensions and they vary together, then they are co-variant. . Example: Two-dimensional data that&#39;s somewhat co-linear: . import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go N = 100 x = np.random.normal(size=N) y = 0.5*x + 0.2*(np.random.normal(size=N)) fig = go.Figure(data=[go.Scatter(x=x, y=y, mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . . . Variance . So, for just the $x$ component of the above data, there&#39;s some mean value (which in this case is zero), and there&#39;s some variance about this mean: technically the variance is the average of the squared differences from the mean. If you&#39;re familiar with the standard deviation, usually denoted by $ sigma$, the variance is just the square of the standard deviation. If $x$ had units of meters, the variance $ sigma^2$ would have units of meters^2. . Think of the variance as the &quot;spread,&quot; or &quot;extent&quot; of the data, about some particular axis (or input, or &quot;feature&quot;). . Similarly we can look at the variance in just the $y$ component of the data. For the above data, the variances in $x$ and $y$ are . print(&quot;Variance in x =&quot;,np.var(x)) print(&quot;Variance in y =&quot;,np.var(y)) . Variance in x = 0.6470431671825421 Variance in y = 0.19318628312072175 . Covariance . You&#39;ll notice in the above graph that as $x$ varies, so does $y$ -- pretty much. So $y$ is &quot;covariant&quot; with $x$. &quot;Covariance indicates the level to which two variables vary together.&quot; To compute it, it&#39;s kind of like the regular variance, except that instead of squaring the deviation from the mean for one variable, we multiply the deviations for the two variables: . $${ rm Cov}(x,y) = {1 over N-1} sum_{j=1}^N (x_j- mu_x)(y_j- mu_j),$$ where $ mu_x$ and $ mu_y$ are the means for the x- and y- componenets of the data, respectively. Note that you can reverse $x$ and $y$ and get the same result, and the covariance of a variable with itself is just the regular variance -- but with one caveat! . The caveat is that we&#39;re dividing by $N-1$ instead of $N$, so unlike the regular variance we&#39;re not quite taking the mean. Why this? Well, for large datasets this makes essentially no difference, but for small numbers of data points, using $N$ can give values that tend to be a bit too small for most people&#39;s tastes, so the $N-1$ was introduced to &quot;reduce small sample bias.&quot; . In Python code, the covariance calculation looks like . def covariance(a,b): return ( (a - a.mean())*(b - b.mean()) ).sum() / (len(a)-1) print(&quot;Covariance of x &amp; y =&quot;,covariance(x,y)) print(&quot;Covariance of y &amp; x =&quot;,covariance(x,y)) print(&quot;Covariance of x with itself =&quot;,covariance(x,x),&quot;, variance of x =&quot;,np.var(x)) print(&quot;Covariance of y with itself =&quot;,covariance(y,y),&quot;, variance of x =&quot;,np.var(y)) . Covariance of x &amp; y = 0.3211758726837525 Covariance of y &amp; x = 0.3211758726837525 Covariance of x with itself = 0.6535789567500425 , variance of x = 0.6470431671825421 Covariance of y with itself = 0.19513765971790076 , variance of x = 0.19318628312072175 . Covariance matrix . So what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that $N-1$ in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn&#39;t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix. . Numpy gives us a handy thing to call: . data = np.stack((x,y),axis=1) # pack the x &amp; y data together in one 2D array print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) # .T b/c numpy wants varibles along rows rather than down columns? print(&quot;covariance matrix = n&quot;,cov) . data.shape = (100, 2) covariance matrix = [[0.65357896 0.32117587] [0.32117587 0.19513766]] . Some 3D data to work with . Now that we know what a covariance matrix is, let&#39;s generate some 3D data that we can use for what&#39;s coming next. Since there are 3 variables or 3 dimensions, the covariance matrix will now be 3x3. . z = -.5*x + 2*np.random.uniform(size=N) data = np.stack((x,y,z)).T print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) print(&quot;covariance matrix = n&quot;,cov) # Plot our data import plotly.graph_objects as go fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . data.shape = (100, 3) covariance matrix = [[ 0.65357896 0.32117587 -0.33982198] [ 0.32117587 0.19513766 -0.15839307] [-0.33982198 -0.15839307 0.5207214 ]] . . . (Note that even though our $z$ data didn&#39;t explicitly depend on $y$, the fact that $y$ is covariant with $x$ means that $y$ and $z$ &#39;coincidentally&#39; have a nonzero covariance. This sort of thing shows up in many datasets where two variables are correlated and may give rise to &#39;confounding&#39; factors.) . So now we have a covariance matrix. The next thing in PCA is find the &#39;principal components&#39;. This means the directions along which the data varies the most. You can kind of estimate these by rotating the 3D graph above. See also this great YouTube video of a teapot (1min 30s) that explains PCA in this manner. . To do Principal Component Analysis, we need to find the aforementioned &quot;components,&quot; and this requires finding eigenvectors for our dataset&#39;s covariance matrix. . What is an eigenvector, really? . First a definition. (Stay with me! We&#39;ll flesh this out in what comes after this.) . Given some matrix (or &#39;linear operator&#39;) ${ bf A}$ with dimensions $n times n$ (i.e., $n$ rows and $n$ columns), there exist a set of $n$ vectors $ vec{v}_i$ (each with dimension $n$, and $i = 1...n$ counts which vector we&#39;re talking about) such that multiplying one of these vectors by ${ bf A}$ results in a vector (anti)parallel to $ vec{v}_i$, with a length that&#39;s multiplied by some constant $ lambda_i$. In equation form: . $${ bf A} vec{v}_i = lambda_i vec{v}_i, (1)$$ . where the constants $ lambda_i$ are called eigenvalues and the vectors $ vec{v}_i$ are called eigenvectors. . (Note that I&#39;m departing a bit from common notation that uses $ vec{x}_i$ instead of $ vec{v}_i$; I don&#39;t want people to get confused when I want to use $x$&#39;s for coordinate variables.) . A graphical version of this is shown in Figure 1: . . Figure 1. &quot;An eigenvector is a vector that a linear operator sends to a multiple of itself&quot; -- Daniel McLaury . Brief Q &amp; A before we go on: . &quot;So what&#39;s with the &#39;eigen&#39; part?&quot; That&#39;s a German prefix, which in this context means &quot;inherent&quot; or &quot;own&quot;. | &quot;Can a non-square matrix have eigenvectors?&quot; Well,...no, and think of it this way: If $ bf{A}$ were an $n times m$ matrix (where $m &lt;&gt; n$), then it would be mapping from $n$ dimensions into $m$ dimensions, but on the &quot;other side&quot; of the equation with the $ lambda_i vec{v}_i$, that would still have $n$ dimensions, so... you&#39;d be saying an $n$-dimensional object equals an $m$-dimensional object, which is a no-go. | &quot;But my dataset has many more rows than columns, so what am I supposed to do about that?&quot; Just wait! It&#39;ll be ok. We&#39;re not actually going to take the eigenvectors of the dataset &#39;directly&#39;, we&#39;re going to take the eigenvectors of the covariance matrix of the dataset. | &quot;Are eigenvectors important?&quot; You bet! They get used in many areas of science. I first encountered them in quantum mechanics.* They describe the &quot;principal vectors&quot; of many objects, or &quot;normal modes&quot; of oscillating systems. They get used in computer vision, and... lots of places. You&#39;ll see them almost anywhere matrices &amp; tensors are employed, such as our topic for today: Data science! | *Ok that&#39;s not quite true: I first encountered them in an extremely boring Linear Algebra class taught by an extremely boring NASA engineer who thought he wanted to try teaching. But it wasn&#39;t until later that I learned anything about their relevance for...anything. Consquently I didn&#39;t &quot;learn them&quot; very well so writing this is a helpful review for me. . How to find the eigenvectors of a matrix . You call a library routine that does it for you, of course! ;-) . from numpy import linalg as LA lambdas, vs = LA.eig(cov) lambdas, vs . (array([1.07311435, 0.26724004, 0.02908363]), array([[-0.73933506, -0.47534042, -0.47690162], [-0.3717427 , -0.30238807, 0.87770657], [ 0.56141877, -0.82620393, -0.04686177]])) . Ok sort of kidding; we&#39;ll do it &quot;from scratch&quot;. But, one caveat before we start: Some matrices can be &quot;weird&quot; or &quot;problematic&quot; and have things like &quot;singular values.&quot; There are sophisticated numerical libraries for doing this, and joking aside, for real-world numerical applications you&#39;re better off calling a library routine that other very smart and very careful people have written for you. But for now, we&#39;ll do the straightforward way which works pretty well for many cases. We&#39;ll follow the basic two steps: . Find the eigenvalues | &#39;Plug in&#39; each eigenvalue to get a system of linear equations for the values of the components of the corresponding eigenvector | Solve this linear system. | 1. Find the eigenvalues . Ok I&#39;m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don&#39;t know what a determinant is good for (e.g. tons of proofs &amp; properties all rest on the determinant), they still at least remember how to calculate one. . The way to get the eigenvalues is to take the determinant of the difference between a $ bf{A}$ and $ lambda$ times the identity matrix $ bf{I}$ (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero... . $$det( bf{A} - lambda I) = 0 $$ . Just another observation:Since ${ bf I}$ is a square matrix, that means $ bf{A}$ has to be a square matrix too. Then solving for $ lambda$ will give you a polynomial equation in $ lambda$, the solutions to (or roots of) which are the eigenvectors $ lambda_i$. . Let&#39;s do an example: . $$ { bf A} = begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} $$To find the eigenvalues we set $$ det( bf{A} - lambda I) = begin{vmatrix} -2- lambda &amp; 2 &amp; 1 -5 &amp; 5- lambda &amp; 1 -4 &amp; 2 &amp; 3- lambda end{vmatrix} = 0.$$ . This gives us the equation... $$0 = lambda^3 - 6 lambda^2 + lambda - 6$$ . which has the 3 solutions (in descending order) $$ lambda = 3, 2, 1.$$ . *(Aside: to create an integer matrix with integer eigenvalues, I used [this handy web tool](https://ericthewry.github.io/integer_matrices/))*. . Just to check that against the numpy library: . A = np.array([[-2,2,1],[-5,5,1],[-4,2,3]]) def sorted_eig(A): # For now we sort &#39;by convention&#39;. For PCA the sorting is key. lambdas, vs = LA.eig(A) # Next line just sorts values &amp; vectors together in order of decreasing eigenvalues lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True)) return lambdas, np.array(vs).T # un-doing the list-casting from the previous line lambdas, vs = sorted_eig(A) lambdas # hold off on printing out the eigenvectors until we do the next part! . (3.0000000000000027, 1.9999999999999991, 1.0000000000000013) . Close enough! . 2. Use the eigenvalues to get the eigenvectors . Although it was anncounced in mid 2019 that you can get eigenvectors directly from eigenvalues, the usual way people have done this for a very long time is to go back to the matrix $ bf{A}$ and solve the linear system of equation (1) above, for each of the eigenvalues. For example, for $ lambda_1=-1$, we have . $$ { bf}A vec{v}_1 = - vec{v}_1 $$i.e. . $$ begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} = - begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} $$This amounts to 3 equations for 3 unknowns,...which I&#39;m going to assume you can handle... For the other eigenvalues things proceed similarly. The solutions we get for the 3 eigenvalues are: . $$ lambda_1 = 3: vec{v}_1 = (1,2,1)^T$$ . $$ lambda_2 = 2: vec{v}_2 = (1,1,2)^T$$ . $$ lambda_3 = 1: vec{v}_3 = (1,1,1)^T$$ . Since our original equation (1) allows us to scale eigenvectors by any artibrary constant, often we&#39;ll express eigenvectors as unit vectors $ hat{v}_i$. This will amount to dividing by the length of each vector, i.e. in our example multiplying by $(1/ sqrt{6},1/ sqrt{6},1/ sqrt{3})$. . In this setting . $$ lambda_1 = 3: hat{v}_1 = (1/ sqrt{6},2/ sqrt{6},1/ sqrt{6})^T$$ . $$ lambda_2 = 2: hat{v}_2 = (1/ sqrt{6},1/ sqrt{6},2/ sqrt{6})^T$$ . $$ lambda_3 = 1: hat{v}_3 = (1,1,1)^T/ sqrt{3}$$ . Checking our answers (left) with numpy&#39;s answers (right): . print(&quot; &quot;*15,&quot;Ours&quot;,&quot; &quot;*28,&quot;Numpy&quot;) print(np.array([1,2,1])/np.sqrt(6), vs[:,0]) print(np.array([1,1,2])/np.sqrt(6), vs[:,1]) print(np.array([1,1,1])/np.sqrt(3), vs[:,2]) . Ours Numpy [0.40824829 0.81649658 0.40824829] [-0.40824829 -0.81649658 -0.40824829] [0.40824829 0.40824829 0.81649658] [0.40824829 0.40824829 0.81649658] [0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027] . The fact that the first one differs by a multiplicative factor of -1 is not an issue. Remember: eigenvectors can be multiplied by an arbitrary constant. (Kind of odd that numpy doesn&#39;t choose the positive version though!) . One more check: let&#39;s multiply our eigenvectors times A to see what we get: . print(&quot;A*v_1 / 3 = &quot;,np.matmul(A, np.array([1,2,1]).T)/3 ) # Dividing by eigenvalue print(&quot;A*v_2 / 2 = &quot;,np.matmul(A, np.array([1,1,2]).T)/2 ) # to get vector back print(&quot;A*v_3 / 1 = &quot;,np.matmul(A, np.array([1,1,1]).T) ) . A*v_1 / 3 = [1. 2. 1.] A*v_2 / 2 = [1. 1. 2.] A*v_3 / 1 = [1 1 1] . Great! Let&#39;s move on. Back to our data! . Eigenvectors for our sample 3D dataset . Recall we named our 3x3 covariance matrix &#39;cov&#39;. So now we&#39;ll compute its eigenvectors, and then re-plot our 3D data and also plot the 3 eigenvectors with it... . lambdas, vs = sorted_eig(cov) # Compute e&#39;vals and e&#39;vectors of cov matrix print(&quot;lambdas, vs = n&quot;,lambdas,&quot; n&quot;,vs) # Re-plot our data fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; ) ]) # Draw some extra &#39;lines&#39; showing eigenvector directions n_ev_balls = 50 # the lines will be made of lots of balls in a line ev_size= 3 # size of balls t = np.linspace(0,1,num=n_ev_balls) # parameterizer for drawing along vec directions for i in range(3): # do this for each eigenvector # Uncomment the next line to scale (unit) vector by size of the eigenvalue # vs[:,i] *= lambdas[i] ex, ey, ez = t*vs[0,i], t*vs[1,i], t*vs[2,i] fig.add_trace(go.Scatter3d(x=ex, y=ey, z=ez,mode=&#39;markers&#39;, marker=dict(size=ev_size,opacity=0.8), name=&quot;v_&quot;+str(i+1))) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . lambdas, vs = (1.073114351318777, 0.26724003566904386, 0.0290836286576176) [[-0.73933506 -0.47534042 -0.47690162] [-0.3717427 -0.30238807 0.87770657] [ 0.56141877 -0.82620393 -0.04686177]] . . . Things to note from the above graph: . The first (red) eigenvector points along the direction of biggest variance | The second (greenish) eigenvector points along the direction of second-biggest variance | The third (purple) eigenvector points along the direction of smallest variance. | (If you edit the above code to rescale the vector length by the eigenvector, you&#39;ll really see these three point become apparent!) . &quot;Principal Component&quot; Analysis . Now we have our components (=eigenvectors), and we have them &quot;ranked&quot; by their &quot;significance.&quot; Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will &quot;squish&quot; the data along the smallest-variance directions. . For the above 3D dataset, we&#39;re going to squish it into a 2D pancake -- by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors. . Yea, but how to do this projection? . Projecting the data . It&#39;s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)! . OH WAIT! Hey, you want to see a cool trick!? Check this out: . lambdas, vs = sorted_eig(cov) proj_cov = vs.T @ cov @ vs # project the covariance matrix, using eigenvectors proj_cov . array([[ 1.07311435e+00, 4.47193943e-18, -4.45219967e-17], [ 5.25950058e-17, 2.67240036e-01, 6.70099547e-17], [-7.05170575e-17, 4.02835192e-17, 2.90836287e-02]]) . What was THAT? Let me clean that up a bit for you... . proj_cov[np.abs(proj_cov) &lt; 1e-15] = 0 proj_cov . array([[1.07311435, 0. , 0. ], [0. , 0.26724004, 0. ], [0. , 0. , 0.02908363]]) . Important point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a &#39;coordinate system&#39; where different &#39;directions&#39; decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the &#39;arbitrary&#39; coordinates that we &#39;humans&#39; may have assigned initially. . The numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z. . So really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we&#39;ll literally just drop a whole index&#39;s-worth of data-dimension in this new coordinate system. :-) . So, instead of $x$, $y$ and $z$, let&#39;s have three coordinates which (following physicist-notation) we&#39;ll call $q_1$, $q_2$ and $q_3$, and these will run along the directions given by the three eigenvectors. . Let&#39;s write our data as a N-by-3 matrix, where N is the number of data points we have. . data = np.stack((x,y,z),axis=1) data.shape # we had a 100 data points, so expecting 100x3 matrix . (100, 3) . There are two ways of doing this, and I&#39;ll show you that they&#39;re numerically equivalent: . Use all the eigenvectors to &quot;rotate&quot; the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data. | Truncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors. | Let&#39;s show them both: . print(&quot; n 1. All data, rotated into new coordinate system&quot;) W = vs[:,0:3] # keep the all the eigenvectors new_data_all = data @ W # project all the data print(&quot;Checking: new_data_all.shape =&quot;,new_data_all.shape) print(&quot;New covariance matrix = n&quot;,np.cov(new_data_all.T) ) print(&quot; n 2. Truncated data projected onto principal axes of coordinate system&quot;) W = vs[:,0:2] # keep only the first and 2nd eigenvectors print(&quot;W.shape = &quot;,W.shape) new_data_proj = data @ W # project print(&quot;Checking: new_data_proj.shape =&quot;,new_data_proj.shape) print(&quot;New covariance matrix in projected space = n&quot;,np.cov(new_data_proj.T) ) # Difference between them diff = new_data_all[:,0:2] - new_data_proj print(&quot; n Absolute maximum difference between the two methods = &quot;,np.max(np.abs(diff))) . 1. All data, rotated into new coordinate system Checking: new_data_all.shape = (100, 3) New covariance matrix = [[1.07311435e+00 7.64444687e-17 3.77081428e-17] [7.64444687e-17 2.67240036e-01 1.21906748e-16] [3.77081428e-17 1.21906748e-16 2.90836287e-02]] 2. Truncated data projected onto principal axes of coordinate system W.shape = (3, 2) Checking: new_data_proj.shape = (100, 2) New covariance matrix in projected space = [[1.07311435e+00 7.64444687e-17] [7.64444687e-17 2.67240036e-01]] Absolute maximum difference between the two methods = 0.0 . ...Nada. The 2nd method will be faster computationally though, because it doesn&#39;t calculate stuff you&#39;re going to throw away. . One more comparison between the two methods. Let&#39;s take a look at the &quot;full&quot; dataset (in blue) vs. the projected dataset (in red): . fig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2], mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;full data&quot; ))]) fig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0, mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;projected&quot; ) ) fig.update_layout(scene_aspectmode=&#39;data&#39;) fig.show() . . . (Darn it, [if only Plot.ly would support orthographic projections](https://community.plot.ly/t/orthographic-projection-for-3d-plots/3897) [[2](https://community.plot.ly/t/orthographic-projection-instead-of-perspective-for-3d-plots/10035)] it&#39;d be a lot easier to visually compare the two datasets!) . Beyond 3D . So typically we use PCA to throw out many more dimensions than just one. Often this is used for data visualization but it&#39;s also done for feature reduction, i.e. to send less data into your machine learning algorithm. (PCA can even be used just as a &quot;multidimensional linear regression&quot; algorithm, but you wouldn&#39;t want to!) . How do you know how many dimensions to throw out? . In other words, how many &#39;components&#39; should you choose to keep when doing PCA? There are a few ways to make this judgement call -- it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an &#39;elbow&#39; at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we&#39;ll make such a graph. For more on this topic, see this post by Arthur Gonsales. . Example: Handwritten Digits . The scikit-learn library uses this as an example and I like it. It goes as follows: . Take a dataset of tiny 8x8 pixel images of handwritten digits. | Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. | Show on a plot how the different digits cluster in different regions of the space. | (This part we&#39;ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.) | To be clear: In what follows, each pixel of the image counts as a &quot;feature,&quot; i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions. . from sklearn.datasets import load_digits from sklearn.decomposition import PCA digits = load_digits() X = digits.data / 255.0 Y = digits.target print(X.shape, Y.shape,&#39; n&#39;) # Let&#39;s look a a few examples for i in range(8): # show 8 examples print(&quot;This is supposed to be a &#39;&quot;,Y[i],&quot;&#39;:&quot;,sep=&quot;&quot;) plt.imshow(X[i].reshape([8,8])) plt.show() . (1797, 64) (1797,) This is supposed to be a &#39;0&#39;: . This is supposed to be a &#39;1&#39;: . This is supposed to be a &#39;2&#39;: . This is supposed to be a &#39;3&#39;: . This is supposed to be a &#39;4&#39;: . This is supposed to be a &#39;5&#39;: . This is supposed to be a &#39;6&#39;: . This is supposed to be a &#39;7&#39;: . Now let&#39;s do the PCA thang... First we&#39;ll try going down to 2 dimensions. This isn&#39;t going to work super great but we&#39;ll try: . digits_cov = np.cov(X.T) print(&quot;digits_cov.shape = &quot;,digits_cov.shape) lambdas, vs = sorted_eig(np.array(digits_cov)) W = vs[:,0:2] # just keep two dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot fig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=6, opacity=0.7, color=Y), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout( xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5))) fig.show() . digits_cov.shape = (64, 64) proj_digits.shape = (1797, 2) . . . This is &#39;sort of ok&#39;: There are some regions that are mostly one kind of digit. But you may say there&#39;s there&#39;s too much intermingling between classes for a lot of this plot. So let&#39;s try it again with 3 dimensions for PCA: . W = vs[:,0:3] # just three dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot, separate them by &quot;z&quot; which is the digit of interest. fig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.8, color=Y, showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout(title=&quot;8x8 Handwritten Digits&quot;, xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . proj_digits.shape = (1797, 3) . . . Now we can start to see some definition! The 6&#39;s are pretty much in one area, the 2&#39;s are in another area, and the 0&#39;s are in still another, and so on. There is some intermingling to be sure (particularly between the 5&#39;s and 8&#39;s), but you can see that this &#39;kind of&#39; gets the job done, and instead of dealing with 64 dimensions, we&#39;re down to 3! . Graphing Variance vs. Components . Earlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the &#39;significance&#39; that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a &#39;kink&#39; or &#39;elbow&#39; in the graph as a place to truncate our representation... . plt.plot( np.abs(lambdas)/np.sum(lambdas) ) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Significance&#39;) plt.show() . ...So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the &#39;elbow&#39; in the graph lies. . Interpretability . What is the meaning of the new coordinate axes or &#39;features&#39; $q_1$, $q_2$, etc? Sometimes there exists a compelling physical intepretation of these features (e.g., as in the case of eigenmodes of coupled oscillators), but often...there may not be any. And yet we haven&#39;t even done any &#39;real machine learning&#39; at this point! ;-) . This is an important topic. Modern data regulations such as the European Union&#39;s GDPR require that models used in algorithmic decision-making be &#39;explainable&#39;. If the data being fed into such algorithmics is already abstracted via methods such as PCA, this could be an issue. Thankfully, the linearity of the components mean that one can describe each principal component as a linear combination of inputs. . Further reading . There are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following... . Examples &amp; links . &quot;Eigenstyle: Principal Component Analysis and Fashion&quot; by Grace Avery. Uses PCA on Fashion-MNIST. It&#39;s good! | Neat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone. | Andrew Ng&#39;s Machine Learning Course, Lecture on PCA. How I first learned about this stuff. | PCA using Python by Michael Galarnyk. Does similar things to what I&#39;ve done here, although maybe better! | Plot.ly PCA notebook examples | . Appendix A: Overkill: Bigger Handwritten Digits . Sure, 8x8 digit images are boring. What about 28x28 images, as in the MNIST dataset? Let&#39;s roll... . from sklearn.datasets import fetch_openml from random import sample #mnist = fetch_mldata(&#39;MNIST original&#39;) mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X2 = mnist.data / 255 Y2 = np.array(mnist.target,dtype=np.int) #Let&#39;s grab some indices for random suffling indices = list(range(X2.shape[0])) # Let&#39;s look a a few examples for i in range(8): # 8 is good i = sample(indices,1) print(&quot;This is supposed to be a &quot;,Y2[i][0],&quot;:&quot;,sep=&quot;&quot;) plt.imshow(X2[i].reshape([28,28])) plt.show() . This is supposed to be a 1: . This is supposed to be a 9: . This is supposed to be a 8: . This is supposed to be a 7: . This is supposed to be a 3: . This is supposed to be a 5: . This is supposed to be a 2: . This is supposed to be a 6: . mnist_cov = np.cov(X2.T) lambdas, vs = sorted_eig(np.array(mnist_cov)) W = vs[:,0:3] # Grab the 3 most significant dimensions # Plotting all 70,000 data points is going to be too dense too look at. # Instead let&#39;s grab a random sample of 5,000 points n_plot = 5000 indices = sample(list(range(X2.shape[0])), n_plot) proj_mnist = np.array(X2[indices] @ W, dtype=np.float32) # Last step of PCA: project fig = go.Figure(data=[go.Scatter3d(x=proj_mnist[:,0], y=proj_mnist[:,1], z=proj_mnist[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.7, color=Y2[indices], showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y2[indices]] )]) fig.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: ComplexWarning: Casting complex values to real discards the imaginary part . . . ...bit of a mess. Not as cleanly separated as the 8x8 image examples. You can see that the 0&#39;s are well separated from the 1&#39;s and the 3&#39;s, but everything else is pretty mixed together. (I suspect the 1&#39;s are clustered strongly because they involve the most dark pixels.) . If you wanted to push this further then either keeping more dimensions (thereby making it un-visualizable) or just using a different method entirely (e.g. t-SNE or even better: UMAP) would be the way to go. Still, it&#39;s neat to see that you can get somewhat intelligible results in 3D even on this &#39;much harder&#39; problem. . Appendix B: Because We Can: Turning it into a Classifier . ...But let&#39;s not do a neural network because all I ever do are neural networks, and because I don&#39;t want to have take the time &amp; space to explain how they work or load in external libraries. Let&#39;s do k-nearest-neighbors instead, because it&#39;s intuitively easy to grasp and it&#39;s not hard to code up: . For any new point we want to evaluate, we take a &#39;vote&#39; of whatever some number (called $k$) of its nearest neighbor points are already assigned as, and we set the class of the new point according to that vote. . Making a efficient classifier is all about finding the *boundaries* between regions (and usually subject to some user-adjustable parameter like $k$ or some numerical threshold). Finding these boundaries can be about finding the &#39;edge cases&#39; that cause the system to &#39;flip&#39; (discontinuously) from one result to another. However, we are *not* going to make an efficient classifier today. ;-) . Let&#39;s go back to the 8x8 digits example, and split it into a training set and a testing set (so we can check ourselves): . indices = sample(list(range(X.shape[0])), X.shape[0]) X_shuf, Y_shuf = X[indices,:], Y[indices] # 80-20 train-test split max_train_ind = int(0.8*X.shape[0]) X_train, Y_train = X_shuf[0:max_train_ind,:], Y_shuf[0:max_train_ind] X_test, Y_test = X_shuf[max_train_ind:-1,:], Y_shuf[max_train_ind:-1] # Do PCA on training set train_cov = np.cov(X_train.T) ell, v = sorted_eig(np.array(train_cov)) pca_dims = 3 # number of top &#39;dimensions&#39; to take W_train = v[:,0:pca_dims] proj_train = X_train @ W_train # also project the testing set while we&#39;re at it proj_test = X_test @ W_train # yes, same W_train . Now let&#39;s make a little k-nearest neighbors routine... . from collections import Counter def knn_predict(xnew, proj_train, Y_train, k=3): &quot;&quot;&quot; xnew is a new data point that has the same shape as one row of proj_train. Given xnew, calculate the (squared) distance to all the points in X_train to find out which ones are nearest. &quot;&quot;&quot; distances = ((proj_train - xnew)**2).sum(axis=1) # stick on an extra column of indexing &#39;hash&#39; for later use after we sort dists_i = np.stack( (distances, np.array(range(Y_train.shape[0]) )),axis=1 ) dists_i = dists_i[dists_i[:,0].argsort()] # sort in ascending order of distance knn_inds = (dists_i[0:k,-1]).astype(np.int) # Grab the indexes for k nearest neighbors # take &#39;votes&#39;: knn_targets = list(Y_train[knn_inds]) # which classes the nn&#39;s belong to votes = Counter(knn_targets) # count up how many of each class are represented return votes.most_common(1)[0][0] # pick the winner, or the first member of a tie # Let&#39;s test it on the first element of the testing set x, y_true = proj_test[0], Y_test[0] guess = knn_predict(x, proj_train, Y_train) print(&quot;guess = &quot;,guess,&quot;, true = &quot;,y_true) . guess = 6 , true = 6 . Now let&#39;s try it for the &#39;unseen&#39; data in the testing set, and see how we do... . mistakes, n_test = 0, Y_test.shape[0] for i in range(n_test): x = proj_test[i] y_pred = knn_predict(x, proj_train, Y_train, k=3) y_true = Y_test[i] if y_true != y_pred: mistakes += 1 if i &lt; 20: # show some examples print(&quot;x, y_pred, y_true =&quot;,x, y_pred, y_true, &quot;YAY!&quot; if y_pred==y_true else &quot; BOO. :-(&quot;) print(&quot;...skipping a lot...&quot;) print(&quot;Total Accuracy =&quot;, (n_test-mistakes)/n_test*100,&quot;%&quot;) . x, y_pred, y_true = [ 0.06075339 0.00153272 -0.0477644 ] 6 6 YAY! x, y_pred, y_true = [ 0.04083212 0.09757529 -0.05361896] 1 1 YAY! x, y_pred, y_true = [-0.0199586 -0.00778773 0.00972962] 8 5 BOO. :-( x, y_pred, y_true = [ 0.02400112 -0.07267613 0.02774141] 0 0 YAY! x, y_pred, y_true = [0.01180771 0.03483923 0.07526469] 1 7 BOO. :-( x, y_pred, y_true = [ 0.00379226 -0.06269449 -0.00195609] 0 0 YAY! x, y_pred, y_true = [-0.06832135 -0.05396545 0.02980845] 9 9 YAY! x, y_pred, y_true = [-0.02397417 -0.04914796 0.0109273 ] 5 5 YAY! x, y_pred, y_true = [ 0.08213707 -0.01608953 -0.08072889] 6 6 YAY! x, y_pred, y_true = [ 0.03056858 -0.04852946 0.02204423] 0 0 YAY! x, y_pred, y_true = [-0.02124777 0.03623541 -0.01773196] 8 8 YAY! x, y_pred, y_true = [0.03035896 0.01398381 0.01415554] 8 8 YAY! x, y_pred, y_true = [ 0.0214849 0.02114674 -0.08951798] 1 1 YAY! x, y_pred, y_true = [ 0.07878152 0.03312015 -0.06488347] 6 6 YAY! x, y_pred, y_true = [-0.01294308 0.00158962 -0.01255491] 8 5 BOO. :-( x, y_pred, y_true = [ 0.01351581 0.11000321 -0.03895516] 1 1 YAY! x, y_pred, y_true = [0.0081306 0.01683952 0.05911389] 7 1 BOO. :-( x, y_pred, y_true = [0.06497268 0.02817075 0.07118004] 4 4 YAY! x, y_pred, y_true = [-0.03879657 -0.04460611 0.02833793] 9 5 BOO. :-( x, y_pred, y_true = [-0.05975051 0.03713843 -0.07174727] 2 2 YAY! ...skipping a lot... Total Accuracy = 76.88022284122563 % . ...eh! Not bad, not amazing. You can improve the accuracy if you go back up and increase the number of PCA dimensions beyond 3, and/or increase the value of $k$. Go ahead and try it! . (For 10 dimensions and $k=7$, I got 97.7% accuracy. The highest I ever got it was 99%, but that was really working overtime, computationally speaking; the point of PCA is to let you dramatically reduce your workload while retaining reasonably high accuracy.) . Just to reiterate: This is NOT supposed to be a state of the art classifier! It&#39;s just a toy that does pretty well and helps illustrate PCA without being hard to understand or code. . The End . Thanks for sticking around! Hope this was interesting. PCA is pretty simple, and yet really useful! ...and writing this really helped me to better understand it. ;-) .",
            "url": "https://hungpq7.github.io/data-science-blog/2019/12/21/pca.html",
            "relUrl": "/2019/12/21/pca.html",
            "date": " • Dec 21, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Principal Component Analysis (PCA) from Scratch",
            "content": ". Relevance . Principal Component Analysis (PCA) is a data-reduction technique that finds application in a wide variety of fields, including biology, sociology, physics, medicine, and audio processing. PCA may be used as a &quot;front end&quot; processing step that feeds into additional layers of machine learning, or it may be used by itself, for example when doing data visualization. It is so useful and ubiquitious that is is worth learning not only what it is for and what it is, but how to actually do it. . In this interactive worksheet, we work through how to perform PCA on a few different datasets, writing our own code as we go. . Other (better?) treatments . My treatment here was written several months after viewing... . the excellent demo page at setosa.io | this quick 1m30s video of a teapot, | this great StatsQuest video | this lecture from Andrew Ng&#39;s course | . Basic Idea . Put simply, PCA involves making a coordinate transformation (i.e., a rotation) from the arbitrary axes (or &quot;features&quot;) you started with to a set of axes &#39;aligned with the data itself,&#39; and doing this almost always means that you can get rid of a few of these &#39;components&#39; of data that have small variance without suffering much in the way of accurcy while saving yourself a ton of computation. . Once you &quot;get it,&quot; you&#39;ll find PCA to be almost no big deal, if it weren&#39;t for the fact that it&#39;s so darn useful! . We&#39;ll define the following terms as we go, but here&#39;s the process in a nutshell: . Covariance: Find the covariance matrix for your dataset | Eigenvectors: Find the eigenvectors of that matrix (these are the &quot;components&quot; btw) | Ordering: Sort the eigenvectors/&#39;dimensions&#39; from biggest to smallest variance | Projection / Data reduction: Use the eigenvectors corresponding to the largest variance to project the dataset into a reduced- dimensional space | (Check: How much did we lose by that truncation?) | Caveats . Since PCA will involve making linear transformations, there are some situations where PCA won&#39;t help but...pretty much it&#39;s handy enough that it&#39;s worth giving it a shot! . Covariance . If you&#39;ve got two data dimensions and they vary together, then they are co-variant. . Example: Two-dimensional data that&#39;s somewhat co-linear: . import numpy as np import matplotlib.pyplot as plt import plotly.graph_objects as go N = 100 x = np.random.normal(size=N) y = 0.5*x + 0.2*(np.random.normal(size=N)) fig = go.Figure(data=[go.Scatter(x=x, y=y, mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . . . Variance . So, for just the $x$ component of the above data, there&#39;s some mean value (which in this case is zero), and there&#39;s some variance about this mean: technically the variance is the average of the squared differences from the mean. If you&#39;re familiar with the standard deviation, usually denoted by $ sigma$, the variance is just the square of the standard deviation. If $x$ had units of meters, the variance $ sigma^2$ would have units of meters^2. . Think of the variance as the &quot;spread,&quot; or &quot;extent&quot; of the data, about some particular axis (or input, or &quot;feature&quot;). . Similarly we can look at the variance in just the $y$ component of the data. For the above data, the variances in $x$ and $y$ are . print(&quot;Variance in x =&quot;,np.var(x)) print(&quot;Variance in y =&quot;,np.var(y)) . Variance in x = 0.6470431671825421 Variance in y = 0.19318628312072175 . Covariance . You&#39;ll notice in the above graph that as $x$ varies, so does $y$ -- pretty much. So $y$ is &quot;covariant&quot; with $x$. &quot;Covariance indicates the level to which two variables vary together.&quot; To compute it, it&#39;s kind of like the regular variance, except that instead of squaring the deviation from the mean for one variable, we multiply the deviations for the two variables: . $${ rm Cov}(x,y) = {1 over N-1} sum_{j=1}^N (x_j- mu_x)(y_j- mu_j),$$ where $ mu_x$ and $ mu_y$ are the means for the x- and y- componenets of the data, respectively. Note that you can reverse $x$ and $y$ and get the same result, and the covariance of a variable with itself is just the regular variance -- but with one caveat! . The caveat is that we&#39;re dividing by $N-1$ instead of $N$, so unlike the regular variance we&#39;re not quite taking the mean. Why this? Well, for large datasets this makes essentially no difference, but for small numbers of data points, using $N$ can give values that tend to be a bit too small for most people&#39;s tastes, so the $N-1$ was introduced to &quot;reduce small sample bias.&quot; . In Python code, the covariance calculation looks like . def covariance(a,b): return ( (a - a.mean())*(b - b.mean()) ).sum() / (len(a)-1) print(&quot;Covariance of x &amp; y =&quot;,covariance(x,y)) print(&quot;Covariance of y &amp; x =&quot;,covariance(x,y)) print(&quot;Covariance of x with itself =&quot;,covariance(x,x),&quot;, variance of x =&quot;,np.var(x)) print(&quot;Covariance of y with itself =&quot;,covariance(y,y),&quot;, variance of x =&quot;,np.var(y)) . Covariance of x &amp; y = 0.3211758726837525 Covariance of y &amp; x = 0.3211758726837525 Covariance of x with itself = 0.6535789567500425 , variance of x = 0.6470431671825421 Covariance of y with itself = 0.19513765971790076 , variance of x = 0.19318628312072175 . Covariance matrix . So what we do is we take the covariance of every variable with every variable (including itself) and make a matrix out of it. Along the diagonal will be the variance of each variable (except for that $N-1$ in the denominator), and the rest of the matrix will be the covariances. Note that since the order of the variables doesn&#39;t matter when computing covariance, the matrix will be symmetric (i.e. it will equal its own transpose, i.e. will have a reflection symmetry across the diagonal) and thus will be a square matrix. . Numpy gives us a handy thing to call: . data = np.stack((x,y),axis=1) # pack the x &amp; y data together in one 2D array print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) # .T b/c numpy wants varibles along rows rather than down columns? print(&quot;covariance matrix = n&quot;,cov) . data.shape = (100, 2) covariance matrix = [[0.65357896 0.32117587] [0.32117587 0.19513766]] . Some 3D data to work with . Now that we know what a covariance matrix is, let&#39;s generate some 3D data that we can use for what&#39;s coming next. Since there are 3 variables or 3 dimensions, the covariance matrix will now be 3x3. . z = -.5*x + 2*np.random.uniform(size=N) data = np.stack((x,y,z)).T print(&quot;data.shape =&quot;,data.shape) cov = np.cov(data.T) print(&quot;covariance matrix = n&quot;,cov) # Plot our data import plotly.graph_objects as go fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; )]) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . data.shape = (100, 3) covariance matrix = [[ 0.65357896 0.32117587 -0.33982198] [ 0.32117587 0.19513766 -0.15839307] [-0.33982198 -0.15839307 0.5207214 ]] . . . (Note that even though our $z$ data didn&#39;t explicitly depend on $y$, the fact that $y$ is covariant with $x$ means that $y$ and $z$ &#39;coincidentally&#39; have a nonzero covariance. This sort of thing shows up in many datasets where two variables are correlated and may give rise to &#39;confounding&#39; factors.) . So now we have a covariance matrix. The next thing in PCA is find the &#39;principal components&#39;. This means the directions along which the data varies the most. You can kind of estimate these by rotating the 3D graph above. See also this great YouTube video of a teapot (1min 30s) that explains PCA in this manner. . To do Principal Component Analysis, we need to find the aforementioned &quot;components,&quot; and this requires finding eigenvectors for our dataset&#39;s covariance matrix. . What is an eigenvector, really? . First a definition. (Stay with me! We&#39;ll flesh this out in what comes after this.) . Given some matrix (or &#39;linear operator&#39;) ${ bf A}$ with dimensions $n times n$ (i.e., $n$ rows and $n$ columns), there exist a set of $n$ vectors $ vec{v}_i$ (each with dimension $n$, and $i = 1...n$ counts which vector we&#39;re talking about) such that multiplying one of these vectors by ${ bf A}$ results in a vector (anti)parallel to $ vec{v}_i$, with a length that&#39;s multiplied by some constant $ lambda_i$. In equation form: . $${ bf A} vec{v}_i = lambda_i vec{v}_i, (1)$$ . where the constants $ lambda_i$ are called eigenvalues and the vectors $ vec{v}_i$ are called eigenvectors. . (Note that I&#39;m departing a bit from common notation that uses $ vec{x}_i$ instead of $ vec{v}_i$; I don&#39;t want people to get confused when I want to use $x$&#39;s for coordinate variables.) . A graphical version of this is shown in Figure 1: . . Figure 1. &quot;An eigenvector is a vector that a linear operator sends to a multiple of itself&quot; -- Daniel McLaury . Brief Q &amp; A before we go on: . &quot;So what&#39;s with the &#39;eigen&#39; part?&quot; That&#39;s a German prefix, which in this context means &quot;inherent&quot; or &quot;own&quot;. | &quot;Can a non-square matrix have eigenvectors?&quot; Well,...no, and think of it this way: If $ bf{A}$ were an $n times m$ matrix (where $m &lt;&gt; n$), then it would be mapping from $n$ dimensions into $m$ dimensions, but on the &quot;other side&quot; of the equation with the $ lambda_i vec{v}_i$, that would still have $n$ dimensions, so... you&#39;d be saying an $n$-dimensional object equals an $m$-dimensional object, which is a no-go. | &quot;But my dataset has many more rows than columns, so what am I supposed to do about that?&quot; Just wait! It&#39;ll be ok. We&#39;re not actually going to take the eigenvectors of the dataset &#39;directly&#39;, we&#39;re going to take the eigenvectors of the covariance matrix of the dataset. | &quot;Are eigenvectors important?&quot; You bet! They get used in many areas of science. I first encountered them in quantum mechanics.* They describe the &quot;principal vectors&quot; of many objects, or &quot;normal modes&quot; of oscillating systems. They get used in computer vision, and... lots of places. You&#39;ll see them almost anywhere matrices &amp; tensors are employed, such as our topic for today: Data science! | *Ok that&#39;s not quite true: I first encountered them in an extremely boring Linear Algebra class taught by an extremely boring NASA engineer who thought he wanted to try teaching. But it wasn&#39;t until later that I learned anything about their relevance for...anything. Consquently I didn&#39;t &quot;learn them&quot; very well so writing this is a helpful review for me. . How to find the eigenvectors of a matrix . You call a library routine that does it for you, of course! ;-) . from numpy import linalg as LA lambdas, vs = LA.eig(cov) lambdas, vs . (array([1.07311435, 0.26724004, 0.02908363]), array([[-0.73933506, -0.47534042, -0.47690162], [-0.3717427 , -0.30238807, 0.87770657], [ 0.56141877, -0.82620393, -0.04686177]])) . Ok sort of kidding; we&#39;ll do it &quot;from scratch&quot;. But, one caveat before we start: Some matrices can be &quot;weird&quot; or &quot;problematic&quot; and have things like &quot;singular values.&quot; There are sophisticated numerical libraries for doing this, and joking aside, for real-world numerical applications you&#39;re better off calling a library routine that other very smart and very careful people have written for you. But for now, we&#39;ll do the straightforward way which works pretty well for many cases. We&#39;ll follow the basic two steps: . Find the eigenvalues | &#39;Plug in&#39; each eigenvalue to get a system of linear equations for the values of the components of the corresponding eigenvector | Solve this linear system. | 1. Find the eigenvalues . Ok I&#39;m hoping you at least can recall what a determinant of a matrix is. Many people, even if they don&#39;t know what a determinant is good for (e.g. tons of proofs &amp; properties all rest on the determinant), they still at least remember how to calculate one. . The way to get the eigenvalues is to take the determinant of the difference between a $ bf{A}$ and $ lambda$ times the identity matrix $ bf{I}$ (which is just ones along the diagonal and zeros otherwise) and set that difference equal to zero... . $$det( bf{A} - lambda I) = 0 $$ . Just another observation:Since ${ bf I}$ is a square matrix, that means $ bf{A}$ has to be a square matrix too. Then solving for $ lambda$ will give you a polynomial equation in $ lambda$, the solutions to (or roots of) which are the eigenvectors $ lambda_i$. . Let&#39;s do an example: . $$ { bf A} = begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} $$To find the eigenvalues we set $$ det( bf{A} - lambda I) = begin{vmatrix} -2- lambda &amp; 2 &amp; 1 -5 &amp; 5- lambda &amp; 1 -4 &amp; 2 &amp; 3- lambda end{vmatrix} = 0.$$ . This gives us the equation... $$0 = lambda^3 - 6 lambda^2 + lambda - 6$$ . which has the 3 solutions (in descending order) $$ lambda = 3, 2, 1.$$ . *(Aside: to create an integer matrix with integer eigenvalues, I used [this handy web tool](https://ericthewry.github.io/integer_matrices/))*. . Just to check that against the numpy library: . A = np.array([[-2,2,1],[-5,5,1],[-4,2,3]]) def sorted_eig(A): # For now we sort &#39;by convention&#39;. For PCA the sorting is key. lambdas, vs = LA.eig(A) # Next line just sorts values &amp; vectors together in order of decreasing eigenvalues lambdas, vs = zip(*sorted(zip(list(lambdas), list(vs.T)),key=lambda x: x[0], reverse=True)) return lambdas, np.array(vs).T # un-doing the list-casting from the previous line lambdas, vs = sorted_eig(A) lambdas # hold off on printing out the eigenvectors until we do the next part! . (3.0000000000000027, 1.9999999999999991, 1.0000000000000013) . Close enough! . 2. Use the eigenvalues to get the eigenvectors . Although it was anncounced in mid 2019 that you can get eigenvectors directly from eigenvalues, the usual way people have done this for a very long time is to go back to the matrix $ bf{A}$ and solve the linear system of equation (1) above, for each of the eigenvalues. For example, for $ lambda_1=-1$, we have . $$ { bf}A vec{v}_1 = - vec{v}_1 $$i.e. . $$ begin{bmatrix} -2 &amp; 2 &amp; 1 -5 &amp; 5 &amp; 1 -4 &amp; 2 &amp; 3 end{bmatrix} begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} = - begin{bmatrix} v_{1x} v_{1y} v_{1z} end{bmatrix} $$This amounts to 3 equations for 3 unknowns,...which I&#39;m going to assume you can handle... For the other eigenvalues things proceed similarly. The solutions we get for the 3 eigenvalues are: . $$ lambda_1 = 3: vec{v}_1 = (1,2,1)^T$$ . $$ lambda_2 = 2: vec{v}_2 = (1,1,2)^T$$ . $$ lambda_3 = 1: vec{v}_3 = (1,1,1)^T$$ . Since our original equation (1) allows us to scale eigenvectors by any artibrary constant, often we&#39;ll express eigenvectors as unit vectors $ hat{v}_i$. This will amount to dividing by the length of each vector, i.e. in our example multiplying by $(1/ sqrt{6},1/ sqrt{6},1/ sqrt{3})$. . In this setting . $$ lambda_1 = 3: hat{v}_1 = (1/ sqrt{6},2/ sqrt{6},1/ sqrt{6})^T$$ . $$ lambda_2 = 2: hat{v}_2 = (1/ sqrt{6},1/ sqrt{6},2/ sqrt{6})^T$$ . $$ lambda_3 = 1: hat{v}_3 = (1,1,1)^T/ sqrt{3}$$ . Checking our answers (left) with numpy&#39;s answers (right): . print(&quot; &quot;*15,&quot;Ours&quot;,&quot; &quot;*28,&quot;Numpy&quot;) print(np.array([1,2,1])/np.sqrt(6), vs[:,0]) print(np.array([1,1,2])/np.sqrt(6), vs[:,1]) print(np.array([1,1,1])/np.sqrt(3), vs[:,2]) . Ours Numpy [0.40824829 0.81649658 0.40824829] [-0.40824829 -0.81649658 -0.40824829] [0.40824829 0.40824829 0.81649658] [0.40824829 0.40824829 0.81649658] [0.57735027 0.57735027 0.57735027] [0.57735027 0.57735027 0.57735027] . The fact that the first one differs by a multiplicative factor of -1 is not an issue. Remember: eigenvectors can be multiplied by an arbitrary constant. (Kind of odd that numpy doesn&#39;t choose the positive version though!) . One more check: let&#39;s multiply our eigenvectors times A to see what we get: . print(&quot;A*v_1 / 3 = &quot;,np.matmul(A, np.array([1,2,1]).T)/3 ) # Dividing by eigenvalue print(&quot;A*v_2 / 2 = &quot;,np.matmul(A, np.array([1,1,2]).T)/2 ) # to get vector back print(&quot;A*v_3 / 1 = &quot;,np.matmul(A, np.array([1,1,1]).T) ) . A*v_1 / 3 = [1. 2. 1.] A*v_2 / 2 = [1. 1. 2.] A*v_3 / 1 = [1 1 1] . Great! Let&#39;s move on. Back to our data! . Eigenvectors for our sample 3D dataset . Recall we named our 3x3 covariance matrix &#39;cov&#39;. So now we&#39;ll compute its eigenvectors, and then re-plot our 3D data and also plot the 3 eigenvectors with it... . lambdas, vs = sorted_eig(cov) # Compute e&#39;vals and e&#39;vectors of cov matrix print(&quot;lambdas, vs = n&quot;,lambdas,&quot; n&quot;,vs) # Re-plot our data fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode=&#39;markers&#39;, marker=dict(size=8,opacity=0.5), name=&quot;data&quot; ) ]) # Draw some extra &#39;lines&#39; showing eigenvector directions n_ev_balls = 50 # the lines will be made of lots of balls in a line ev_size= 3 # size of balls t = np.linspace(0,1,num=n_ev_balls) # parameterizer for drawing along vec directions for i in range(3): # do this for each eigenvector # Uncomment the next line to scale (unit) vector by size of the eigenvalue # vs[:,i] *= lambdas[i] ex, ey, ez = t*vs[0,i], t*vs[1,i], t*vs[2,i] fig.add_trace(go.Scatter3d(x=ex, y=ey, z=ez,mode=&#39;markers&#39;, marker=dict(size=ev_size,opacity=0.8), name=&quot;v_&quot;+str(i+1))) fig.update_layout( xaxis_title=&quot;x&quot;, yaxis_title=&quot;y&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . lambdas, vs = (1.073114351318777, 0.26724003566904386, 0.0290836286576176) [[-0.73933506 -0.47534042 -0.47690162] [-0.3717427 -0.30238807 0.87770657] [ 0.56141877 -0.82620393 -0.04686177]] . . . Things to note from the above graph: . The first (red) eigenvector points along the direction of biggest variance | The second (greenish) eigenvector points along the direction of second-biggest variance | The third (purple) eigenvector points along the direction of smallest variance. | (If you edit the above code to rescale the vector length by the eigenvector, you&#39;ll really see these three point become apparent!) . &quot;Principal Component&quot; Analysis . Now we have our components (=eigenvectors), and we have them &quot;ranked&quot; by their &quot;significance.&quot; Next we will eliminate one or more of the less-significant directions of variance. In other words, we will project the data onto the various principal components by projecting along the less-significant components. Or even simpler: We will &quot;squish&quot; the data along the smallest-variance directions. . For the above 3D dataset, we&#39;re going to squish it into a 2D pancake -- by projecting along the direction of the 3rd (purple) eigenvector onto the plane defined by the 1st (red) and 2nd (greenish) eigenvectors. . Yea, but how to do this projection? . Projecting the data . It&#39;s actually not that big of a deal. All we have to do is multiply by the eigenvector (matrix)! . OH WAIT! Hey, you want to see a cool trick!? Check this out: . lambdas, vs = sorted_eig(cov) proj_cov = vs.T @ cov @ vs # project the covariance matrix, using eigenvectors proj_cov . array([[ 1.07311435e+00, 4.47193943e-18, -4.45219967e-17], [ 5.25950058e-17, 2.67240036e-01, 6.70099547e-17], [-7.05170575e-17, 4.02835192e-17, 2.90836287e-02]]) . What was THAT? Let me clean that up a bit for you... . proj_cov[np.abs(proj_cov) &lt; 1e-15] = 0 proj_cov . array([[1.07311435, 0. , 0. ], [0. , 0.26724004, 0. ], [0. , 0. , 0.02908363]]) . Important point: What you just saw is the whole reason eigenvectors get used for so many things, because they give you a &#39;coordinate system&#39; where different &#39;directions&#39; decouple from each other. See, the system has its own (German: eigen) inherent set of orientations which are different the &#39;arbitrary&#39; coordinates that we &#39;humans&#39; may have assigned initially. . The numbers in that matrix are the covariances in the directions of the eigenvectors, instead of in the directions of the original x, y, and z. . So really all we have to do is make a coordinate transformation using the matrix of eigenvectors, and then in order to project we&#39;ll literally just drop a whole index&#39;s-worth of data-dimension in this new coordinate system. :-) . So, instead of $x$, $y$ and $z$, let&#39;s have three coordinates which (following physicist-notation) we&#39;ll call $q_1$, $q_2$ and $q_3$, and these will run along the directions given by the three eigenvectors. . Let&#39;s write our data as a N-by-3 matrix, where N is the number of data points we have. . data = np.stack((x,y,z),axis=1) data.shape # we had a 100 data points, so expecting 100x3 matrix . (100, 3) . There are two ways of doing this, and I&#39;ll show you that they&#39;re numerically equivalent: . Use all the eigenvectors to &quot;rotate&quot; the full dataset into the new coordinate system. Then perform a projection by truncating the last column of the rotated data. | Truncate the last eigenvector, which will make a 3x2 projection matrix which will project the data onto the 2D plane defined by those two eigenvectors. | Let&#39;s show them both: . print(&quot; n 1. All data, rotated into new coordinate system&quot;) W = vs[:,0:3] # keep the all the eigenvectors new_data_all = data @ W # project all the data print(&quot;Checking: new_data_all.shape =&quot;,new_data_all.shape) print(&quot;New covariance matrix = n&quot;,np.cov(new_data_all.T) ) print(&quot; n 2. Truncated data projected onto principal axes of coordinate system&quot;) W = vs[:,0:2] # keep only the first and 2nd eigenvectors print(&quot;W.shape = &quot;,W.shape) new_data_proj = data @ W # project print(&quot;Checking: new_data_proj.shape =&quot;,new_data_proj.shape) print(&quot;New covariance matrix in projected space = n&quot;,np.cov(new_data_proj.T) ) # Difference between them diff = new_data_all[:,0:2] - new_data_proj print(&quot; n Absolute maximum difference between the two methods = &quot;,np.max(np.abs(diff))) . 1. All data, rotated into new coordinate system Checking: new_data_all.shape = (100, 3) New covariance matrix = [[1.07311435e+00 7.64444687e-17 3.77081428e-17] [7.64444687e-17 2.67240036e-01 1.21906748e-16] [3.77081428e-17 1.21906748e-16 2.90836287e-02]] 2. Truncated data projected onto principal axes of coordinate system W.shape = (3, 2) Checking: new_data_proj.shape = (100, 2) New covariance matrix in projected space = [[1.07311435e+00 7.64444687e-17] [7.64444687e-17 2.67240036e-01]] Absolute maximum difference between the two methods = 0.0 . ...Nada. The 2nd method will be faster computationally though, because it doesn&#39;t calculate stuff you&#39;re going to throw away. . One more comparison between the two methods. Let&#39;s take a look at the &quot;full&quot; dataset (in blue) vs. the projected dataset (in red): . fig = go.Figure(data=[(go.Scatter3d(x=new_data_all[:,0], y=new_data_all[:,1], z=new_data_all[:,2], mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;full data&quot; ))]) fig.add_trace(go.Scatter3d(x=new_data_proj[:,0], y=new_data_proj[:,1], z=new_data_proj[:,0]*0, mode=&#39;markers&#39;, marker=dict(size=4,opacity=0.5), name=&quot;projected&quot; ) ) fig.update_layout(scene_aspectmode=&#39;data&#39;) fig.show() . . . (Darn it, [if only Plot.ly would support orthographic projections](https://community.plot.ly/t/orthographic-projection-for-3d-plots/3897) [[2](https://community.plot.ly/t/orthographic-projection-instead-of-perspective-for-3d-plots/10035)] it&#39;d be a lot easier to visually compare the two datasets!) . Beyond 3D . So typically we use PCA to throw out many more dimensions than just one. Often this is used for data visualization but it&#39;s also done for feature reduction, i.e. to send less data into your machine learning algorithm. (PCA can even be used just as a &quot;multidimensional linear regression&quot; algorithm, but you wouldn&#39;t want to!) . How do you know how many dimensions to throw out? . In other words, how many &#39;components&#39; should you choose to keep when doing PCA? There are a few ways to make this judgement call -- it will involve a trade-off between accuracy and computational speed. You can make a graph of the amount of variance you get as a function of how many components you keep, and often there will be a an &#39;elbow&#39; at some point on the graph indicating a good cut-off point to choose. Stay tuned as we do the next example; we&#39;ll make such a graph. For more on this topic, see this post by Arthur Gonsales. . Example: Handwritten Digits . The scikit-learn library uses this as an example and I like it. It goes as follows: . Take a dataset of tiny 8x8 pixel images of handwritten digits. | Run PCA to break it down from 8x8=64 dimensions to just two or 3 dimensions. | Show on a plot how the different digits cluster in different regions of the space. | (This part we&#39;ll save for the Appendix: Draw boundaries between the regions and use this as a classifier.) | To be clear: In what follows, each pixel of the image counts as a &quot;feature,&quot; i.e. as a dimension. Thus an entire image can be represented as a single point in a multidimensional space, in which distance from the origin along each dimension is given by the pixel intensity. In this example, the input space is not a 2D space that is 8 units wide and 8 units long, rather it consists of 8x8= 64 dimensions. . from sklearn.datasets import load_digits from sklearn.decomposition import PCA digits = load_digits() X = digits.data / 255.0 Y = digits.target print(X.shape, Y.shape,&#39; n&#39;) # Let&#39;s look a a few examples for i in range(8): # show 8 examples print(&quot;This is supposed to be a &#39;&quot;,Y[i],&quot;&#39;:&quot;,sep=&quot;&quot;) plt.imshow(X[i].reshape([8,8])) plt.show() . (1797, 64) (1797,) This is supposed to be a &#39;0&#39;: . This is supposed to be a &#39;1&#39;: . This is supposed to be a &#39;2&#39;: . This is supposed to be a &#39;3&#39;: . This is supposed to be a &#39;4&#39;: . This is supposed to be a &#39;5&#39;: . This is supposed to be a &#39;6&#39;: . This is supposed to be a &#39;7&#39;: . Now let&#39;s do the PCA thang... First we&#39;ll try going down to 2 dimensions. This isn&#39;t going to work super great but we&#39;ll try: . digits_cov = np.cov(X.T) print(&quot;digits_cov.shape = &quot;,digits_cov.shape) lambdas, vs = sorted_eig(np.array(digits_cov)) W = vs[:,0:2] # just keep two dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot fig = go.Figure(data=[go.Scatter(x=proj_digits[:,0], y=proj_digits[:,1],# z=Y, #z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=6, opacity=0.7, color=Y), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout( xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.update_layout(scene_camera=dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=0, z=0), eye=dict(x=0, y=0, z=1.5))) fig.show() . digits_cov.shape = (64, 64) proj_digits.shape = (1797, 2) . . . This is &#39;sort of ok&#39;: There are some regions that are mostly one kind of digit. But you may say there&#39;s there&#39;s too much intermingling between classes for a lot of this plot. So let&#39;s try it again with 3 dimensions for PCA: . W = vs[:,0:3] # just three dimensions proj_digits = X @ W print(&quot;proj_digits.shape = &quot;, proj_digits.shape) # Make the plot, separate them by &quot;z&quot; which is the digit of interest. fig = go.Figure(data=[go.Scatter3d(x=proj_digits[:,0], y=proj_digits[:,1], z=proj_digits[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.8, color=Y, showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y] )]) fig.update_layout(title=&quot;8x8 Handwritten Digits&quot;, xaxis_title=&quot;q_1&quot;, yaxis_title=&quot;q_2&quot;, yaxis = dict(scaleanchor = &quot;x&quot;,scaleratio = 1) ) fig.show() . proj_digits.shape = (1797, 3) . . . Now we can start to see some definition! The 6&#39;s are pretty much in one area, the 2&#39;s are in another area, and the 0&#39;s are in still another, and so on. There is some intermingling to be sure (particularly between the 5&#39;s and 8&#39;s), but you can see that this &#39;kind of&#39; gets the job done, and instead of dealing with 64 dimensions, we&#39;re down to 3! . Graphing Variance vs. Components . Earlier we asked the question of how many components one should keep. To answer this quantitatively, we note that the eigenvalues of the covariance matrix are themselves measures of the variance in the datset. So these eigenvalues encode the &#39;significance&#39; that each feature-dimension has in the overall dataset. We can plot these eigenvalues in order and then look for a &#39;kink&#39; or &#39;elbow&#39; in the graph as a place to truncate our representation... . plt.plot( np.abs(lambdas)/np.sum(lambdas) ) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Significance&#39;) plt.show() . ...So, if we were wanting to represent this data in more than 3 dimensions but less than the full 64, we might choose around 10 principal compnents, as this looks like roughly where the &#39;elbow&#39; in the graph lies. . Interpretability . What is the meaning of the new coordinate axes or &#39;features&#39; $q_1$, $q_2$, etc? Sometimes there exists a compelling physical intepretation of these features (e.g., as in the case of eigenmodes of coupled oscillators), but often...there may not be any. And yet we haven&#39;t even done any &#39;real machine learning&#39; at this point! ;-) . This is an important topic. Modern data regulations such as the European Union&#39;s GDPR require that models used in algorithmic decision-making be &#39;explainable&#39;. If the data being fed into such algorithmics is already abstracted via methods such as PCA, this could be an issue. Thankfully, the linearity of the components mean that one can describe each principal component as a linear combination of inputs. . Further reading . There are many books devoted entirely to the intricacies of PCA and its applications. Hopefully this post has helped you get a better feel for how to construct a PCA transformation and what it might be good for. To expand on this see the following... . Examples &amp; links . &quot;Eigenstyle: Principal Component Analysis and Fashion&quot; by Grace Avery. Uses PCA on Fashion-MNIST. It&#39;s good! | Neat paper by my friend Dr. Ryan Bebej from when he was a student and used PCA to classify locomotion types of prehistoric acquatic mammals based on skeletal measurements alone. | Andrew Ng&#39;s Machine Learning Course, Lecture on PCA. How I first learned about this stuff. | PCA using Python by Michael Galarnyk. Does similar things to what I&#39;ve done here, although maybe better! | Plot.ly PCA notebook examples | . Appendix A: Overkill: Bigger Handwritten Digits . Sure, 8x8 digit images are boring. What about 28x28 images, as in the MNIST dataset? Let&#39;s roll... . from sklearn.datasets import fetch_openml from random import sample #mnist = fetch_mldata(&#39;MNIST original&#39;) mnist = fetch_openml(&#39;mnist_784&#39;, version=1, cache=True) X2 = mnist.data / 255 Y2 = np.array(mnist.target,dtype=np.int) #Let&#39;s grab some indices for random suffling indices = list(range(X2.shape[0])) # Let&#39;s look a a few examples for i in range(8): # 8 is good i = sample(indices,1) print(&quot;This is supposed to be a &quot;,Y2[i][0],&quot;:&quot;,sep=&quot;&quot;) plt.imshow(X2[i].reshape([28,28])) plt.show() . This is supposed to be a 1: . This is supposed to be a 9: . This is supposed to be a 8: . This is supposed to be a 7: . This is supposed to be a 3: . This is supposed to be a 5: . This is supposed to be a 2: . This is supposed to be a 6: . mnist_cov = np.cov(X2.T) lambdas, vs = sorted_eig(np.array(mnist_cov)) W = vs[:,0:3] # Grab the 3 most significant dimensions # Plotting all 70,000 data points is going to be too dense too look at. # Instead let&#39;s grab a random sample of 5,000 points n_plot = 5000 indices = sample(list(range(X2.shape[0])), n_plot) proj_mnist = np.array(X2[indices] @ W, dtype=np.float32) # Last step of PCA: project fig = go.Figure(data=[go.Scatter3d(x=proj_mnist[:,0], y=proj_mnist[:,1], z=proj_mnist[:,2], mode=&#39;markers&#39;, marker=dict(size=4, opacity=0.7, color=Y2[indices], showscale=True), text=[&#39;digit=&#39;+str(j) for j in Y2[indices]] )]) fig.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: ComplexWarning: Casting complex values to real discards the imaginary part . . . ...bit of a mess. Not as cleanly separated as the 8x8 image examples. You can see that the 0&#39;s are well separated from the 1&#39;s and the 3&#39;s, but everything else is pretty mixed together. (I suspect the 1&#39;s are clustered strongly because they involve the most dark pixels.) . If you wanted to push this further then either keeping more dimensions (thereby making it un-visualizable) or just using a different method entirely (e.g. t-SNE or even better: UMAP) would be the way to go. Still, it&#39;s neat to see that you can get somewhat intelligible results in 3D even on this &#39;much harder&#39; problem. . Appendix B: Because We Can: Turning it into a Classifier . ...But let&#39;s not do a neural network because all I ever do are neural networks, and because I don&#39;t want to have take the time &amp; space to explain how they work or load in external libraries. Let&#39;s do k-nearest-neighbors instead, because it&#39;s intuitively easy to grasp and it&#39;s not hard to code up: . For any new point we want to evaluate, we take a &#39;vote&#39; of whatever some number (called $k$) of its nearest neighbor points are already assigned as, and we set the class of the new point according to that vote. . Making a efficient classifier is all about finding the *boundaries* between regions (and usually subject to some user-adjustable parameter like $k$ or some numerical threshold). Finding these boundaries can be about finding the &#39;edge cases&#39; that cause the system to &#39;flip&#39; (discontinuously) from one result to another. However, we are *not* going to make an efficient classifier today. ;-) . Let&#39;s go back to the 8x8 digits example, and split it into a training set and a testing set (so we can check ourselves): . indices = sample(list(range(X.shape[0])), X.shape[0]) X_shuf, Y_shuf = X[indices,:], Y[indices] # 80-20 train-test split max_train_ind = int(0.8*X.shape[0]) X_train, Y_train = X_shuf[0:max_train_ind,:], Y_shuf[0:max_train_ind] X_test, Y_test = X_shuf[max_train_ind:-1,:], Y_shuf[max_train_ind:-1] # Do PCA on training set train_cov = np.cov(X_train.T) ell, v = sorted_eig(np.array(train_cov)) pca_dims = 3 # number of top &#39;dimensions&#39; to take W_train = v[:,0:pca_dims] proj_train = X_train @ W_train # also project the testing set while we&#39;re at it proj_test = X_test @ W_train # yes, same W_train . Now let&#39;s make a little k-nearest neighbors routine... . from collections import Counter def knn_predict(xnew, proj_train, Y_train, k=3): &quot;&quot;&quot; xnew is a new data point that has the same shape as one row of proj_train. Given xnew, calculate the (squared) distance to all the points in X_train to find out which ones are nearest. &quot;&quot;&quot; distances = ((proj_train - xnew)**2).sum(axis=1) # stick on an extra column of indexing &#39;hash&#39; for later use after we sort dists_i = np.stack( (distances, np.array(range(Y_train.shape[0]) )),axis=1 ) dists_i = dists_i[dists_i[:,0].argsort()] # sort in ascending order of distance knn_inds = (dists_i[0:k,-1]).astype(np.int) # Grab the indexes for k nearest neighbors # take &#39;votes&#39;: knn_targets = list(Y_train[knn_inds]) # which classes the nn&#39;s belong to votes = Counter(knn_targets) # count up how many of each class are represented return votes.most_common(1)[0][0] # pick the winner, or the first member of a tie # Let&#39;s test it on the first element of the testing set x, y_true = proj_test[0], Y_test[0] guess = knn_predict(x, proj_train, Y_train) print(&quot;guess = &quot;,guess,&quot;, true = &quot;,y_true) . guess = 6 , true = 6 . Now let&#39;s try it for the &#39;unseen&#39; data in the testing set, and see how we do... . mistakes, n_test = 0, Y_test.shape[0] for i in range(n_test): x = proj_test[i] y_pred = knn_predict(x, proj_train, Y_train, k=3) y_true = Y_test[i] if y_true != y_pred: mistakes += 1 if i &lt; 20: # show some examples print(&quot;x, y_pred, y_true =&quot;,x, y_pred, y_true, &quot;YAY!&quot; if y_pred==y_true else &quot; BOO. :-(&quot;) print(&quot;...skipping a lot...&quot;) print(&quot;Total Accuracy =&quot;, (n_test-mistakes)/n_test*100,&quot;%&quot;) . x, y_pred, y_true = [ 0.06075339 0.00153272 -0.0477644 ] 6 6 YAY! x, y_pred, y_true = [ 0.04083212 0.09757529 -0.05361896] 1 1 YAY! x, y_pred, y_true = [-0.0199586 -0.00778773 0.00972962] 8 5 BOO. :-( x, y_pred, y_true = [ 0.02400112 -0.07267613 0.02774141] 0 0 YAY! x, y_pred, y_true = [0.01180771 0.03483923 0.07526469] 1 7 BOO. :-( x, y_pred, y_true = [ 0.00379226 -0.06269449 -0.00195609] 0 0 YAY! x, y_pred, y_true = [-0.06832135 -0.05396545 0.02980845] 9 9 YAY! x, y_pred, y_true = [-0.02397417 -0.04914796 0.0109273 ] 5 5 YAY! x, y_pred, y_true = [ 0.08213707 -0.01608953 -0.08072889] 6 6 YAY! x, y_pred, y_true = [ 0.03056858 -0.04852946 0.02204423] 0 0 YAY! x, y_pred, y_true = [-0.02124777 0.03623541 -0.01773196] 8 8 YAY! x, y_pred, y_true = [0.03035896 0.01398381 0.01415554] 8 8 YAY! x, y_pred, y_true = [ 0.0214849 0.02114674 -0.08951798] 1 1 YAY! x, y_pred, y_true = [ 0.07878152 0.03312015 -0.06488347] 6 6 YAY! x, y_pred, y_true = [-0.01294308 0.00158962 -0.01255491] 8 5 BOO. :-( x, y_pred, y_true = [ 0.01351581 0.11000321 -0.03895516] 1 1 YAY! x, y_pred, y_true = [0.0081306 0.01683952 0.05911389] 7 1 BOO. :-( x, y_pred, y_true = [0.06497268 0.02817075 0.07118004] 4 4 YAY! x, y_pred, y_true = [-0.03879657 -0.04460611 0.02833793] 9 5 BOO. :-( x, y_pred, y_true = [-0.05975051 0.03713843 -0.07174727] 2 2 YAY! ...skipping a lot... Total Accuracy = 76.88022284122563 % . ...eh! Not bad, not amazing. You can improve the accuracy if you go back up and increase the number of PCA dimensions beyond 3, and/or increase the value of $k$. Go ahead and try it! . (For 10 dimensions and $k=7$, I got 97.7% accuracy. The highest I ever got it was 99%, but that was really working overtime, computationally speaking; the point of PCA is to let you dramatically reduce your workload while retaining reasonably high accuracy.) . Just to reiterate: This is NOT supposed to be a state of the art classifier! It&#39;s just a toy that does pretty well and helps illustrate PCA without being hard to understand or code. . The End . Thanks for sticking around! Hope this was interesting. PCA is pretty simple, and yet really useful! ...and writing this really helped me to better understand it. ;-) .",
            "url": "https://hungpq7.github.io/data-science-blog/2019/12/21/pca-Copy1.html",
            "relUrl": "/2019/12/21/pca-Copy1.html",
            "date": " • Dec 21, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hungpq7.github.io/data-science-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hungpq7.github.io/data-science-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}