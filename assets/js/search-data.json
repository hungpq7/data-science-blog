{
  
    
        "post0": {
            "title": "Matplotlib Interface",
            "content": "1. Plotting interfaces . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Object-oriented interface . Every plot created by Matplotlib is under the control of two objects, figure and axes. . An figure object is the whole image generated by Matplotlib, where everything is drawn on. It is a top-level object and works as a container for a number of axes objects. | Each axes object usually refers to a 2-dimensional Cartesian coordinate system. It gives access to plot elements such as plots, labels, titles, text. | . fig = plt.figure(figsize=(4,3)) # add an axes with some text ax = fig.add_subplot() ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . Instead of creating figure and axes independently, Matplotlib provides a single function subplots() creates the two objects at once. This function is highly recommended in practice, and the introduction to the previous method is to clarify how figure and axes work and how they are related. . fig, ax = plt.subplots(figsize=(4,3)) ax.text(0.5, 0.5, &#39;DATA&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . When there are more than one axes, Matplotlib arranges them in a matrix of axes objects. Accessing each axes can be done using NumPy&#39;s array slicing. . fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,6), sharex=True, sharey=True) ax[0,2].text(0.5, 0.5, &#39;Row = 0 nColumn = 2&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) ax[1,1].text(0.5, 0.5, &#39;Row = 1 nColumn = 1&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=20) plt.show() . State-machine interface . Besides the object-oriented interface, Matplotlib also provides another way that makes use of state-machine to create plots. When using this, the state-machine implicitly and automatically creates figures and axes to achieve the desired plot. Then a set of simple functions is used to add plot elements to the current axes in the current figure. . Compared to object-oriented, state-machine interface is a bit more convenient for making a single axes, but it is not recommended for arranging multiple axes. Overall, object-oriented interface is still the go-to method. . plt.text(0.5, 0.5, &#39;Data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, size=32) plt.axvline(x=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.axhline(y=0.5, lw=1, color=&#39;g&#39;, linestyle=&#39;--&#39;) plt.title(&#39;State-machine interface&#39;) plt.show() . 2. Controlling axes . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Equalizing axes . Graphs require the two axes to have the same scale. . fig, ax = plt.subplots() # make the two axes scaled ax.axis(&#39;scaled&#39;) # set tick frequencies of both axes to be 0.02 ax.xaxis.set_major_locator(plt.MultipleLocator(0.02)) ax.yaxis.set_major_locator(plt.MultipleLocator(0.02)) plt.show() . Axes limits . fig, ax = plt.subplots() ax.axis(&#39;scaled&#39;) # set limit for each axis ax.set_xlim(0, 2) ax.set_ylim(0, 1) ax.xaxis.set_major_locator(plt.MultipleLocator(0.2)) plt.show() . Formatting axes . def axis_formatter(value, tick): N = int(np.round(2 * value / np.pi)) if N == 0: return &quot;0&quot; elif N == 2: return r&quot;$ pi$&quot; elif N == -2: return r&#39;$- pi$&#39; elif N % 2 == 1 and N &gt; 0: return fr&quot;$ frac{{{N}}}{{2}} pi$&quot; elif N % 2 == 1 and N &lt; 0: return fr&quot;$- frac{{{-N}}}{{2}} pi$&quot; else: return fr&quot;${N//2} pi$&quot; . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(10,10)) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) # set x-tick frequency to be pi/2 and apply a custom format strategy ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi/2)) ax.xaxis.set_major_formatter(plt.FuncFormatter(axis_formatter)) ax.set_ylim(-2.5, 2.5) plt.show() . 3. Plotting . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Graphs . Matplotlib does not really have a way to make graphs, but this can be achieved indirectly by using the axes.plot() method. The original functionality of this function is to plot a polyline connects data points. . It has an optional parameter, fmt that defines basic formatting following the syntax: &#39;{marker}{linestyle}{color}&#39;. The formatting string must not strictly follow the order in the syntax, but note that the parsing may be ambiguous. The table below summarizes some useful formatting strings: . Parameter Character Meaning . marker | &#39;.&#39; | point marker | . marker | &#39;o&#39; | big point marker | . linestyle | &#39;-&#39; | solid line style | . linestyle | &#39;--&#39; | dashed line style | . linestyle | &#39;:&#39; | dotted line style | . linestyle | &#39;-.&#39; | dash-dot line style | . color | &#39;k&#39; | black | . color | &#39;r&#39; | red | . color | &#39;c&#39; | cyan | . color | &#39;m&#39; | magenta | . color | &#39;g&#39; | green | . With different combinations of formatting strings, axes.plot() can result in graphs (indirectly) and points (directly). In order to make graphs, the input for x-axis needs to be a dense set of values. . x1 = np.linspace(-5, 10, 1000) y1 = np.sin(x1) + np.sin(2*x1) x2 = np.array([0, 4, 5.5, -4]) y2 = np.array([1, 2, 0, -2]) fig, ax = plt.subplots(figsize=(10,4)) ax.plot(x1, y1, &#39;k&#39;) ax.plot(x2, y2, &#39;oc&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Spans . x = np.linspace(-5, 10, 1000) y = np.sin(x) + np.sin(2*x) fig, ax = plt.subplots(figsize=(12,4)) # add two lines represent the two axes ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # add a vertical span across y-axis ax.axvspan(xmin=4.5, xmax=6.5, alpha=0.3) ax.plot(x, y, &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Vectors . The axes.quiver() method in Matplotlib allows plotting vectors. It has five important parameters, which must have the same length: . X and Y locate the origins of vectors | U and V define the vectors | color sets vector colors | . For some reasons, the parameters units=&#39;xy&#39; and scale=1 must always be set in order to draw vectors correctly. . fig, ax = plt.subplots() ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.quiver( [1, 0, 2], [1, 0, 0], [1, -3, -4], [2, 1, -2], color=[&#39;firebrick&#39;, &#39;teal&#39;, &#39;seagreen&#39;], units=&#39;xy&#39;, scale=1 ) ax.axis(&#39;scaled&#39;) ax.set_xlim(-4,3) ax.set_ylim(-3,4) plt.show() . 4. Annotations . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Text positioning . The axes.text() method adds text to the axes at the location defined by two parameters, x and y, use true coordinate values by default. By setting transform=ax.transAxes (highly recommended), these two parameters are now on a relative coordinates where (0,0) is the lower-left corner and (1,1) is the upper-right corner. . There is also a pair of parameters, ha and va (horizontal/vertical alignment), specify the relative position of the coordinates to the text box. Possible values for these parameters are left right center and top bottom center, respectively. . fig, ax = plt.subplots(figsize=(5,5)) ax.text(0.5, 0.5, &#39;(0.5, 0.5) ncenter center&#39;, va=&#39;center&#39;, ha=&#39;center&#39;, size=14, transform=ax.transAxes) ax.text(0, 0, &#39;(0, 0) nbottom left&#39;, va=&#39;bottom&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 1, &#39;(1, 1) ntop right&#39;, va=&#39;top&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.text(0, 1, &#39;(0, 1) ntop left&#39;, va=&#39;top&#39;, ha=&#39;left&#39;, size=14, transform=ax.transAxes) ax.text(1, 0, &#39;(1, 0) nbottom right&#39;, va=&#39;bottom&#39;, ha=&#39;right&#39;, size=14, transform=ax.transAxes) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.yaxis.set_major_formatter(plt.NullFormatter()) plt.show() . Plot titles . x = np.linspace(-1, 8, 1000) fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(5,7)) fig.suptitle(&#39;Mathematics&#39;, fontsize=16) ax[0].plot(x, np.sin(x)+2*np.sin(2*x), &#39;k&#39;) ax[0].set_title(&#39;$y= sin(x)+2 sin(x)$&#39;) ax[0].axis(&#39;equal&#39;) ax[0].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[0].yaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax[1].set_title(&#39;$y= sin(x2 pi) cdot e^{-x}$&#39;) ax[1].axis(&#39;scaled&#39;) ax[1].xaxis.set_major_locator(plt.MultipleLocator(1)) ax[1].yaxis.set_major_locator(plt.MultipleLocator(1)) plt.show() . Axis labels . x = np.linspace(-1, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x*2*np.pi)*np.exp(-x), &#39;k&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) # set axis labels ax.set_xlabel(&#39;x-axis&#39;) ax.set_ylabel(&#39;y-axis&#39;) plt.show() . Legends . The axes.lengend() method takes a list of plot names, which is indexed the same as the number of axes.plot() methods called. It can also use the value of label parameter in each axes.plot() method to generate the legend. . By default, Matplotlib automatically determines the best location for the legend inside the plot. This can be changed using the parameter loc, which specifies locations using strings such as &quot;lower center&quot; and &quot;upper right&quot;. When there is no optimized location like in the example below, the legend should be placed outside of the axes. This can be achieved using bbox_to_anchor alongside with loc; this pair of parameters behaves exactly the same as text positioning. . Another useful configuration is the ncol parameter. It specifies the number of columns in the grid of legend labels. . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) # pass the list of labels into the ax.legend method ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x), &#39;:k&#39;, label=&#39;$ sin(x)$&#39;) ax.plot(x, np.sin(2*x), &#39;--k&#39;, label=&#39;$ sin(2x)$&#39;) ax.plot(x, np.sin(x)+np.sin(2*x), &#39;r&#39;, label=&#39;$ sin(x)+ sin(2x)$&#39;) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend(bbox_to_anchor=(0.5, 1), loc=&#39;lower center&#39;, ncol=3, title=&#39;Graphs&#39;) plt.show() . 5. Themes and colors . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . Themes . Matplotlib supports a variety of themes such as ggplot seaborn tableau-colorblind10 inspired by other popular visualization tools. However, since drawing graphs requires a white background, then I usually use seaborn-whitegrid. . plt.style.use(&#39;ggplot&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . plt.style.use(&#39;seaborn&#39;) x = np.linspace(-4, 4, 1000) fig, ax = plt.subplots() ax.plot(x, np.sin(x)) ax.plot(x, np.sin(2*x)) ax.plot(x, np.sin(x)+np.sin(2*x)) ax.axis(&#39;scaled&#39;) ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.axhline(y=0, c=&#39;grey&#39;) ax.axvline(x=0, c=&#39;grey&#39;) ax.legend([&#39;$ sin(x)$&#39;, &#39;$ sin(2x)$&#39;, &#39;$ sin(x)+ sin(2x)$&#39;], bbox_to_anchor=(1, 0.5), loc=&#39;center left&#39;) plt.show() . Built-in Matplotlib colors . Besides themes, Matplotlib also provides a collection of colors for better visualization. Some nice colors are: dimgrey indianred tomato goldenrod seagreen teal darkturquoise darkslategrey slategrey royalblue rebeccapurple. . .",
            "url": "https://hungpq7.github.io/data-science-blog/visualization/matplotlib/2022/08/24/matplotlib-graph.html",
            "relUrl": "/visualization/matplotlib/2022/08/24/matplotlib-graph.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Algebra",
            "content": "1. Notation . From now on, the following notations are applied: . Regular, lowercase letters are used for scalars). For example: | . $$x=5, ; alpha=0.2$$ . Bold, lowercase letters are used for column vectors. Adding a number in square brackets specifies the entry in the corresponding position. For example: | . $$ mathbf{x}= begin{bmatrix}2 4 5 end{bmatrix}, ; mathbf{x}[2]=4$$ . For row vectors, add a transpose operator to the the column form. For example: | . $$ mathbf{x}^{ text{T}}= begin{bmatrix}2 &amp; 4 &amp; 5 end{bmatrix}$$ . Bold, uppercase letters are for matrices). Adding $M times N$ as subscript indicates that the matrix has $M$ rows and $N$ columns. Adding $m,n$ as subscript specifies the entry in the $m^{th}$ row and the $n^{th}$ column. For example: | . $$ mathbf{A}_{3 times4}= begin{bmatrix}1&amp;2&amp;3&amp;4 5&amp;6&amp;7&amp;8 9&amp;10&amp;11&amp;12 end{bmatrix}, ; mathbf{A}_{2,3}= mathbf{A}[2,3]=7$$ . 2. Special matrices . import numpy as np np.set_printoptions(precision=4, suppress=True) . Square matrix . A square matrix is a matrix with the same number of rows and columns. . np.random.random((3,3)) . array([[0.57084056, 0.19949711, 0.06148951], [0.57086295, 0.24929257, 0.0558644 ], [0.33376222, 0.36694373, 0.09584761]]) . np.arange(9).reshape(3,3) . array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) . Unit matrix . A unit matrix (sometimes called identity matrix) is a square matrix with ones on the main diagonal and zeros elsewhere, denoted by $ mathbf{I}_N$ where $N$ is the length of each size. . np.eye(N=5, dtype=int) . array([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]]) . Diagonal matrix . A diagonal matrix is a square matrix in which the elements outside the main diagonal are all zero. Given $ mathbf{a}= begin{bmatrix}1 &amp; 2 &amp; 3 end{bmatrix}^{ text{T}}$, a diagonal matrix can be represented as below: . $$ begin{bmatrix}1&amp;0&amp;0 0&amp;2&amp;0 0&amp;0&amp;3 end{bmatrix}= text{diag}( mathbf{a})= text{diag}(1,2,3)$$ . array = np.random.randint(10, size=(4,4)) np.diag(np.diag(array)) . array([[6, 0, 0, 0], [0, 8, 0, 0], [0, 0, 7, 0], [0, 0, 0, 4]]) . 3. Matrix operations . import numpy as np np.set_printoptions(precision=4, suppress=True) . Addition . The matrices to be used in addition (or subtraction) must be of the same shape. The sum of $ mathbf{A} in mathbb{R}^{M times N}$ and $ mathbf{B} in mathbb{R}^{M times N}$ is calculated as: . $$( mathbf{A}+ mathbf{B})_{m,n} = mathbf{A}_{m,n} + mathbf{B}_{m,n} qquad text{for }m in {1,2, dots,M } text{ and }n in {1,2, dots,N }$$ . A = np.array([[1, 2, 3], [4, 5, 6]]) . B = np.array([[7, 5, 8], [2, 0, 7]]) . A + B . array([[ 8, 7, 11], [ 6, 5, 13]]) . B - A . array([[ 6, 3, 5], [-2, -5, 1]]) . Alternatively, np.add() can be used. . np.add(A, B) . array([[ 8, 7, 11], [ 6, 5, 13]]) . Scalar multiplication . For a scalar $c$ and a matrix $ mathbf{A} in mathbb{R}^{M times N}$, the multiplication can be computed: $(c mathbf{A})_{m,n} = c cdot mathbf{A}_{m,n}$ . A = np.array([[1, 2, 3], [4, 5, 6]]) . A * 2 . array([[ 2, 4, 6], [ 8, 10, 12]]) . Matrix multiplication . The dot product of $ mathbf{A} in mathbb{R}^{M times N}$ and $ mathbf{B} in mathbb{R}^{N times P}$ is a matrix $ mathbf{C} in mathbb{R}^{M times P}$ where the element of $ mathbf{C}$ in the $m^{th}$ row and $p^{th}$ column is calculated by the formula: $$ mathbf{C}_{m,p} = sum_{n=1}^N { mathbf{A}_{m,n} mathbf{B}_{n,p}}$$ . A = np.linspace(1, 6, 6, dtype=int).reshape(2, 3) B = np.linspace(1, 12, 12, dtype=int).reshape(3, 4) . A . array([[1, 2, 3], [4, 5, 6]]) . B . array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]]) . np.dot(A, B) . array([[ 38, 44, 50, 56], [ 83, 98, 113, 128]]) . A @ B . array([[ 38, 44, 50, 56], [ 83, 98, 113, 128]]) . Transposition . Transposition is an operator where the matrix is flipped over its diagonal. The transpose of the matrix $ mathbf{A} in mathbb{R}^{M times N}$ is denoted $( mathbf{A}^{ text{T}}) in mathbb{R}^{N times M}$ and satisfies $ mathbf{A}_{m,n}$ = $( mathbf{A}^{ text{T}})_{n,m}$ for $m in {1,2, dots,M }$ and $n in {1,2, dots,N }$. . A = np.arange(24).reshape(3,8) A . array([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23]]) . A.T . array([[ 0, 8, 16], [ 1, 9, 17], [ 2, 10, 18], [ 3, 11, 19], [ 4, 12, 20], [ 5, 13, 21], [ 6, 14, 22], [ 7, 15, 23]]) . np.diag(A) == np.diag(A.T) . array([ True, True, True]) . Properties . Here are some properties of matrix operations: . $ mathbf{A}+ mathbf{B} = mathbf{B}+ mathbf{A}$ (commutativity) | $(c mathbf{A})^{ text{T}} = c cdot mathbf{A}^{ text{T}}$ | $( mathbf{A}+ mathbf{B})^{ text{T}} = mathbf{A}^{ text{T}} + mathbf{B}^{ text{T}}$ | $( mathbf{A}^{ text{T}})^{ text{T}} = mathbf{A}$ | $( mathbf{A} mathbf{B}) mathbf{C} = mathbf{A}( mathbf{B} mathbf{C}) = mathbf{A} mathbf{B} mathbf{C}$ (associativity) | $( mathbf{A}+ mathbf{B}) mathbf{C} = mathbf{A} mathbf{C} + mathbf{B} mathbf{C}$ (distributivity) | $ mathbf{A} mathbf{B} neq mathbf{B} mathbf{A}$ | . 4. Echelon form and rank . import scipy import numpy as np np.set_printoptions(precision=4, suppress=True) . Gaussian elimination . Gaussian elimination (also known as row reduction) is a linear algebra algorithm for solving a system of linear equations; finding the rank, the determinant and the inverse of matrices. There are three types of elementary matrix operations: . Interchanging two rows | Multiplying a row by a non-zero number | Adding a multiple of a row to another row | . Echelon form . Using Gaussian elimination, any matrix can be transformed to the row echelon form or column echelon form. From now on, echelon form refers to row echelon. . $$ begin{bmatrix} a &amp; * &amp; * &amp; * &amp; * &amp; * &amp; * &amp; * &amp; * 0 &amp; 0 &amp; b &amp; * &amp; * &amp; * &amp; * &amp; * &amp; * 0 &amp; 0 &amp; 0 &amp; c &amp; * &amp; * &amp; * &amp; * &amp; * 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; d &amp; * &amp; * 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; e &amp; * 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 end{bmatrix} $$A matrix is in echelon form when it satisfies the following conditions: . All zero rows (rows with all zero elements) are at the bottom. | Each pivot (or leading entry) - the leftmost non-zero element of each row is always to the right of the above pivot. In the example above, $a$, $b$, $c$, $d$ and $e$ are pivots. | . A = np.array( [[0, 1, 2], [1, 2, 1], [2, 7, 8]] ) import scipy.linalg _, _, echelon = scipy.linalg.lu(A) echelon . array([[ 2. , 7. , 8. ], [ 0. , -1.5, -3. ], [ 0. , 0. , 0. ]]) . echelon.astype(int) . array([[ 2, 7, 8], [ 0, -1, -3], [ 0, 0, 0]]) . Rank-nullity theorem . The rank) of a matrix is the highest number of linear independent rows in the matrix. A vector is linear dependent if it equals to the sum of scalar multiples of other vectors. The maximum rank of the matrix $ mathbf{A} in mathbb{R}^{M times N}$ is $ min(M,N)$. $ mathbf{A}$ is said to be full rank if its rank reaches the maximum value and to be rank deficient otherwise. This can be expressed using the rank-nullity theorem: . $$ text{rank}(A)+ text{nullity}(A)= min(M,N)$$ . np.linalg.matrix_rank(A) . 2 . 5. Determinant and inverse . These attributes are defined only for square matrices. . import scipy import numpy as np np.set_printoptions(precision=4, suppress=True) . Minors and cofactors . The minor) and the cofactor are required to calculate the determinant and the inverse of square matrices. Given a square matrix $ mathbf{A} in mathbb{R}^{N times N}$ and $i,j in {1,2, dots,N }$: . The $(i,j)$ minor, denoted $m_{ij}$, is the determinant of the $(N-1) times(N-1)$ sub-matrix formed by deleting the $i^{th}$ row and the $j^{th}$ column. | The corresponding cofactor is $ mathbf{C}_{i,j}=(-1)^{i+j} ,m_{ij}$. | . From the calculated cofactors, we can build the matrix of cofactors (adjugate matrix): . $$ mathbf{C} = begin{bmatrix} mathbf{C}_{1,1} &amp; mathbf{C}_{1,2} &amp; dots &amp; mathbf{C}_{1,n} mathbf{C}_{2,1} &amp; mathbf{C}_{2,2} &amp; dots &amp; mathbf{C}_{2,n} vdots &amp; vdots &amp; ddots &amp; vdots mathbf{C}_{n,1} &amp; mathbf{C}_{n,2} &amp; dots &amp; mathbf{C}_{n,n} end{bmatrix} $$ Determinant . The determinant of a $2 times 2$ matrix is: $ begin{vmatrix} a_{1,1} &amp; a_{1,2} a_{2,1} &amp; a_{2,2} end{vmatrix} = a_{1,1}a_{2,2} - a_{1,2}a_{2,1} $ . To calculate the determinant of the matrix $ mathbf{A} in mathbb{R}^{N times N}$ (denoted $| mathbf{A}|$), use the Laplace formula (a recursion with the base case of $N=2$): . $$ det left( mathbf{A} right) = | mathbf{A}| = sum_{i=1}^N a_{ij}(-1)^{i+j}m_{ij} = sum_{i=1}^N a_{ij}C_{ij}$$ where: $i, j in {1,2,...,N }$. . import scipy import numpy as np np.set_printoptions(precision=4, suppress=True) . np.random.seed(0) A = np.random.randint(1, 10, (5,5)) A . array([[6, 1, 4, 4, 8], [4, 6, 3, 5, 8], [7, 9, 9, 2, 7], [8, 8, 9, 2, 6], [9, 5, 4, 1, 4]]) . np.linalg.det(A) . -1553.0000000000011 . Inverse . The inverse of the square matrix $ mathbf{A} in mathbb{R}^{N times N}$ is denoted $ mathbf{A}^{-1}$ and satisfies $ mathbf{A} cdot mathbf{A}^{-1} = mathbf{I}_N$ where $ mathbf{I}_N$ is the unit matrix. $ mathbf{A}$ is invertible only if $| mathbf{A}| ne 0$. The formula for inversion is: . $$ mathbf{A}^{-1} = frac{1}{| mathbf{A}|}C^{ text{T}}$$ . Infered from the above formula, a square matrix having determinant of $0$ has no inverse. Such a matrix is called a singular matrix. Otherwise, the matrix is said to be invertible. . A = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]]) np.linalg.inv(A) . array([[-24., 18., 5.], [ 20., -15., -4.], [ -5., 4., 1.]]) . Let&#39;s check if the dot product of $ mathbf{A}$ and $ mathbf{A}^{-1}$ is an identity matrix. . A @ np.linalg.inv(A).astype(int) . array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) . 6. Eigen-stuffs . Eigenvalues . Eigenvalues and eigenvectors are defined only for square matrices. Each eigenvalue $( lambda)$ and a corresponding eigenvector $( mathbf{x})$ of the square matrix $ mathbf{A}$ satisfies: $ mathbf{A} mathbf{x} = lambda mathbf{x}$. To find eigenvalues and eigenvectors: . Solve the equation: $| mathbf{A} - lambda mathbf{I}| = mathbf{0}$ (called the characteristic equation) for $ lambda$. | For each value of $ lambda$ found, solve the equation $( mathbf{A}- lambda) ; mathbf{x}= mathbf{0}$ or use Gaussian elimination to convert the augmented matrix $( mathbf{A} - lambda mathbf{I} mid0)$ to echelon form then solve it as a system of linear equations. | . Note that there are an infinite number of linear dependent eigenvectors associated with a single eigenvalue. . import scipy import numpy as np np.set_printoptions(precision=4, suppress=True) . A = np.array([[3, 4, -2], [1, 4, -1], [2, 6, -1]]) eigenvalues, eigenvectors = np.linalg.eig(A) . eigenvalues . array([3., 2., 1.]) . eigenvalues.sum() . 6.000000000000002 . eigenvectors # each column is an eigenvector . array([[-0.4082, 0. , 0.7071], [-0.4082, 0.4472, -0. ], [-0.8165, 0.8944, 0.7071]]) . eigenvectors[:, 0] . array([-0.4082, -0.4082, -0.8165]) . Now calculate the value of $ mathbf{A} mathbf{x}$ and $ lambda mathbf{x}$ and check if they are equal. . lhs = A @ eigenvectors[:,0] rhs = eigenvalues[0] * eigenvectors[:,0] np.allclose(lhs, rhs) . True . Here are some properties of eigenvalues: . The sum of all eigenvalues is the same as the trace of $ mathbf{A}$. | The product of all eigenvalues is the same as the determinant of $ mathbf{A}$. | The number of eigenvalues equals to the rank of $ mathbf{A}$. | $ mathbf{A}$ and $ mathbf{A}^T$ have the same set of eigenvalues. | . Eigendecomposition . Eigendecomposition of a square matrix $ mathbf{A} in mathbb{R}^{N times N}$ is the factorization that is based on its eigenvalues and eigenvectors. $ mathbf{A}$ can be written as: . $$ mathbf{A}= mathbf{Q} mathbf{D} mathbf{Q}^{-1}$$ . Note that eigendecomposition is only applied for non-defective matrices (matrices that are diagonizable) . import scipy import numpy as np from numpy.linalg import eig, inv, matrix_power np.set_printoptions(precision=4, suppress=True) . A = np.array( [[3, 4, -2], [1, 4, -1], [2, 6, -1]], dtype=float, ) eigenvalues, eigenvectors = eig(A) D = np.diag(eigenvalues) Q = eigenvectors . lhs = A rhs = Q @ D @ inv(Q) np.allclose(lhs, rhs) . True . Properties of eigendecomposition: . We can easily prove that: $ mathbf{A}^k= mathbf{Q} mathbf{D}^k mathbf{Q}^{-1}$. By letting $k=-1$, this property can be used to compute the inverse of $ mathbf{A}$. | When $ mathbf{A}$ is a symmetric matrix ($ mathbf{A}= mathbf{A}^{ text{T}}$), it is always diagonalizable and its eigenvectors are orthonormal. Thus, $ mathbf{Q}$ becomes an orthogonal matrix. This wonderful property is the foundation of famous PCA (Principle Components Analysis). | . power = 5 lhs = matrix_power(A, power) rhs = Q @ matrix_power(L, power) @ inv(Q) np.allclose(lhs, rhs) . True . Diagonalization . Diagonalization is the process of transform a square matrix to a diagonal matrix so that it retains the main properties. By performing eigendecomposition, we are diagonalizing $ mathbf{A}$ into $ mathbf{D}$. Let&#39;s analyze the situation to understand how $ mathbf{D}$ shares the same characteristics with $ mathbf{A}$. . $$ begin{align} mathbf{Ax} &amp;= mathbf{y} Leftrightarrow mathbf{Q} mathbf{D} mathbf{Q}^{-1} mathbf{x} &amp;= mathbf{y} Leftrightarrow mathbf{D} left( mathbf{Q}^{-1} mathbf{x} right) &amp;= left( mathbf{Q}^{-1} mathbf{y} right) end{align} $$In the first equation, $ mathbf{y}$ is the image of $ mathbf{x}$ in the new coordinates system proposed by $ mathbf{A}$. In the third equation, notice that both $ mathbf{x}$ and $ mathbf{y}$ are transformed in the same way. We can conclude that $ mathbf{D}$ represents the simplest form of $ mathbf{A}$ that requires less entries ($N$ instead of $N^2$). . 9. SVD . SVD (Singular Value Decomposition) is the generalization of eigendecomposition of a square matrix to any rectangular matrix. In Machine Learning, it has a lot of applications such as finding pseudo-inverse and solving for least squares problems. SVD says that any matrix $ mathbf{A} in mathbb{R}^{M times N}$ can be factorized into: . $$ mathbf{A}= mathbf{U} mathbf{S} mathbf{V}^{ text{T}}$$ . where: . $ mathbf{U} in mathbb{R}^{M times M}$ stores the eigenvectors of $ mathbf{A} mathbf{A}^{ text{T}}$ | $ mathbf{V} in mathbb{R}^{N times N}$ stores the eigenvectors of $ mathbf{A}^{ text{T}} mathbf{A}$ | $ mathbf{S} in mathbb{R}^{M times N}$ is a diagonal matrix with singular values lying on the diagonal | . import scipy import numpy as np from numpy.linalg import svd np.set_printoptions(precision=4, suppress=True) import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . A = np.array([ [4, 9, 4, 6], [5, 7, 4, 5], [3, 1, 7, 9], ]) U, S, Vh = svd(A) . U . array([[-0.6253, -0.453 , 0.6354], [-0.5556, -0.3133, -0.7702], [-0.548 , 0.8346, 0.0558]]) . S = np.diag(S) . Vh . array([[-0.368 , -0.5351, -0.455 , -0.6093], [-0.1257, -0.781 , 0.3991, 0.4637], [-0.8789, 0.295 , -0.1142, 0.3571], [ 0.2763, -0.1293, -0.7878, 0.535 ]]) . Properties . Notice that both $ mathbf{A} mathbf{A}^{ text{T}}$ and $ mathbf{A}^{ text{T}} mathbf{A}$ are symmetric matrices, thus their eigenvectors are orthogonal. As $ mathbf{U}$ and $ mathbf{V}$ are constructed of these vectors, we call them orthogonal matrices. An orthogonal matrix is unitary: . $$ mathbf{U} mathbf{U}^{ text{T}}= mathbf{U} mathbf{U}^{-1}= mathbf{I}$$ . $ mathbf{A} mathbf{A}^{ text{T}}$ and $ mathbf{A}^{ text{T}} mathbf{A}$ in fact share the same set of positive eigenvalues. It turns out, their square roots are the singular values which lies on the diagonal of $ mathbf{S}$. This relationship can be expressed using the eigendecomposition of either $ mathbf{A} mathbf{A}^{ text{T}}$ or $ mathbf{A}^{ text{T}} mathbf{A}$: . $$ begin{align*} mathbf{A} mathbf{A}^{ text{T}} &amp;= left( mathbf{U} mathbf{S} mathbf{V}^{ text{T}} right) left( mathbf{U} mathbf{S} mathbf{V}^{ text{T}} right)^{ text{T}} &amp;= mathbf{U} mathbf{S} mathbf{V}^{ text{T}} mathbf{V} mathbf{S}^{ text{T}} mathbf{U}^{ text{T}} &amp;= mathbf{U}( mathbf{S} mathbf{S}^{ text{T}}) mathbf{U}^{-1} end{align*}$$As $ mathbf{S}$ is a diagonal matrix, to calculate $ mathbf{S} mathbf{S}^{ text{T}}$ we only need to square the elements on the diagonal of $ mathbf{S}$. In other words, squaring singular values of $ mathbf{A}$ gives us the eigenvalues of $ mathbf{A} mathbf{A}^{ text{T}}$. . 10. Geometric meaning . def plot_vectors(data): data = np.array(data).T num_vectors = data.shape[1] initial = np.zeros((2, num_vectors), dtype=float) data = np.concatenate([initial, data], axis=0) default = np.array([0, 1e-6, 0, 0, &#39;black&#39;]).reshape(-1, 1) data = np.concatenate([default, data], axis=1) c = data[4] data = data[:4].astype(float) x, y, u, v = data edge = 0.5 right = np.max(x + u) if right &lt; 0: right = 0 right += edge left = np.min(x + u) if left &gt; 0: left = 0 left -= edge upper = np.max(y + v) if upper &lt; 0: upper = 0 upper += edge lower = np.min(y + v) if lower &gt; 0: lower = 0 lower -= edge fig, ax = plt.subplots() ax.xaxis.set_major_locator(plt.MultipleLocator(1)) ax.yaxis.set_major_locator(plt.MultipleLocator(1)) ax.quiver(x, y, u, v, color=c, units=&#39;xy&#39;, scale=1 ) ax.axis(&#39;scaled&#39;) ax.set_xlim(left, right) ax.set_ylim(lower, upper) plt.show() . . Scalars . A scalar $c$ when times a vector $ mathbf{a}$ has the following effects: . If $|c|&gt;1$ then $ mathbf{a}$ stretches, if $|c|&lt;1$ then $ mathbf{a}$ shrinks. | The sign of $c$ tells us the direction of the new vector: positive means same direction, negative means opposite direction. | . data = [ [8, 4, &#39;deepskyblue&#39;], [-2, -1, &#39;coral&#39;], [4, 2, &#39;black&#39;], ] plot_vectors(data) . Column vectors . In a 2-dimensional $xy$-coordinates, we conventionally call $ mathbf{i}_1$ and $ mathbf{i}_2$ the basis/unit vectors: . $$ mathbf{i}_1= begin{bmatrix}1 0 end{bmatrix}, ; mathbf{i}_2= begin{bmatrix}0 1 end{bmatrix}$$ . Any vector $ mathbf{a}$ can always be represented by these basis vectors, for example: . $$ mathbf{a}= begin{bmatrix}3 2 end{bmatrix}=3 mathbf{i}_1+2 mathbf{i}_2$$ . Note that there are an infinite number of representations of $ mathbf{a}$. By convention, I use the origin as the initial point for all vectors. In other words, only the terminal point is neccessary to represent a vector. . data = [ [3, 2, &#39;deepskyblue&#39;], [3, 0, &#39;coral&#39;], [0, 2, &#39;coral&#39;], [1, 0, &#39;black&#39;], [0, 1, &#39;black&#39;], ] plot_vectors(data) . Square matrices . A square matrix represents a change of basis vectors. For example, let&#39;s consider the matrix: . $$ mathbf{A}= begin{bmatrix}2&amp;1 0&amp;3 end{bmatrix}$$ . $ mathbf{A}$ represents a new coordinates system, in which $( mathbf{a}_1, mathbf{a}_2)$ become the new basis: . $$ mathbf{a}_1= begin{bmatrix}2 0 end{bmatrix}, ; mathbf{a}_2= begin{bmatrix}1 3 end{bmatrix}$$ . data = [ [2, 0, &#39;coral&#39;], [1, 3, &#39;coral&#39;], [1, 0, &#39;black&#39;], [0, 1, &#39;black&#39;], ] plot_vectors(data) . Dot product . In this part, we are going to discover the geometric meaning of the dot a product between a square matrix $ mathbf{A}_{N times N}$ with a column vector $ mathbf{b}_{N times 1}$. As far as we know, a square matrix can be thought as a container of new basis, where each column represents a basis vector: $ mathbf{A}=[ mathbf{a}_1, mathbf{a}_2, dots, mathbf{a}_N]$. For a specific case of $N=2$, the dot product $ mathbf{Ab}$ is derived as follow: . $$ mathbf{A} mathbf{b} = begin{bmatrix} begin{array}{r} 2&amp;-1 0&amp;1 end{array} end{bmatrix} begin{bmatrix} begin{array}{r} 3 4 end{array} end{bmatrix} = 3 begin{bmatrix} begin{array}{r} 2 0 end{array} end{bmatrix} + 4 begin{bmatrix} begin{array}{r} -1 1 end{array} end{bmatrix} = 3 mathbf{a}_1+4 mathbf{a}_2 $$By treating $ mathbf{A}$ as a set of new basis vectors, $( mathbf{a}_1, mathbf{a}_2)$, we can now construct a brand new coordinates system. The geometric meaning of the product $ mathbf{Ab}$ is where $ mathbf{b}$ points to in the new coordinates. A special case of dot product is when $ mathbf{A}$ is a unit matrix, then we have $ mathbf{b}= mathbf{A} mathbf{b}$. This equation tells us a unit matrix is just the same old representation of the original basis. . data = [ [2, 4, &#39;deepskyblue&#39;], [6, 0, &#39;coral&#39;], [-4, 4, &#39;coral&#39;], [2, 0, &#39;black&#39;], [-1, 1, &#39;black&#39;], ] plot_vectors(data) . Determinant . The determinant of a matrix $ mathbf{M}_{2 times 2}= begin{bmatrix} begin{array}{r} a&amp;c b&amp;d end{array} end{bmatrix}$ is the area of the parallellogram formed by new basis vectors. . . For the matrix $ mathbf{R}_{3 times 3}= left[ begin{array}{c|c|c|c} mathbf{r}_1 &amp; mathbf{r}_2 &amp; mathbf{r}_3 end{array} right]$, its determinant is the volume of the parallelepiped formed by 3 new basis vectors. . . Note: A square matrix has the determinant of $0$ only when there is at least one of its vectors is linear dependent on the others. Assume the matrix $ mathbf{R}_{3 times 3}$ above has the determinant of $0$, then it can be one of the following situations: . There are at least one vector is $ mathbf{0}$ | There are at least a pair of vectors is parallel | All three vectors lies in the same plane | . Basically, if $ det left( mathbf{R} right)=0$ then the new coordinates system has less than 3 dimensions. . Inverse . If a matrix $ mathbf{A}_{N times N}$ changes the basis, then $ mathbf{A}^{-1}$ is the inverse transform that changes back to the old coordinates. The reason behind this is the equation $ mathbf{A} cdot mathbf{A}^{-1} = mathbf{I}_N$. . If $ det left( mathbf{A} right)=0$ then $ mathbf{A}$ maps the original coordinates into a new, lower dimensional space. In this case, the inverse transform cannot be performed; in other words, $ mathbf{A}$ is non-invertible. .",
            "url": "https://hungpq7.github.io/data-science-blog/linear-algebra/2022/08/24/linear-algebra.html",
            "relUrl": "/linear-algebra/2022/08/24/linear-algebra.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Ensemble Learning",
            "content": "1. Overview . Classical Machine Learning algorithms are usually shown to be poor when handling real-world datasets. Models fit from these algorithms often suffer from two problems: high bias and high variance; such a model is called a weak learner. In this topic, we are going through some elegant techniques that combine multiple weak learners to form a powerful model, which produces an improved overall result. This is referred to generally as Ensemble Learning. Enemble Learning methods have proven their effectiveness in many Machine Learing competitions. . Big picture . Stacking (Wolpert, 1992) Voting | Stacking | Blending | . | Bagging RandomForest (Breiman, 1995) | ExtraTrees (Geurts, 2006) | . | Boosting Adaptive Boosting AdaBoost (Freund and Schapire, 1995) | . | Gradient Boosting GBDT (Friedman, 2001) XGBoost (Chen and Guestrin, 02/2014) | LightGBM (Guolin, 08/2016) | CatBoost (Yandex, 07/2017) | NGBoost (Duan and Avati, 06/2018) | . | . | . | . 2. Stacking . Voting . Voting (for classification) or Averaging (for regression) is the simplest ensembling method. When doing voting for classification, there are two strategies can be applied: marjority voting on predicted results (hard voting) and taking argmax of the weighted average of predicted probabilities (soft voting). . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import VotingClassifier, VotingRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . clf1 = SVC(probability=True) clf2 = LogisticRegression(solver=&#39;liblinear&#39;) clf3 = DecisionTreeClassifier() modelsBase = [clf1, clf2, clf3] modelsBaseNamed = [(model.__class__.__name__, model) for model in modelsBase] ensembler = VotingClassifier(modelsBaseNamed, voting=&#39;soft&#39;) . for model in modelsBase + [ensembler]: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9464 [SVC] AUC = 0.9931 [LogisticRegression] AUC = 0.8512 [DecisionTreeClassifier] AUC = 0.9851 [VotingClassifier] . Stacking . Stacking technique organizes its members into two levels: . Level 1, a number of base models is fit to ther dataset. Build a new dataset where the values predicted by base models are input variables while the output variable remains the same. | Level 2, a meta model is train on the new dataset to get final prediction. | . The idea behind stacking is that each base model has an unique approach, it might discover some parts of the ground truth that other models do hot have. Combining them might utilize the their strengths and thus improve the overall quality. Note that Voting is a special case of Stacking, where the final combiner is a very simple model. . In the implementation of Stacking, the base models are often selected heterogeneously, and the meta model is often a simple Logistic Regression model. . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import StackingClassifier, StackingRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . clf1 = KNeighborsClassifier() clf2 = GaussianNB() clf3 = SVC(probability=True) clf4 = LogisticRegression(solver=&#39;liblinear&#39;) clf5 = DecisionTreeClassifier() modelsBase = [clf1, clf2, clf3, clf4, clf5] modelsBaseNamed = [(model.__class__.__name__, model) for model in modelsBase] modelMeta = LogisticRegression() ensembler = StackingClassifier(modelsBaseNamed, modelMeta) . for model in modelsBase + [ensembler]: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9568 [KNeighborsClassifier] AUC = 0.9775 [GaussianNB] AUC = 0.9464 [SVC] AUC = 0.9931 [LogisticRegression] AUC = 0.8442 [DecisionTreeClassifier] AUC = 0.9891 [StackingClassifier] . 3. Bagging . Bootstrap Aggregating (Bagging) uses averaging/voting method over a number of homogeneous weak models in order to reduce variance. Specifically, Bagging is divided into two parts: bootstrapping and aggregating. . Boostrapping: The entire dataset is performed random sampling with replacement on both rows and columns. This outputs a number of bootstraps where each of them is different from the others. | Aggregating: after boostrap samples are generated, they are fit into the weak learners. All the model results will be combined by averaging (for regression) or voting (for classification). | . A Bagging ensembler operates as a committee that outperforms any individual weak model. This wonderful effect - the wisdom of crowds - can be explained that weak models protect each other from their individual errors. If the members share the same behaviors, they also make the same mistakes. Therefore, the low correlation between weak models is the key. Note that the Bagging method requires the initial sample to be large enough for the bootstrapping step to be statistical significant. . Random Forest . Random Forest is the implementation of Bagging method on Decision Trees. It can be easily parallelized, does not requires too much hyperparameters tuning and has a decent prediction power. Random Forest is a very popular algorithm, before Boosting methods take the crown. . RandomForestClassifier and RandomForestRegressor classes have the following Bagging hyperparameters (ones inherited from Decision Tree are not mentioned): . n_estimators: the number of trees in the forest, defaults to 100. Control the complexity of the algorithm. Try increasing this when the model is underfitting, but it will take a longer training time. | max_features: the ratio of features used in each tree, defaults to auto (square root of nFeature). A lower value increases bias and reduces variance. | max_samples: the ratio of instances used in each tree, defaults to None (100% of nSample). A lower value increases bias and reduces variance. | . Extra Trees . Besides Random Forest, Sickit-learn also develops a quite similar algorithm, Extremely Randomized Trees (Extra Trees for short). Instead of finding the split with highest information gain at each step, this method goes one step further in randomness by selecting the best candidate among a number of randomly-generated cut points. . The ExtraTreesClassifier and ExtraTreesRegressor classes have same hyperparameters as in Random Forest, there are only some small differences in their default values. . Implementation . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.linear_model import LogisticRegression from sklearn.ensemble import BaggingClassifier, BaggingRegressor from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . modelBase = LogisticRegression(solver=&#39;liblinear&#39;, class_weight={0:1, 1:10}) ensembler = BaggingClassifier(modelBase, 10, random_state=7) model1 = RandomForestClassifier(n_estimators=20, random_state=7) model2 = ExtraTreesClassifier(n_estimators=20, random_state=7) . models = [modelBase, ensembler, model1, model2] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9845 [LogisticRegression] AUC = 0.9897 [BaggingClassifier] AUC = 0.9688 [RandomForestClassifier] AUC = 0.9749 [ExtraTreesClassifier] . 4. Boosting . Boosting works in the same spirit as Bagging: it also build a group of homogeneous models to obtain a more powerful predictor. The difference is that Boosting trains weak models sequentially while Bagging perform the training independently. The idea behind Boosting is to fit models iteratively such that the training of each model depends on the previous ones. Using this strategy, badly handled observations in the earlier steps will be taken care better in the later steps. Since the Boosting method puts its efforts on important cases, we end up have a strong learner with lower bias. . In many competitions, Boosting methods used on Decision Trees are so effective for tabular datasets and is widely used by top competitors. For the rest of this article, we will take a deep dive into a bunch of interesting Boosting algorithms. To start off, let&#39;s take a quick overview of two Boosting approaches, Adaptive Boosting and Gradient Boosting. They do both train trees consequently, but behave differently. We first denote: $ eta$ - the learning rate, $T$ - the number of iterations, $f^{(t)}$ and $f^{(t)}( mathbf{X})$ - the tree number $t$ and its predicted value for $t=1,2, dots,T$. . Adaptive Boosting . Train $f^{(1)}$ | Train $f^{(2)}$ base on $f^{(1)}$ | Train $f^{(3)}$ base on $f^{(2)}$ | ... | Train $f^{(T)}$ base on $f^{(T-1)}$ | Scale each tree by a coefficient $ eta$ and predict $ hat{ mathbf{y}} leftarrow eta f^{(1)}( mathbf{X})+ eta f^{(2)}( mathbf{X})+ dots+ eta f^{(T)}( mathbf{X})$ | . Gradient Boosting . Intialize $ hat{ mathbf{y}}^{(0)}$ | Train $f^{(1)}$ base on $ hat{ mathbf{y}}^{(0)}$ and compute $ hat{ mathbf{y}}^{(1)}= hat{ mathbf{y}}^{(0)}+ eta f^{(1)}( mathbf{X})$ | Train $f^{(2)}$ base on $ hat{ mathbf{y}}^{(1)}$ and compute $ hat{ mathbf{y}}^{(2)}= hat{ mathbf{y}}^{(1)}+ eta f^{(2)}( mathbf{X})$ | ... | Train $f^{(T)}$ base on $ hat{ mathbf{y}}^{(T-1)}$ and compute $ hat{ mathbf{y}}^{(T)}= hat{ mathbf{y}}^{(T-1)}+ eta f^{(T)}( mathbf{X})$ | Predict $ hat{ mathbf{y}} leftarrow hat{ mathbf{y}}^{(T)}$ | . 5. Adaptive Boosting . Adaptive Boosting was originally designed for binary classification problems. This method can be used to boost any algorithm, but Decision Tree is always the go-to choice. More specifically, Decision Trees used here are very shallow, they only have one root and two leaves, explaining why they are also called Decision Stumps. . Algorithm . Input: . A dataset having $N$ observations $( mathbf{X}, mathbf{y})= {( mathbf{s}_n,y_n) }_{n=1}^N$ where $y_n in {-1,1 }$ | The number of weak models, $T$ | The learning rate, $ eta$ | . Step 1. Initialize the weight for each observation: $w_n^{(1)}=1/N$. . Step 2. For each iteration number $t$ where $t=1,2, dots,T$: . Train a weak model $f^{(t)}$ that minimizes the sum of weights over misclassifications, represented by the error: . $$ epsilon^{(t)}= sum_{n=1}^{N}{w_n^{(t)} left[ hat{y}_n neq y_n right]}$$ . | Calculate $ alpha^{(t)}$ the amount of say for the current weak classifier; deciding how much $f^{(t)}$ will contribute in the final prediction. This calculation rewards $f^{(t)}$ a very high influence if its total error is low and penalizes $f^{(t)}$ a negative influence for a high total error. . $$ alpha^{(t)}= frac{ eta}{2} , log{ frac{1- epsilon^{(t)}}{ epsilon^{(t)}}}$$ . | Update sample weights for the next iteration so that: the weights of the correctly classied samples decrease $ exp{( alpha^{(t)})}$ times and the weights of misclassifications increase the same amount. Notice that the term $- hat{y}_n y_n$ equals to $1$ if the prediction is correct and equals to $-1$ if the prediction is incorrect. . $$w_n^{(t+1)}=w_n^{(t)} exp{ left(- hat{y}_n y_n alpha^{(t)} right)}$$ . | Normalize new weights so that they add up to $1$. This step is required to make the calculation of $ alpha^{(t+1)}$ meaningful. At this step, some implementations resample the dataset so that the distribution of observations follows the newly calculated weights. . | . Step 3. Build an additive strong model that performs weighted voting over $T$ weak learners; this is model outputs the prediction of the algorithm, $ hat{ mathbf{y}}$. The formula uses the notation $ text{sign}( bullet)$, indicating the sign function. . $$ hat{ mathbf{y}}= text{sign} left( sum_{t=1}^T alpha^{(t)} f^{(t)}( mathbf{X}) right)$$ . import numpy as np import matplotlib.pyplot as plt plt.style.use([&#39;seaborn&#39;, &#39;seaborn-whitegrid&#39;]) %config InlineBackend.figure_format = &#39;retina&#39; . eta = 0.2 x = np.linspace(0.01, 0.99, 1000) y = eta * 1/2 * np.log(1/x - 1) fig, ax = plt.subplots(figsize=(5,5)) ax.plot(x, y, &#39;k&#39;, label=fr&#39;Learning rate = {eta}&#39;) ax.axis(&#39;scaled&#39;) ax.set_xlabel(r&#39;Error, $ epsilon_t$&#39;) ax.set_ylabel(r&#39;Influence, $ alpha_t$&#39;) ax.legend() plt.show() . Implementation . The classes AdaBoostClassifier and AdaBoostRegressor have the following Boosting hyperparameters: . base_estimator: the algorithm to be boosted, defaults to None (Decision Tree with max depth of 1). | n_estimators: the number of boosting stages ($T$), defaults to 50. | learning_rate: the learning rate ($ eta$), defaults to 1. | . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . modelBase = DecisionTreeClassifier(max_depth=1) ensembler = AdaBoostClassifier(modelBase, n_estimators=50, learning_rate=1) . models = [ensembler] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9828 [AdaBoostClassifier] . 6. Gradient Boosting . Grandient Boosting is another boosting strategy beside Adaptive Boosting. The idea of this method is mostly inspired by Gradient Descent, thus the name Gradient Boosting. Just like other ensembling methods, this algorithm works best on Decision Trees and becomes the foundation for its modern variants such as XGBoost, LightGBM and CatBoost. . Gradient Boosting Trees was originally designed for regression problems. For classification, we use regression approach to predict log of the odds. . Algorithm . Input: . A dataset having $N$ labeled observations $( mathbf{X}, mathbf{y})= {( mathbf{s}_n,y_n) }_{n=1}^N$ | The number of weak models, $T$ | The learning rate, $ eta$ | A differentiable loss function $ mathcal{L}( hat{ mathbf{y}})$ (squared error is a popular choice) | . Step 1. Initialize the prediction as a constant. Since this is the very first prediction and will be updated step-by-step, we denote this value $ hat{ mathbf{y}}^{(0)}$. When the loss function is MSE, this value is nothing but the mean of $ mathbf{y}$. . $$ hat{ mathbf{y}}^{(0)}= arg min sum_{n=1}^N mathcal{L}( hat{y}_n)$$ . Step 2. For $t=1$ to $T$: . Compute the psuedo-residual $r_n^{(t)}$ equals to the negative gradient of the loss function with respect to the prediction of the iteration $t-1$. When MSE is used, this term is proportional to the actual residual, $y_n- hat{y}_n^{(t-1)}$. In general, we call it pseudo-residual which allows plugging in different loss functions. . $$r_n^{(t)}=-g_n^{(t)}=- frac{ partial mathcal{L}( hat{y}_n^{(t-1)})}{ partial hat{y}_n^{(t-1)}}$$ . | Fit a weak learner (regression tree) $f^{(t)}$ using the training set $( mathbf{X}, mathbf{r}^{(t)})$. This step results in a tree with $M$ leaf nodes; meaning the input space is split into $M$ disjoint regions, each region is denoted $R_m ;(m=1,2, dots,M)$. Trees in this step are not restricted to be stumps as in AdaBoost. Compute $f^{(t)}( mathbf{X})$, the predicted value for the model $f^{(t)}$ so that it minimizes the loss function at the current step. . $$f^{(t)}( mathbf{X})= underset{f}{ arg min} sum_{n=1}^{N}{ mathcal{L} left( hat{y}_n^{(t-1)}+f( mathbf{s}_n) right)}$$ . | Use first-order Taylor approximation: $f(x=a) approx f(a)+f&#39;(a)(x-a)$ to estimate the loss function evaluated at step $t-1$. Here, $x$ corresponds to $ hat{y}_n^{(t-1)}+f( mathbf{s}_n)$ and $a$ corresponds to $ hat{y}_n^{(t-1)}$. Using the notation $g_n^{(t)}$ defined earlier, we have: $ mathcal{L} left( hat{y}_n^{(t-1)}+f( mathbf{s}_n) right) approx mathcal{L} left( hat{y}_n^{(t-1)} right)+g_n^{(t)}f( mathbf{s}_n)$. We can prove that $f^{(t)}( mathbf{X})$ is proportional to the negative gradient as follows. When MSE is chosen, is simply computes the average residual in each leaf. . $$ begin{aligned} f^{(t)}( mathbf{X}) &amp; approx underset{f}{ arg min} sum_{n=1}^{N} mathcal{L} left( hat{y}_n^{(t-1)} right)+g_n^{(t)}f( mathbf{s}_n) &amp;= underset{f}{ arg min} sum_{n=1}^{N}g_n^{(t)}f( mathbf{s}_n) propto -g_n^{(t)} end{aligned}$$ . | Compute the predicted value up to the current step, $ hat{ mathbf{y}}^{(t)}$. Since we are adding negative gradient $-g_n^{(t-1)}$ scaled by the learning rate $ eta$ step-by-step, this can be considered a Gradient Descent process. . $$ hat{ mathbf{y}}^{(t)}= hat{ mathbf{y}}^{(t-1)}+ eta f^{(t)}( mathbf{X})$$ . | . Step 3. Take the last round&#39;s predicted value as the final prediction: $ hat{ mathbf{y}} leftarrow hat{ mathbf{y}}^{(T)}$. Note that in Gradient Boosting, the prediction at each iteration $ hat{ mathbf{y}}^{(t)}$ has taken into account all weak learners up to the current step. This behaviour is not like Adaptive Boosting, in which the strong model is only built once all weak leaners was trained successfully. . Implementation . The GradientBoostingClassifier and GradientBoostingRegressor classes are the original impelementation of Gradient Boosting Trees. After a while, HistGradientBoostingClassifier and HistGradientBoostingRegressor which use histogram-based split finding were introduced. The later implementation is significantly faster for big datasets. The common hyperparameters of these algorithms are: . loss: the type of loss function, defaults to deviance (classification) and squared_error (regression). | n_estimators: the number of boosting stages ($T$), defaults to 100. | learning_rate: the learning rate ($ eta$), defaults to 0.1. | max_features: the ratio of features used in each tree, defaults to auto (square root of nFeature). A lower value increases bias and reduces variance. | subsample: the ratio of instances used in each tree, defaults to 1 (100% of nSample). A lower value increases bias and reduces variance. | criterion: the measure of quality of splits, defaults to friedman_mse. | . import numpy as np import pandas as pd np.set_printoptions(precision=4, suppress=True) from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier . dfCancer = pd.read_csv(&#39;data/breast_cancer.csv&#39;) x = dfCancer.drop(columns=&#39;target&#39;) y = dfCancer.target xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=1/5, random_state=1) xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=1/4, random_state=1) . model1 = GradientBoostingClassifier(random_state=7) model2 = HistGradientBoostingClassifier(random_state=7) models = [model1, model2] for model in models: model = model.fit(xTrain, yTrain) yPred = model.predict_proba(xTest)[: ,1] auc = roc_auc_score(yTest, yPred) print(f&#39;AUC = {auc:.4f} [{model.__class__.__name__}]&#39;) . AUC = 0.9729 [GradientBoostingClassifier] AUC = 0.9831 [HistGradientBoostingClassifier] .",
            "url": "https://hungpq7.github.io/data-science-blog/machine-learning/ensemble-learning/decision-tree/bagging/boosting/2022/08/24/ensemble-learning.html",
            "relUrl": "/machine-learning/ensemble-learning/decision-tree/bagging/boosting/2022/08/24/ensemble-learning.html",
            "date": " • Aug 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hungpq7.github.io/data-science-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hungpq7.github.io/data-science-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}